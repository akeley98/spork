% xelatex </dev/null spork_mini.tex

\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Exo GPU (Spork) Summary 2025-03-11}

I'll outline a sketch of how we'll extend Exo to target CUDA, then go over the core features of the language extension and their relationships to CUDA concepts:

\begin{enumerate}
  \item Async blocks \& actor kind: CUDA asynchronous kernel launch and asynchronous instructions
  \item Parallel loops: explict scheduling at each level of the CUDA thread hierarchy
  \item Synchronization statements
\end{enumerate}

\filbreak
\mainSub{Language Sketch}

We introduce two core concepts for the Spork Exo-GPU extension:
\begin{enumerate}
  \item The \myKeyA{actor kind}; broadly, ``what kind'' of hardware instructions Exo code will be lowered to.
    The most common actor kinds are \lighttt{cpu} (default) and \lighttt{cuda\_classic} (typical, synchronous CUDA instructions).
    We will also have actor kinds for categories of CUDA asynchronous instructions.
  \item The \myKeyA{collective unit}; broadly, how many cooperating threads execute a statement.
    Currently, each Exo statement is assumed to be executed by 1 thread, but for the GPU, some statements could be executed by multiple cooperating threads (e.g. a warp of 32 convergent threads).
\end{enumerate}
The \myKeyA{actor kind} and \myKeyA{collective unit} are statically analyzed for each statement in an Exo proc.
Both of these terms are my invention (CUDA programmers won't know what you're talking about).

\filbreak
At multiple levels, we will have to model blocks of Exo code that run on a different timeline than the surrounding code:
\begin{enumerate}
  \item CPU code launching a CUDA kernel:\\
  subsequent CPU instructions don't wait for the launched kernel.
  \item CUDA code executing asynchronous CUDA instructions:\\
  subsequent CUDA instructions (on the same thread!) don't wait for the async instruction.
\end{enumerate}

\filbreak
\mainKey{Async Block:} Each level corresponds to a change in \myKeyA{actor kind}.
To do this, the user will wrap the subtree in an async block (overloaded \lighttt{with} statement).
For example, wrapping code with \lighttt{with CudaDeviceFunction(...)} lowers the wrapped code to CUDA (\lighttt{cuda\_classic} actor kind).

\filbreak
\mainKey{Parallel-for:} The user will then set the \myKeyA{collective unit} for code by using parallel-for loops.
Each loop splits the parent ``collective lane'' into a fixed number of smaller collective lanes, with one collective lane assigned to execute each ``loop iteration''. For example:

{\color{lightttColor}
\begin{verbatim}
# Assume current collective unit is 128 cuda threads
for w in cuda_threads(0, 4, unit=32 * cuda_thread):
    # Collective unit is now 32 cuda threads (warp)
    # Parent collective unit was split in 4 (perfectly matches 4 iterations)
    warp_instr(...) # Executed cooperatively by one warp
\end{verbatim}
}

\filbreak
\mainKey{Synchronization:}
We will \textit{not} be taking a fork-join approach here; this is too restrictive to model the highly pipelined and asynchronous instructions needed to write tensor processing kernels for the A100 and especially the H100.

\filbreak
Instead, we will provide (and task) the user with inserting explicit \myKeyA{synchronization statements}.
In the simple case, the user can use an all-to-all sync (all threads in a certain collective lane wait for all others),
but we will also provide finer-grained arrive/await synchronization (e.g. threads 128-383 wait for threads 0-31).

\filbreak
By statically analyzing the \myKeyA{actor kind} and \myKeyA{collective unit} for each statement, we can prevent entire categories of synchronization bugs.
If you have prior CUDA experience, you may see how we can prevent the following bugs:

\filbreak
\mainKey{Async Instruction Mistakes:} If the user employs CUDA async instructions (e.g. cp.async, TMA, wgmma tensor cores) then the user must explicitly place them in async blocks with the correct actor kind.
We will then enforce that the correct barriers were used to wait for their results prior to usage in future statements.

\filbreak
\mainKey{Barrier Divergence:} Suppose the user inserts a \lighttt{\_\_syncthreads} somewhere.
(Exo syntax to be described in later sections).
We will enforce that this appears in a scope where the collective unit is one CTA.

\filbreak
\mainKey{Invalid Divergence:} Some instructions expect to be called by a certain convergent collective unit (e.g. warp/warpgroup tensor core instructions) and we will enforce this at the call site of each instr.
This is really a generalization of the barrier divergence issue, although older literature won't be aware of this, since these new instructions weren't invented yet.

\filbreak
\mainSub{Code Example Sketch}
{\color{lightttColor}
\begin{verbatim}
def my_proc(...):
    # CPU code here (actor kind: cpu)
    with CudaDeviceFunction(blockDim = 128):  # 128 threads per block
        # CUDA code here (actor kind: cuda_classic)
        # Lowered to CUDA device function
        #
        # ... distribute work across blocks (to be explained)
            for y in cuda_threads(0, 16, unit=8 * cuda_thread):
                # 16 "iterations", executing collective unit = 8 cuda threads
                # So block of 128 threads is subdivided to 16 groups of 8 threads each.
                for x in cuda_threads(0, 8, unit=cuda_thread):
                    # 8 "iterations", executing collective unit = 1 cuda thread
                    # We further subdivided the 8 threads into 8 single threads
\end{verbatim}
}

\filbreak
\mainSub{Core Idea: S/M Equivalence}

The key idea of this design is to keep the parallelism constructs orthogonal to the core language, as if they were pragmas that only control code lowering.
In particular, these constructs don't change the dataflow.
All rewrite operations (and probably Chexo) will continue to interpret procs under \myKeyA{S-semantics} (single-threaded semantics), where
\begin{enumerate}
  \item parallel-for loops are treated like sequential loops
  \item async blocks are treated like \lighttt{if True}
\end{enumerate}

\filbreak
Corollary: current rewrites don't need to change, and any new rewrites that parallelize loops, add async blocks, or modify synchronization don't need any checking.

\filbreak
The final lowered proc will exhibit \myKeyB{M-semantics} (multi-threaded semantics), where parallel/async Exo constructs are lowered to actual parallel/async code.
To complete the chain-of-equivalence \textbf{(figure \ref{fig:chain})} between the original proc and the lowered CUDA code, we just have to prove that the \textit{final} proc interpreted under S-semantics gives the same result as said proc interpreted under M-semantics.
In other words, prove that sufficient synchronization exists to guarantee that the parallel program produces the same
output as-if it were forcibly executed sequentially (interpreted under \myKeyA{S-semantics}).

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=6mm]
\node(proc0) [normalnode] {$proc_0$\\\myKeyA{S-semantics}};
\node(proc1) [normalnode, right=of proc0] {$proc_1$\\\myKeyA{S-semantics}};
\node(procNS) [normalnode, right=of proc1] {$proc_N$\\\myKeyA{S-semantics}};
\node(procNM) [normalnode, right=of procNS] {$proc_N$\\\myKeyB{M-semantics}};
\node(cuda) [smallnode, right=of procNM] {CUDA C++};

\draw [arrow] (proc0) -- (proc1);
\draw [arrow, dotted] (proc1) to node(rewrites)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=7cm, minimum width=7cm, below=of rewrites, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg, xshift=-2cm] {Existing Exo Rewrites + \textbf{parallelism rewrites} (ignored in S-semantics)};
\node(sync) [normalnode, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Synchronization Checking};
\node(spork) [normalnode, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Codegen};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg] {Physically same proc, \textbf{different interpretation}};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (proc1);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\end{tikzpicture}
\caption{Chain-of-equivalence (S/M equivalence)} \label{fig:chain}
\end{figure*}

\filbreak
\myTitle{Parallelism \& Memory}

\mainSub{CUDA Thread \& Memory Hierarchy}

My core observation of the CUDA programminng model is that the mapping of the algorithm to the parallel hardware is very \myKeyA{explicit}, at least compared to HLS and other ``clever'' quasi-C compilers.
The CUDA programmer is tasked with choosing the number of threads to launch, programming individual threads (workload distribution), and ensuring correct synchronization.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=3.5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKeyA{GMEM}};
\node (gmeminfo) [widenode, right=of gmem] {\myKeyA{GMEM:} Global memory. \myKeyA{``slow''}, 10s of GB. Any thread in grid may access.};
\draw[line] (grid) -- (gmem);

\node (block0) [smallishnode, fill=violetBoxBg, below=of grid, xshift=-15mm, yshift=-16mm] {block};
\node (block1) [smallishnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallishnode, right=of block1, xshift=1.6cm] {block};

\node (smem0) [smallishsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallishsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallishsmemnode, above=of block2] {\myKeyB{SMEM}};
\node(smeminfo) [widenode, right=of smem2, yshift=-6mm] {\myKeyB{SMEM:} Shared memory. Per-CTA memory (L1 cache carveout). \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math.};

\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=100] ($(block2.west)+(0,0.6)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=330,in=100] (gridDim.north);

\node (thread0) [smallishnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallishnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallishnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (threadinfo) [widenode, right=of thread2, yshift=2mm, fill=violetBoxBg] {Threads in the same block (CTA) may synchronize explicitly with each other.};
\node (rmem0) [smallishnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallishnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallishnode, below=of thread2] {\textbf{RMEM}};
\node (rmeminfo) [widenode, right=of rmem2] {\textbf{RMEM:}\\Register ``memory''. 255 per thread.};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=150] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\caption{CUDA thread and memory hierarchy} \label{fig:hierarchy}
\end{figure*}

\filbreak
\mainKey{Kernel Launch:} CUDA threads are launched from the CPU, organized as a grid of \lighttt{gridDim}-many blocks (\myKeyB{CTA}: cooperative thread array), each consisting of \lighttt{blockDim}-many threads.
Both parameters are set by the programmer.
Furthermore, the hardware arranges threads into aligned 32-thread \myKeyA{warps} and 128-thread \myKeyA{warpgroups} (this is important for certain SIMD operations).

\filbreak
\mainKey{Work Distribution:} For the most part, from CUDA, each thread is programmed as if it were a scalar processor.
The programmer assigns work to each thread.

\filbreak
\mainKey{Memory Types:} The programmer manually manages moving data between different memory types in CUDA.
These form a hierarchy roughly parallel to the thread hierarchy \textbf{(figure \ref{fig:hierarchy})}.
Each thread block has its own private shared memory (\myKeyB{SMEM}), whose size and usage is controlled by the programmer.
This is typically used as a manual cache, or to communicate values between threads.

\filbreak
\mainKey{Programmer-managed Synchronization:}
  The programmer manages synchronization between threads.
  \textit{Within} a single block, there are constructs for synchronizing all or a subset of threads with another set of threads.
  \textit{Across} blocks, for the most part, there are no native synchronization mechanisms besides atomic reductions.
  Thus, typical algorithms have \myKeyB{frequent} communication \myKeyB{within} blocks, and \myKeyA{minimal} communication \myKeyA{between} blocks.
  With the H100 (Hopper generation), CTAs are now grouped into clusters, which allow some limited cross-CTA communication.

\filbreak
\minorSub{Extra Info: Thread Block Clusters}

Prior to the H100, shared memory was truly local to a single block.
The H100 introduces clusters of 2, 4, 8, or 16 thread blocks.
Threads within the same cluster support synchronizing with each other and can view each others' shared memory (\myKeyA{distributed shared memory}), with restrictions.
For the most part, this feature is set up assuming that all CTAs' shared memory layout is the same, and multicasting patterns are used.
Direct access to another within-cluster CTA's shared memory is possible, but expensive.

\filbreak
\mainSub{Kernel Launch}

The \lighttt{with CudaDeviceFunction(blockDim, clusterDim=1, blocks\_per\_sm=1)} async block defines a CUDA kernel/grid launch.
The parameters are defined by arguments used to construct the \lighttt{CudaDeviceFunction} object.

\filbreak
\mainKey{Block (CTA) Size}: \lighttt{blockDim}.

\filbreak
\mainKey{Cluster Size:} \lighttt{clusterDim} (must be 1 for pre-H100).

\filbreak
\mainKey{Grid Size:} \lighttt{blocks\_per\_sm} times SM count (I'm not sure if this is the best way to expose \lighttt{gridDim} yet).
If \lighttt{clusterDim > 1}, we further need to query the maximum number of clusters of the given size that can execute on the H100 device concurrently.

\filbreak
\mainKey{Collective Unit:} Besides changing the actor kind from \lighttt{cpu} to \lighttt{cuda\_classic}, this defines the \myKeyA{top-level collective unit}, which is an entire cluster of \lighttt{clusterDim × blockDim} threads.

\filbreak
\mainSub{Parallel Loops}

Exo currently provides a binary choice of loop mode: \lighttt{seq} or \lighttt{par}.
We will generalize this to a loop mode object defined in Python code.
The Spork extension will define two new loop modes: \lighttt{cuda\_tasks}, and \lighttt{cuda\_threads}.

\filbreak
\mainKey{Cuda Tasks:} At the top-level, an Exo CUDA kernel is defined by a single nest of \lighttt{cuda\_tasks} loops.
Consecutive \myKeyA{tasks} (i.e. body of the inner-most \lighttt{cuda\_tasks} loop) are assigned to \myKeyA{top-level collectives} (clusters or CTAs) in a round-robin order.
Such loops correspond to the ``\myKeyA{minimal} communication \myKeyA{between} blocks'' (or clusters) portion of a typical CUDA algorithm.

\filbreak
Example:
{\color{lightttColor}
\begin{verbatim}
with CudaDeviceFunction(...):
    for y in cuda_tasks(0, size_y):
        for x in cuda_tasks(0, size_x):  # say size_x = 10
            foo
            bar
\end{verbatim}
}
Say there are 48 CTAs on the device.
Then CTA 0 executes the task (\lighttt{foo, bar}) for \lighttt{(y,x) = (0,0)}, CTA 1 executes the task for \lighttt{(y,x) = (0,1)} ... CTA 47 executes for \lighttt{(y,x) = (4,7)}, and we wraparound to CTA 0 for executing \lighttt{(y,x) = (4,8)}.

\filbreak
The \lighttt{cuda\_task} loop can basically do anything an ordinary \lighttt{seq} loop is, since they're lowered to sequential loops (plus some modular arithmetic for task distribution).

\filbreak
NOTE: the \lighttt{cuda\_tasks} loop does \textit{not} change the actor kind or collective unit.
It's simply a tool for allowing the user to define what order tasks are executed, which can have a huge impact on L2 hit rate.
TODO maybe support if statements nested in \lighttt{cuda\_tasks} loops, for imperfect tiling.

\filbreak
\mainKey{Cuda Threads:} Below the \lighttt{cuda\_tasks} loops, the \lighttt{cuda\_threads} loops may appear anywhere, although the valid usage is much more restrictive.

Each statement, including a for loop, is executed by a certain \myKeyA{collective lane} of cooperating threads (base case: one thread).
This number and arrangement of threads within this collective lane is statically described with a \myKeyA{collective unit} (e.g. thread, warp, warpgroup).

\filbreak
A \lighttt{cuda\_threads(lo, hi, unit=...)} loop statement takes the executing ``parent'' collective lane and subdivides it into collective lanes of the collective unit type specified.

\filbreak
The loop must have \lighttt{lo=0}, and \lighttt{hi} a constant.
Each iteration is assigned to a different collective lane, and the total number of threads required (iteration count times collective unit size) must not exceed the number available in the parent collective lane.
(More precisely, the number of tiles created by applying the argument collective unit as a tile operator to the parent collective unit must be at least the number of loop iterations; this is not described in this document).
However, it's allowed for there to be fewer threads used than available.

\filbreak
Such loops correspond to the ``\myKeyB{frequent} communication \myKeyB{within} blocks'' (or clusters) portion of a typical CUDA algorithm.

\filbreak
Example:
{\color{lightttColor}
\begin{verbatim}
with CudaDeviceFunction(blockDim=512):  # top-level collective: CTA/block of 512 T (threads)
    for foo in cuda_tasks(...):         # cuda_tasks: no effect on collective unit
        for w in cuda_threads(0, 16, unit=cuda_warp):
            # Parent collective lane (512 T) cut into 16 warps (32 T)
            warp_fn()  # executed by collective lane of 32 convergent threads
            for t in cuda_threads(0, 32, unit=cuda_thread):
                # Parent collective lane (32 T) cut into 32 single threads.
        for y in cuda_threads(0, 64, unit=8*cuda_thread):
            # Parent collective lane (512 T) cut into 64 * 8 T
            # Note 8 threads isn't any special hardware unit -- just defined by user ad-hoc
            for x in cuda_threads(0, 8, unit=cuda_thread):
                # Parent collective lane (8 T) cut into 8 single threads.
        for z in cuda_threads(0, 12, unit=cuda_warp):
            # Parent collective lane (512 T) cut into 12 warps (32 T)
            # with 128 threads wasted.
        for w in cuda_threads(0, 16, unit=64 * cuda_thread):
            # Invalid: need 16 * 64 T (1024 T) but parent collective lane has only 512 T.
\end{verbatim}
}

\filbreak
No \myKeyA{Store} or \myKeyA{Reduce} statement may appear anywhere the collective unit is not a single thread.
Generally, only synchronization, control flow (including nested parallel for), and custom instrs appear as valid statements in blocks of code with non-single-thread collective units.

\filbreak
\minorSub{Extra Info: Parallel-for Syntax}

This is a little bit strange, but the currently implemented Spork loop syntax is

\lighttt{\ \ for \greenBox{iter} in \violetBox{loop\_mode\_name}(\greenBox{lo}, \greenBox{hi}, \blueBox{kwarg1 = value1}, \blueBox{kwarg2 = value2}...):}

where \greenBox{iter}, \greenBox{lo}, and \greenBox{hi} are \textit{parsed} as Exo code (can refer to values created in the Exo object code), wheras the keyword argument \blueBox{values} are \textit{evaluated} as Python code (can refer to Python values in-scope at the time the \lighttt{proc} is parsed).
The loop mode object is constructed as

\filbreak
\lighttt{~~\violetBox{loop\_mode\_name}(\blueBox{kwarg1 = value1}, \blueBox{kwarg2 = value2}...)}\\
\lighttt{~~\# often no args, e.g. seq()}

\filbreak
Currently, loop modes can't take positional arguments, only keyword arguments.

%% \filbreak
%% \minorSub{Extra Info: Collective Units}

\filbreak
\mainSub{Memory Allocation}

Thoughts on how to extend tensors and memory types for CUDA.

\mainKey{Memory Types:} Of course we can re-use Exo's memory types to express different kinds of CUDA memory:
\begin{itemize}
  \filbreak
  \item \lighttt{CudaBasicDeviceVisible}: All device-visible memory
  \filbreak
  \item \lighttt{CudaGmem*}: Global memory (allocated by the CPU)
  \filbreak
  \item \lighttt{CudaHost*}: Global memory sub-type; allocated by CPU; visible to CPU and CUDA device.
  \filbreak
  \item \lighttt{CudaSmem*}: Shared memory
  \filbreak
  \item \lighttt{CudaRmem}: Registers
  \filbreak
  \item \lighttt{GridConstant*}: CPU-to-CUDA constants
\end{itemize}

\filbreak
Since Exo uses \lighttt{issubclass} to check correct memory types, we may need to use multiple inheritance for more complicated cases (e.g. \lighttt{CudaHost*} types have to inherit from both \lighttt{DRAM} and \lighttt{CudaBasicDeviceVisible}).

\filbreak
The grid constants need a bit more explanation.
Unlike other memory types, grid constants don't have a fixed address, and are copied from the CPU to CUDA on kernel launch.
(These are the function parameters of a CUDA kernel).
We can model this in Exo as \lighttt{DRAM\_STACK} that the GPU is allowed to read (but not write) and is exempt from synchronization checking (due to the implicit copy, which protects ``concurrent'' CPU/CUDA access).

\filbreak
\mainKey{Memory Actor Kind:} We need to add a new member function allowing us to query whether each \myKeyA{actor kind} has permission to read, write, and allocate each type of memory.
For backwards compatibility, by default, memory types will allow all operations by the CPU and none by any other actor kind.

\filbreak
\begin{itemize}
  \filbreak
  \item \lighttt{CudaGmem*}: \lighttt{cpu} allocated; \lighttt{cuda\_classic} read/write
  \filbreak
  \item \lighttt{CudaHost*}: \lighttt{cpu} allocated, read/write; \lighttt{cuda\_classic} read/write
  \filbreak
  \item \lighttt{CudaSmem*}, \lighttt{CudaRmem}: \lighttt{cuda\_classic} allocated, read/write
  \filbreak
  \item \lighttt{GridConstant*}: \lighttt{cpu} allocated, read/write; \lighttt{cuda\_classic} read-only
\end{itemize}

\filbreak
These permissions don't apply to instrs, which are also the only way to interact with memory for actor kinds corresponding to CUDA async instructions.

\filbreak
\mainKey{Distributed Memory:} This is the trickiest part.
Consider a $64 \times 64$ matrix tile stored in CUDA registers.
This is far too big for one thread, so let's say it's divided into sub-tiles of size $16 \times 8$, with each tile resident in the registers of one thread of a $4 \times 8$ grid of threads.

\filbreak
There's two notions of the ``shape'' of this matrix tile.
\begin{enumerate}
  \item In \myKeyA{Exo}, the shape is $64 \times 64$, since here, we're concerned about the \textit{dataflow}, so we need the true (logical) tile size.
  \item In \myKeyA{CUDA}, the shape is $16 \times 8$, since registers are allocated by threads, and that's the size of the sub-tile for each thread.
\end{enumerate}
We bridge this gap with distributed memory analysis.

\filbreak
Each memory type has a ``native collective unit'' corresponding to its visibility scope in hardware (one thread for registers; one CTA for shared memory; one warp/warpgroup for tensor core matrix tiles).
If the collective unit at the point a tensor is allocated doesn't match the memory type's native collective unit, some of the leading dimensions are deduced to be ``distributed dimensions''.
The lowered allocation is based only on the remaining tensor dimensions, and may be a scalar, if all dimensions are distributed.

\filbreak
Example:

\graytt{\# Assume collective unit here is a CTA of only 1 warp (32 threads).}\\
\graytt{\# \yellowBox{SMEM:} no distributed dimensions; native unit (CTA) matches collective unit.}\\
\blacktt{A\_smem: f32[\blueBox{64, 16}] @ \yellowBox{CudaSmemLinear}}\\
\blacktt{B\_smem: f32[\blueBox{16, 64}] @ \yellowBox{CudaSmemLinear}}\\
\graytt{\# \yellowBox{Register} tile: \redBox{distributed}, \blueBox{non-distributed} dimensions}\\
\blacktt{C\_tile: f32[\redBox{4, 8}, \blueBox{16, 8}] @ \yellowBox{CudaRmem}}\\
\graytt{\# NB compiler looks ahead to see usage is C\_tile[\redBox{mt,nt},\blueBox{ms,ns}]}\\
\graytt{\# Deduction based on following usage:}\\
\blacktt{for ks in seq(0, 16):} \graytt{\# irrelevant, ignored.}\\
\blacktt{~~for \redBox{mt} in cuda\_threads(0, 4, unit=8*cuda\_thread):}\\
\blacktt{~~~~for \redBox{nt} in cuda\_threads(0, 8, unit=cuda\_thread):}\\
\graytt{~~~~~~\# Collective unit is now 1 thread -- matches native unit.}\\
\graytt{~~~~~~\# Further index variables deduced as non-distributed,}\\
\graytt{~~~~~~\# whether they are from parallel or sequential loops.}\\
\blacktt{~~~~~~for \blueBox{ms} in seq(0, 16):}\\
\blacktt{~~~~~~~~for \blueBox{ns} in seq(0, 8):}\\
\blacktt{~~~~~~~~~~C\_tile[\redBox{mt,nt},\blueBox{ms,ns}] += A\_smem[mt*16+ms,ks] * B\_smem[ks,nt*8+ns]}

\filbreak
The details of how this deduction is made are not included here, but crucially, know that we enforce that each thread only accesses its own tile of the distributed memory.
This same logic also applies to distributed shared memory (allocated per-CTA, but may hold a larger logical tile distributed across CTAs in a cluster).

\filbreak
\myTitle{Async Instructions \& Actor Kind}

CUDA defines some asynchronous instructions.
These are part of the instruction stream for a CUDA thread, but execution continues to the next instruction without waiting.
This means the following won't work if the highlighted statements are async:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$B \leftarrow Y$}
  \item $C \leftarrow C + AB$ (incorrectly assumes $A,B$ written to)
\end{enumerate}

\filbreak
Two async instructions issued by the same thread/warpgroup don't even complete in the same order relative to each other, e.g., the following is a data race:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$A \leftarrow Y$} (might be overwritten by ``prior'' $A \leftarrow X$)
\end{enumerate}

\filbreak
These instructions are also unaffected by ``ordinary'' synchronization (e.g. \lighttt{\_\_syncthreads});
you have to use specialialized barriers to wait for these instructions
(except, they are still protected by implicit ``stop the world'' barriers between grid launches on the same stream).

\filbreak
\mainSub{Async Instructions Summary}

These instructions were introduced with Ampere (a.k.a. sm\_80, A100) or Hopper (a.k.a. sm\_90a, H100).
Blackwell also introduces 5th generation tensor cores, which I don't cover here.

\filbreak
\mainKey{Non-bulk Async Copy (sm\_80):}
These instructions allow a single thread to copy 4, 8, or 16 aligned bytes from \myKeyA{GMEM} to \myKeyB{SMEM}.
The 16 byte copy is extra nice because it skips the L1 cache (avoids pollution).
Generally, to copy a matrix tile, you distribute the copies over many threads.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (gmembig) [bignode, right=of smem, yshift=-3mm] {};
\node (gmem) [gmemnode, right=of smem, xshift=9mm] {\myKeyA{GMEM tile}};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);

\node (smem2) [smemnode, right=of gmem, yshift=4mm] {\myKeyB{SMEM tile}};
\node (op2) [normalnode, right=of smem2, fill=violetBoxBg] {AtomicOp};
\draw [arrow] (smem2) -- (op2);
\node (gmembig2) [bignode, below=of op2, xshift=-8mm] {};
\node (gmem2) [gmemnode, below=of op2, yshift=-6mm] {\myKeyA{GMEM tile}};
\draw [arrow] (gmem2.east) to[out=0,in=0] (op2.east);
\draw [arrow] (op2.south) to[out=250,in=110] (gmem2.north);
\end{tikzpicture}
\caption{TMA matrix tile copy (left), reduce (right)} \label{fig:tma}
\end{figure*}

\filbreak
\mainKey{TMA (sm\_90a bulk async copy):}
These copies are ``bulk'' because a single thread can now copy an entire range of memory, or a (strided) matrix tile.
The copy can be in either direction between \myKeyA{GMEM} and \myKeyB{SMEM}.
Matrix tile reads/writes to \myKeyA{GMEM} are predicated \textbf{(figure \ref{fig:tma}, left)}.
Out-of-bounds writes are discarded; reads return 0 or NaN (configurable).

\filbreak
For matrix tile copies, you have to construct a \lighttt{CUtensorMap}, which describes the shape and strides of the source matrix.
This will be modelled as \lighttt{SpecialWindow} in Exo, not described in this document.

\filbreak
Furthermore, when copying \myKeyB{SMEM}$\to$\myKeyA{GMEM}, TMA instructions take an optional reduction operator.
This changes the behavior from \lighttt{GMEM[slice] = SMEM} to \lighttt{GMEM[slice] \violetBox{+=} SMEM} \textbf{(figure \ref{fig:tma}, right)}.
This reduction is \myKeyA{atomic} per-element.
Exo will expose only addition (\lighttt{+=}) for now, but the underlying instruction supports other reduction operators (e.g. max).

\filbreak
\mainKey{wgmma (sm\_90a tensor cores):}
These instructions need to be called convergently by a warpgroup (128 aligned threads).
They perform a matrix multiply-accumulate operation $D = D + AB$ where each is a matrix tile.
The $D$ (accumulator) must be in \textbf{RMEM} (register tile); $B$ is in \myKeyB{SMEM}, and $A$ is in either.
(Note, unlike the copy instructions, registers themselves are accessed and modified asynchronously).

\filbreak
The $A$ operand (if in \myKeyB{SMEM}) must be row-major, and the $B$ operand column-major.
The register tiles distribute the full matrix tile evenly over the registers of the 128 threads.
Although register tiles don't inherently have a majorness, in practice wgmma is most efficient when the $D$ tile is read/written in column-major order.
Finally, the \myKeyB{SMEM} operands may be swizzled for improved memory access patterns (this is really complicated and poorly documented).

\filbreak
\minorSub{Extra Info: sm\_90a Instructions}

\minorKey{Async Proxy:}
In CUDA terminology, TMA and wgmma (sm\_90a) are considered to operate in the \myKeyA{async proxy}
\begin{itemize}
  \item \myKeyA{generic proxy}: most other instructions
  \item \myKeyA{tensorMap proxy}: we'll ignore this
  \item Memory not visible by default across proxies
\end{itemize}

\filbreak
PTX docs has \textit{tomes} about this; boils down to
\begin{itemize}
  \item Need a \lighttt{fence.proxy.async} for generic$\to$async proxy data flow
  \item \textit{Nothing} needed for async$\to$generic
  \begin{itemize}
    \item (detail: the fence is required in both directions, but built in to the ``wait for async instruction'' machinery)
  \end{itemize}
\end{itemize}

\filbreak
\minorKey{TMA:}
Handled with \lighttt{cp.async.bulk.tensor} PTX instructions.

\begin{itemize}
\item Async 1D-5D tile copy \myKeyA{GMEM}$\leftrightarrow$\myKeyB{SMEM} \textbf{(figure \ref{fig:tma})}.
\item \myKeyB{SMEM} tile: densely packed C-order matrix
\begin{itemize}
  \item 128 byte alignment
\end{itemize}
\item \myKeyA{GMEM} tile: tile from big C-order matrix
\begin{itemize}
  \item \myKeyA{Predicated} and strided
  \item 16 byte aligned memory and strides
  \item Innermost rank can't be strided
\end{itemize}
\end{itemize}

\filbreak
\minorKey{TMA Synchronization, generic$\to$TMA:} Async proxy fence (\lighttt{fence.proxy.async}) required for TMA to see memory written by generic instructions.

\filbreak
\minorKey{TMA Synchronization, TMA$\to$generic:}

\filbreak
If the TMA copies \myKeyB{SMEM}$\to$\myKeyA{GMEM}, the completion mechanism is \textbf{commit\_group}

\filbreak
If the TMA copies \myKeyA{GMEM}$\to$\myKeyB{SMEM}, the completion mechanism is \textbf{mbarrier} (split barrier)

\filbreak
\begin{itemize}
  \item Initialized in SMEM with arrive-count $A$
  \item Any number of threads can wait until both of the following complete:
  \begin{itemize}
    \item $A$-many threads arrive
    \item \lighttt{tx-count}-many bytes copied by TMA
  \end{itemize}
  \item Re-usable (detail: requires parity tracking)
\end{itemize}

\filbreak
Usually, we nominate only \myKeyA{1 thread} to issue the TMA instruction.
Hence, we use a 1-to-many mbarrier to synchronize ($A = 1$).

\filbreak
NB mbarrier is usable without TMA (\lighttt{tx-count=0})

\begin{figure*}[b!]
\sffamily
\begin{tikzpicture}[node distance=1mm]
\node (fence0) [smallnode] {fence};
\node (mma00) [smallishnode, right=of fence0, fill=violetBoxBg] {MMA\\scale-d=0};
\node (mma01) [smallnode, right=of mma00, fill=violetBoxBg] {MMA};
\node (commit0) [smallnode, right=of mma01, fill=violetBoxBg] {commit\\group};
\node (fence1) [smallnode, right=of commit0] {fence};
\node (mma10) [smallnode, right=of fence1] {MMA};
\node (mma11) [smallnode, right=of mma10] {MMA};
\node (commit1) [smallnode, right=of mma11] {commit\\group};
\node (wait1) [smallishnode, right=of commit1, fill=violetBoxBg] {wait\_group \textbf{1}};
\draw[arrow] (mma00.south) to[out=330,in=210] (mma01.south);
\draw[arrow] (mma01.south) to[out=330,in=210] (commit0.south);
\draw[arrow] (commit0.south) to[out=350,in=190] (wait1.south);
\end{tikzpicture}
\caption{Two example pipeline stages for wgmma. The highlighted \lighttt{wait\_group} waits for prior highlighted instructions, but no other instructions.} \label{fig:wgmmaPipeline}
\end{figure*}

\filbreak
\minorKey{wgmma:} Each \lighttt{wgmma.mma\_async} instruction is issued by a warpgroup (128 aligned threads) and computes
\begin{itemize}
  \item $D \leftarrow AB$ or \hfill (scale-d = 0)
  \item $D \leftarrow AB + D$ \hfill (scale-d = 1)
\end{itemize}

\filbreak
As mentioned, $D$ (accumulator) is a \textbf{register tile}, $B$ is in \myKeyB{SMEM}, and $A$ is in either storage type.
Usually, scale-d=0 only for the first iteration.

\filbreak
\minorKey{Synchronization, generic$\to$wgmma:}
Synchronization differs depending on whether the values in question are in \textbf{RMEM} or \myKeyB{SMEM}.
For \textbf{registers}, issue \lighttt{wgmma.fence}.
For \myKeyB{SMEM}, issue \lighttt{fence.proxy.async}.

\filbreak
\minorKey{Synchronization, wgmma$\to$wgmma:}
wgmma defines an important implicit synchronization guarantee.
Consecutive \lighttt{wgmma.mma\_async} instructions using the same $D$ (accumulator) have their effects on $D$ implicitly ordered.
This applies to no other effects.

\filbreak
\minorKey{Synchronization, wgmma$\to$generic:}
The completion mechanism is ``commit group''.

\filbreak
wgmma synchronization only occurs within a single warpgroup, with MMAs pipelined:

\begin{itemize}
\item \lighttt{wgmma.fence} begins a \myKeyA{pipeline stage}
\item \lighttt{wgmma.mma} must appear within \myKeyA{pipeline stage}
\item \lighttt{wgmma.commit\_group} ends a \myKeyA{pipeline stage} \textbf{(figure \ref{fig:wgmmaPipeline})}
\item ptxas generates bad code if it can't recognize the pattern
\end{itemize}
\lighttt{wgmma.wait\_group N} waits for the MMAs of the $N^{th}$ prior pipeline stage (0-indexed)

\filbreak
\mainSub{Actor Kind, Actor Signature}

You probably didn't read all details on async instructions above.
That's OK; the purpose of the \myKeyA{actor kind} and \myKeyA{actor signature} abstractions is to allow us to categorize instructions and memory accesses by what kind of synchronization is required, without knowing the details of how the instruction or synchronization is implemented.
We just care that the synchronization performed matches that expected for the instruction.

\filbreak
For checking synchronization, we need to annotate, for each memory access (read/write) performed
\begin{itemize}
  \item The ID of the thread that performed the access (or set of threads, for warp/warpgroup cooperative instructions)
  \item ``How'' the instruction performed the access. This is the \myKeyA{actor signature}.
\end{itemize}

\filbreak
We have distinct \myKeyA{actor signatures} whenever we need to encode that different synchronization requirements hold.
(e.g. actions performed by sm\_80 cp.async, TMA, and wgmma all have different actor signatures since they are all synchronized differently).

\filbreak
\mainKey{Actor Kind:} We informally defined actor kind as a ``category'' of instructions.
Formally, an actor kind is a set of actor signatures (the categorized instructions may only use these signatures for accesses).
In addition, we allow ``synthetic'' actor kinds that aren't used to categorize instructions, but are just supersets or subsets of other actor kinds.
These will be used for synchronization statements, to be explained in later sections.

\filbreak
The actor kind also includes a $V_1$-transitive flag.
This is out of the scope of this document.

\filbreak
\mainKey{Sigthread:} Thread ID $\times$ actor signature pair.
This annotates each memory access.
A set of sigthreads can be summarized as $(T:\text{thread IDs} \times A:\text{actor kind})$, or a union of such.

\filbreak
\mainKey{List of actor signatures:} The separate actor signatures for the two types of wgmma register accesses are needed in order to handle the special implicit synchronization for back-to-back wgmma writes to accumulators.
\begin{align*}
\lighttt{sig\_cpu: } & \text{Default CPU}\\
\lighttt{sig\_cuda\_classic: } & \text{Default CUDA (synchronous CUDA instructions)}\\
\lighttt{sig\_Sm80\_cp\_async: } & \text{Ampere \lighttt{cp.async} (async memcpy in generic proxy)}\\
\lighttt{sig\_tma\_to\_smem: } & \text{Hopper TMA \lighttt{cp.bulk.async} to shared memory}\\
\lighttt{sig\_tma\_to\_gmem: } & \text{Hopper TMA \lighttt{cp\{.reduce\}.bulk.async} to global memory}\\
\lighttt{sig\_wgmma\_rmem\_a: } & \text{wgmma access to `A' parameter when stored in registers}\\
\lighttt{sig\_wgmma\_rmem\_d: } & \text{wgmma access to `D' parameter (accumulator) when stored in registers}\\
\lighttt{sig\_wgmma\_smem: } & \text{wgmma access to shared memory (either `A' or `B' parameter)}
\end{align*}

\filbreak
\mainKey{Illustration:} sigthread annotations for memory accesses for this example program: thread 0 issues a TMA instruction copying \textsf{GMEM} to \textsf{SMEM}, then each thread reads one value from \textsf{SMEM}.
This program would not be correct without (unseen) synchronization inserted between the two.
{\sffamily
\begin{align*}
    & \blueBox{\textit{[[ thread 0 kicks off TMA ]]}} \\
    \text{SMEM[$0$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+0$]} \\
    \text{SMEM[$1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+1$]} \\
    & ... \\
    \text{SMEM[$N-1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+N-1$]} \\
    & \blueBox{\textit{[[ thread $x$ reads SMEM[$x$] ]]}} \\
    \text{Y[$0$]} & \xleftarrow{\text{(0, sig\_cuda\_classic)}} \text{SMEM[$0$]} \\
    \text{Y[$1$]} & \xleftarrow{\text{(1, sig\_cuda\_classic)}} \text{SMEM[$1$]} \\
    & ... \\
    \text{Y[$N-1$]} & \xleftarrow{\text{(N-1, sig\_cuda\_classic)}} \text{SMEM[$N-1$]} \\
\end{align*}
}



\filbreak
\myTitle{Synchronization}

% Subsequent sigthreads to rely on the completion of actions...

\end{document}
