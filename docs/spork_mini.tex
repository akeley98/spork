% xelatex </dev/null spork_mini.tex

\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Exo GPU (Spork) Summary 2025-03-11}

I'll outline a sketch of how we'll extend Exo to target CUDA, then go over the core features of the language extension and their relationships to CUDA concepts:

\begin{enumerate}
  \item Async blocks \& actor kind: CUDA asynchronous kernel launch and asynchronous instructions
  \item Parallel loops: explict scheduling at each level of the CUDA thread hierarchy
  \item Synchronization statements
\end{enumerate}

\filbreak
\mainSub{Language Sketch}

We introduce two core concepts for the Spork Exo-GPU extension:
\begin{enumerate}
  \item The \myKeyA{actor kind}; broadly, ``what kind'' of hardware instructions Exo code will be lowered to.
    The most common actor kinds are \lighttt{cpu} (default) and \lighttt{cuda\_classic} (typical, synchronous CUDA instructions).
    We will also have actor kinds for categories of CUDA asynchronous instructions.
  \item The \myKeyA{collective unit}; broadly, how many cooperating threads execute a statement.
    Currently, each Exo statement is assumed to be executed by 1 thread, but for the GPU, some statements could be executed by multiple cooperating threads (e.g. a warp of 32 convergent threads).
\end{enumerate}
The \myKeyA{actor kind} and \myKeyA{collective unit} are statically analyzed for each statement in an Exo proc.
Both of these terms are my invention (CUDA programmers won't know what you're talking about).

\filbreak
At multiple levels, we will have to model blocks of Exo code that run on a different timeline than the surrounding code:
\begin{enumerate}
  \item CPU code launching a CUDA kernel:\\
  subsequent CPU instructions don't wait for the launched kernel.
  \item CUDA code executing asynchronous CUDA instructions:\\
  subsequent CUDA instructions (on the same thread!) don't wait for the async instruction.
\end{enumerate}

\filbreak
\mainKey{Async Block:} Each level corresponds to a change in \myKeyA{actor kind}.
To do this, the user will wrap the subtree in an async block (overloaded \lighttt{with} statement).
For example, wrapping code with \lighttt{with CudaDeviceFunction(...)} lowers the wrapped code to CUDA (\lighttt{cuda\_classic} actor kind).

\filbreak
\mainKey{Parallel-for:} The user will then set the \myKeyA{collective unit} for code by using parallel-for loops.
Each loop splits the parent ``collective lane'' into a fixed number of smaller collective lanes, with one collective lane assigned to execute each ``loop iteration''. For example:

{\color{lightttColor}
\begin{verbatim}
# Assume current collective unit is 128 cuda threads
for w in cuda_threads(0, 4, unit=32 * cuda_thread):
    # Collective unit is now 32 cuda threads (warp)
    # Parent collective unit was split in 4 (perfectly matches 4 iterations)
    warp_instr(...) # Executed cooperatively by one warp
\end{verbatim}
}

\filbreak
\mainKey{Synchronization:}
We will \textit{not} be taking a fork-join approach here; this is too restrictive to model the highly pipelined and asynchronous instructions needed to write tensor processing kernels for the A100 and especially the H100.

\filbreak
Instead, we will provide (and task) the user with inserting explicit \myKeyA{synchronization statements}.
In the simple case, the user can use an all-to-all sync (all threads in a certain collective lane wait for all others),
but we will also provide finer-grained arrive/await synchronization (e.g. threads 128-383 wait for threads 0-31).

\filbreak
\mainSub{Code Example Sketch}
{\color{lightttColor}
\begin{verbatim}
def my_proc(...):
    # CPU code here (actor kind: cpu)
    with CudaDeviceFunction(blockDim = 128):  # 128 threads per block
        # CUDA code here (actor kind: cuda_classic)
        # Lowered to CUDA device function
        #
        # ... distribute work across blocks (to be explained)
            for y in cuda_threads(0, 16, unit=8 * cuda_thread):
                # 16 "iterations", executing collective unit = 8 cuda threads
                # So block of 128 threads is subdivided to 16 groups of 8 threads each.
                for x in cuda_threads(0, 8, unit=cuda_thread):
                    # 8 "iterations", executing collective unit = 1 cuda thread
                    # We further subdivided the 8 threads into 8 single threads

\end{verbatim}
}

\filbreak
\mainSub{Core Idea: S/M Equivalence}

The key idea of this design is to keep the parallelism constructs orthogonal to the core language, as if they were pragmas that only control code lowering.
In particular, these constructs don't change the dataflow.
All rewrite operations (and probably Chexo) will continue to interpret procs under \myKeyA{S-semantics} (single-threaded semantics), where
\begin{enumerate}
  \item parallel-for loops are treated like sequential loops
  \item async blocks are treated like \lighttt{if True}
\end{enumerate}

\filbreak
Corollary: current rewrites don't need to change, and any new rewrites that parallelize loops, add async blocks, or modify synchronization don't need any checking.

\filbreak
The final lowered proc will exhibit \myKeyB{M-semantics} (multi-threaded semantics), where parallel/async Exo constructs are lowered to actual parallel/async code.
To complete the chain-of-equivalence \textbf{(figure \ref{fig:chain})} between the original proc and the lowered CUDA code, we just have to prove that the \textit{final} proc interpreted under S-semantics gives the same result as said proc interpreted under M-semantics.
In other words, prove that sufficient synchronization exists to guarantee that the parallel program produces the same
output as-if it were forcibly executed sequentially (interpreted under \myKeyA{S-semantics}).

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=6mm]
\node(proc0) [normalnode] {$proc_0$\\\myKeyA{S-semantics}};
\node(proc1) [normalnode, right=of proc0] {$proc_1$\\\myKeyA{S-semantics}};
\node(procNS) [normalnode, right=of proc1] {$proc_N$\\\myKeyA{S-semantics}};
\node(procNM) [normalnode, right=of procNS] {$proc_N$\\\myKeyB{M-semantics}};
\node(cuda) [smallnode, right=of procNM] {CUDA C++};

\draw [arrow] (proc0) -- (proc1);
\draw [arrow, dotted] (proc1) to node(rewrites)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=7cm, minimum width=7cm, below=of rewrites, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg, xshift=-2cm] {Existing Exo Rewrites + \textbf{parallelism rewrites} (ignored in S-semantics)};
\node(sync) [normalnode, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Synchronization Checking};
\node(spork) [normalnode, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Codegen};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg] {Physically same proc, \textbf{different interpretation}};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (proc1);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\end{tikzpicture}
\caption{Chain-of-equivalence (S/M equivalence)} \label{fig:chain}
\end{figure*}

\filbreak
\myTitle{Parallelism \& Memory}

\mainSub{CUDA Thread \& Memory Hierarchy}

My core observation of the CUDA programminng model is that the mapping of the algorithm to the parallel hardware is very \myKeyA{explicit}, at least compared to HLS and other ``clever'' quasi-C compilers.
The CUDA programmer is tasked with choosing the number of threads to launch, programming individual threads (workload distribution), and ensuring correct synchronization.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=3.5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKeyA{GMEM}};
\node (gmeminfo) [widenode, right=of gmem] {\myKeyA{GMEM:} Global memory. \myKeyA{``slow''}, 10s of GB. Any thread in grid may access.};
\draw[line] (grid) -- (gmem);

\node (block0) [smallishnode, fill=violetBoxBg, below=of grid, xshift=-15mm, yshift=-16mm] {block};
\node (block1) [smallishnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallishnode, right=of block1, xshift=1.6cm] {block};

\node (smem0) [smallishsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallishsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallishsmemnode, above=of block2] {\myKeyB{SMEM}};
\node(smeminfo) [widenode, right=of smem2, yshift=-6mm] {\myKeyB{SMEM:} Shared memory. Per-CTA memory (L1 cache carveout). \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math.};

\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=100] ($(block2.west)+(0,0.6)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=330,in=100] (gridDim.north);

\node (thread0) [smallishnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallishnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallishnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (threadinfo) [widenode, right=of thread2, yshift=2mm, fill=violetBoxBg] {Threads in the same block (CTA) may synchronize explicitly with each other.};
\node (rmem0) [smallishnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallishnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallishnode, below=of thread2] {\textbf{RMEM}};
\node (rmeminfo) [widenode, right=of rmem2] {\textbf{RMEM:}\\Register ``memory''. 255 per thread.};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=150] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\caption{CUDA thread and memory hierarchy} \label{fig:hierarchy}
\end{figure*}

\filbreak
\mainKey{Kernel Launch:} CUDA threads are launched from the CPU, organized as a grid of \lighttt{gridDim}-many blocks (\myKeyB{CTA}: cooperative thread array), each consisting of \lighttt{blockDim}-many threads.
Both parameters are set by the programmer.
Furthermore, the hardware arranges threads into aligned 32-thread \myKeyA{warps} and 128-thread \myKeyA{warpgroups} (this is important for certain SIMD operations).

\filbreak
\mainKey{Work Distribution:} For the most part, from CUDA, each thread is programmed as if it were a scalar processor.
The programmer assigns work to each thread.

\filbreak
\mainKey{Memory Types:} The programmer manually manages moving data between different memory types in CUDA.
These form a hierarchy roughly parallel to the thread hierarchy \textbf{(figure \ref{fig:hierarchy})}.
Each thread block has its own private shared memory (\myKeyB{SMEM}), whose size and usage is controlled by the programmer.
This is typically used as a manual cache, or to communicate values between threads.

\filbreak
\mainKey{Programmer-managed Synchronization:}
  The programmer manages synchronization between threads.
  \textit{Within} a single block, there are constructs for synchronizing all or a subset of threads with another set of threads.
  \textit{Across} blocks, for the most part, there are no native synchronization mechanisms besides atomic reductions.
  Thus, typical algorithms have \myKeyB{frequent} communication \myKeyB{within} blocks, and \myKeyA{minimal} communication \myKeyA{between} blocks.
  With the H100 (Hopper generation), CTAs are now grouped into clusters, which allow some limited cross-CTA communication.

\filbreak
\minorSub{Extra Info: Thread Block Clusters}

Prior to the H100, shared memory was truly local to a single block.
The H100 introduces clusters of 2, 4, 8, or 16 thread blocks.
Threads within the same cluster support synchronizing with each other and can view each others' shared memory (\myKeyA{distributed shared memory}), with restrictions.
For the most part, this feature is set up assuming that all CTAs' shared memory layout is the same, and multicasting patterns are used.
Direct access to another within-cluster CTA's shared memory is possible, but expensive.

\filbreak
\mainSub{Kernel Launch}

The \lighttt{with CudaDeviceFunction(blockDim, clusterDim=1, blocks\_per\_sm=1)} async block defines a CUDA kernel/grid launch.
The parameters are defined by arguments used to construct the \lighttt{CudaDeviceFunction} object.

\filbreak
\mainKey{Block (CTA) Size}: \lighttt{blockDim}.

\filbreak
\mainKey{Cluster Size:} \lighttt{clusterDim} (must be 1 for pre-H100).

\filbreak
\mainKey{Grid Size:} \lighttt{blocks\_per\_sm} times SM count (I'm not sure if this is the best way to expose \lighttt{gridDim} yet).
If \lighttt{clusterDim > 1}, we further need to query the maximum number of clusters of the given size that can execute on the H100 device concurrently.

\filbreak
\mainKey{Collective Unit:} Besides changing the actor kind from \lighttt{cpu} to \lighttt{cuda\_classic}, this defines the \myKeyA{top-level collective unit}, which is an entire cluster of \lighttt{clusterDim × blockDim} threads.

\filbreak
\mainSub{Parallel Loops}

Exo currently provides a binary choice of loop mode: \lighttt{seq} or \lighttt{par}.
We will generalize this to a loop mode object defined in Python code.
The Spork extension will define two new loop modes: \lighttt{cuda\_tasks}, and \lighttt{cuda\_threads}.

\filbreak
\mainKey{Cuda Tasks:} At the top-level, an Exo CUDA kernel is defined by a single nest of \lighttt{cuda\_tasks} loops.
Consecutive \myKeyA{tasks} (i.e. body of the inner-most \lighttt{cuda\_tasks} loop) are assigned to \myKeyA{top-level collectives} (clusters or CTAs) in a round-robin order.
Such loops correspond to the ``\myKeyA{minimal} communication \myKeyA{between} blocks'' (or clusters) portion of a typical CUDA algorithm.

\filbreak
Example:
{\color{lightttColor}
\begin{verbatim}
with CudaDeviceFunction(...):
    for y in cuda_tasks(0, size_y):
        for x in cuda_tasks(0, size_x):  # say size_x = 10
            foo
            bar
\end{verbatim}
}
Say there are 48 CTAs on the device.
Then CTA 0 executes the task (\lighttt{foo, bar}) for \lighttt{(y,x) = (0,0)}, CTA 1 executes the task for \lighttt{(y,x) = (0,1)} ... CTA 47 executes for \lighttt{(y,x) = (4,7)}, and we wraparound to CTA 0 for executing \lighttt{(y,x) = (4,8)}.

\filbreak
The \lighttt{cuda\_task} loop can basically do anything an ordinary \lighttt{seq} loop is, since they're lowered to sequential loops (plus some modular arithmetic for task distribution).

\filbreak
NOTE: the \lighttt{cuda\_tasks} loop does \textit{not} change the actor kind or collective unit.
It's simply a tool for allowing the user to define what order tasks are executed, which can have a huge impact on L2 hit rate.
TODO maybe support if statements nested in \lighttt{cuda\_tasks} loops, for imperfect tiling.

\filbreak
\mainKey{Cuda Threads:} Below the \lighttt{cuda\_tasks} loops, the \lighttt{cuda\_threads} loops may appear anywhere, although the valid usage is much more restrictive.

Each statement, including a for loop, is executed by a certain \myKeyA{collective lane} of cooperating threads (base case: one thread).
This number and arrangement of threads within this collective lane is statically described with a \myKeyA{collective unit} (e.g. thread, warp, warpgroup).

\filbreak
A \lighttt{cuda\_threads(lo, hi, unit=...)} loop statement takes the executing ``parent'' collective lane and subdivides it into collective lanes of the collective unit type specified.

\filbreak
The loop must have \lighttt{lo=0}, and \lighttt{hi} a constant.
Each iteration is assigned to a different collective lane, and the total number of threads required (iteration count times collective unit size) must not exceed the number available in the parent collective lane.
(More precisely, the number of tiles created by applying the argument collective unit as a tile operator to the parent collective unit must be at least the number of loop iterations; this is not described in this document).
However, it's allowed for there to be fewer threads used than available.

\filbreak
Such loops correspond to the ``\myKeyB{frequent} communication \myKeyB{within} blocks'' (or clusters) portion of a typical CUDA algorithm.

\filbreak
Example:
{\color{lightttColor}
\begin{verbatim}
with CudaDeviceFunction(blockDim=512):  # top-level collective: CTA/block of 512 T (threads)
    for foo in cuda_tasks(...):         # cuda_tasks: no effect on collective unit
        for w in cuda_threads(0, 16, unit=cuda_warp):
            # Parent collective lane (512 T) cut into 16 warps (32 T)
            warp_fn()  # executed by collective lane of 32 convergent threads
            for t in cuda_threads(0, 32, unit=cuda_thread):
                # Parent collective lane (32 T) cut into 32 single threads.
        for y in cuda_threads(0, 64, unit=8*cuda_thread):
            # Parent collective lane (512 T) cut into 64 * 8 T
            # Note 8 threads isn't any special hardware unit -- just defined by user ad-hoc
            for x in cuda_threads(0, 8, unit=cuda_thread):
                # Parent collective lane (8 T) cut into 8 single threads.
        for z in cuda_threads(0, 12, unit=cuda_warp):
            # Parent collective lane (512 T) cut into 12 warps (32 T)
            # with 128 threads wasted.
        for w in cuda_threads(0, 16, unit=64 * cuda_thread):
            # Invalid: need 16 * 64 T (1024 T) but parent collective lane has only 512 T.
\end{verbatim}
}

\filbreak
No \myKeyA{Store} or \myKeyA{Reduce} statement may appear anywhere the collective unit is not a single thread.
Generally, only synchronization, control flow (including nested parallel for), and custom instrs appear as valid statements in blocks of code with non-single-thread collective units.

\filbreak
\minorSub{Extra Info: Parallel-for Syntax}

This is a little bit strange, but the currently implemented Spork loop syntax is

\lighttt{\ \ for \greenBox{iter} in \violetBox{loop\_mode\_name}(\greenBox{lo}, \greenBox{hi}, \blueBox{kwarg1 = value1}, \blueBox{kwarg2 = value2}...):}

where \greenBox{iter}, \greenBox{lo}, and \greenBox{hi} are \textit{parsed} as Exo code (can refer to values created in the Exo object code), wheras the keyword argument \blueBox{values} are \textit{evaluated} as Python code (can refer to Python values in-scope at the time the \lighttt{proc} is parsed).
The loop mode object is constructed as

\filbreak
\lighttt{\ \ \violetBox{loop\_mode\_name}(\blueBox{kwarg1 = value1}, \blueBox{kwarg2 = value2}...)}\\
\lighttt{\ \ \# often no args, e.g. seq()}

\filbreak
Currently, loop modes can't take positional arguments, only keyword arguments.

%% \filbreak
%% \minorSub{Extra Info: Collective Units}

\filbreak
\mainSub{Memory Allocation}

Thoughts on how to extend tensors and memory types for CUDA.

\mainKey{Memory Types:} Of course we can re-use Exo's memory types to express different kinds of CUDA memory:
\begin{itemize}
  \item \lighttt{CudaBasicDeviceVisible}: All device-visible memory
  \item \lighttt{CudaGmem*}: Global memory (allocated by the CPU)
  \item \lighttt{CudaHost*}: Global memory sub-type; allocated by CPU; visible to CPU and CUDA device.
  \item \lighttt{CudaSmem*}: Shared memory
  \item \lighttt{CudaRmem*}: Registers
  \item \lighttt{GridConstant*}: CPU-to-CUDA constants
\end{itemize}

\filbreak
Since Exo uses \lighttt{issubclass} to check correct memory types, we may need to use multiple inheritance for more complicated cases (e.g. \lighttt{CudaHost*} types have to inherit from both \lighttt{DRAM} and \lighttt{CudaBasicDeviceVisible}).

\filbreak
The grid constants need a bit more explanation.
Unlike other memory types, grid constants don't have a fixed address, and are copied from the CPU to CUDA on kernel launch.
(These are the function parameters of a CUDA kernel).
We can model this in Exo as \lighttt{DRAM\_STACK} that the GPU is allowed to read (but not write) and is exempt from synchronization checking (due to the implicit copy protecting concurrent CPU/CUDA access).

\filbreak
\mainKey{Memory Actor Kind:} We need to add a new member function allowing us to query whether each \myKeyA{actor kind} has permission to read, write, and allocate each type of memory.
For backwards compatibility, by default, memory types will allow all operations by the CPU and none by any other actor kind.

\filbreak
\begin{itemize}
  \item \lighttt{CudaGmem*}: \lighttt{cpu} allocated; \lighttt{cuda\_classic} read/write
  \item \lighttt{CudaHost*}: \lighttt{cpu} allocated, read/write; \lighttt{cuda\_classic} read/write
  \item \lighttt{CudaSmem*}, \lighttt{CudaRmem*}: \lighttt{cuda\_classic} allocated, read/write
  \item \lighttt{GridConstant*}: \lighttt{cpu} allocated, read/write; \lighttt{cuda\_classic} read-only
\end{itemize}

\filbreak
These permissions don't apply to instrs, which are also the only way to interact with memory for actor kinds corresponding to CUDA async instructions.

\filbreak
\mainKey{Distributed Memory:} This is the trickiest part.
In Exo's scheduling system, we are concerned with the dataflow, so each tensor has to be declared with its true size
(e.g. \lighttt{f32[64,64]} for a $64 \times 64$ matrix tile).
However, in CUDA, the storage for such a tensor is often distributed across threads' registers.
For example, the $64 \times 64$ tile may be split into sub-tiles of size $16 \times 8$, with each tile living in the registers of one thread of a $4 \times 8$ grid of threads.

\filbreak
Since registers are allocated per-thread, the declared size of such a tensor in CUDA source code is that of the per-register sub-tile, and not the full tile size given by the Exo type.
We bridge this gap with distributed memory analysis.

\filbreak
Each memory type has a ``native collective unit'' corresponding to its visibility scope in hardware (1 thread for registers; CTA for shared memory; warp/warpgroup for tensor core matrix tiles).
If the collective unit at the point a tensor is allocated doesn't match the memory type's native collective unit, some of the leading dimensions are deduced to be ``distributed dimensions''.
The lowered allocation is based only on the remaining tensor dimensions, and may be a scalar, if all dimensions are distributed.

\filbreak
Example:

\graytt{\# Assume collective unit here is a CTA of 32 threads}\\
\graytt{\# \yellowBox{SMEM:} no distributed dimensions; native unit (CTA) matches collective unit}\\
\blacktt{A\_smem: f32[\blueBox{64, 16}] @ \yellowBox{CudaSmemLinear}}\\
\blacktt{B\_smem: f32[\blueBox{16, 64}] @ \yellowBox{CudaSmemLinear}}\\
\graytt{\# \yellowBox{Register} tile: \redBox{distributed}, \blueBox{non-distributed} dimensions}\\
\blacktt{C\_tile: f32[\redBox{4, 8}, \blueBox{16, 8}] @ \yellowBox{CudaRmemLinear}}\\
\graytt{\# NB compiler looks ahead to see usage is C\_tile[\redBox{mt,nt},\blueBox{ms,ns}]}\\
\graytt{\# Deduction based on following usage:}\\
\blacktt{~~for ks in seq(0, 16):} \graytt{\# irrelevant, ignored}\\
\blacktt{~~~~for \redBox{mt} in cuda\_threads(0, 4, unit=8*cuda\_thread):}\\
\blacktt{~~~~~~for \redBox{nt} in cuda\_threads(0, 8, unit=cuda\_thread):}\\
\graytt{~~~~~~~~\# Collective unit is now 1 thread -- matches native unit}\\
\graytt{~~~~~~~~\# Further index variables deduced as non-distributed,}\\
\graytt{~~~~~~~~\# whether they are from parallel or sequential loops}\\
\blacktt{~~~~~~~~for \blueBox{ms} in seq(0, 16):}\\
\blacktt{~~~~~~~~~~for \blueBox{ns} in seq(0, 8):}\\
\blacktt{~~~~~~~~~~~~C\_tile[\redBox{mt,nt},\blueBox{ms,ns}] += A\_tile[mt*16+ms,ks] * B\_tile[ks,nt*8+ns]}

\filbreak
The details of how this deduction is made are not included here, but crucially, know that we enforce that each thread only accesses its own tile of the distributed memory.
This same logic also applies to distributed shared memory (allocated per-CTA, but may hold a larger logical tile distributed across CTAs in a cluster).

\filbreak
\myTitle{Async Instructions \& Actor Kind}

\filbreak
\myTitle{Synchronization}

\end{document}
