% xelatex </dev/null spork_mini.tex

\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Exo GPU (Spork) Summary 2025-03-11}

I'll outline a sketch of how we'll extend Exo to target CUDA, then go over the core features of the language extension and their relationships to CUDA concepts:

\begin{enumerate}
  \item Async blocks \& actor kind: CUDA asynchronous kernel launch and asynchronous instructions
  \item Parallel loops: explict scheduling at each level of the CUDA thread hierarchy
  \item Synchronization statements
\end{enumerate}

\filbreak
\mainSub{Language Sketch}

We introduce two core concepts for the Spork Exo-GPU extension:
\begin{enumerate}
  \item The \myKeyA{actor kind}; broadly, ``what kind'' of hardware instructions Exo code will be lowered to.
    The most common actor kinds are \lighttt{cpu} (default) and \lighttt{cuda\_classic} (typical, synchronous CUDA instructions).
    We will also have actor kinds for categories of CUDA asynchronous instructions.
  \item The \myKeyA{collective unit}; broadly, how many cooperating threads execute a statement.
    Currently, each Exo statement is assumed to be executed by 1 thread, but for the GPU, some statements could be executed by multiple cooperating threads (e.g. a warp of 32 convergent threads).
\end{enumerate}
The \myKeyA{actor kind} and \myKeyA{collective unit} are statically analyzed for each statement in an Exo proc.

\filbreak
At multiple levels, we will have to model blocks of Exo code that run on a different timeline than the surrounding code:
\begin{enumerate}
  \item CPU code launching a CUDA kernel:\\
  subsequent CPU instructions don't wait for the launched kernel.
  \item CUDA code executing asynchronous CUDA instructions:\\
  subsequent CUDA instructions (on the same thread!) don't wait for the async instruction.
\end{enumerate}

\filbreak
\mainKey{Async Block:} Each level corresponds to a change in \myKeyA{actor kind}.
To do this, the user will wrap the subtree in an async block (overloaded \lighttt{with} statement).
For example, wrapping code with \lighttt{with CudaDeviceFunction(...)} lowers the wrapped code to CUDA (\lighttt{cuda\_classic} actor kind).

\filbreak
\mainKey{Parallel-for:} The user will then set the \myKeyA{collective unit} for code by using parallel-for loops.
Each loop splits the parent collective into a fixed number of smaller collectives, with one collective assigned to execute each ``loop iteration''. For example:

{\color{lightttColor}
\begin{verbatim}
# Assume current collective unit is 128 cuda threads
for w in cuda_threads(0, 4, unit=32 * cuda_thread):
    # Collective unit is now 32 cuda threads (warp)
    # Parent collective unit was split in 4 (perfectly matches 4 iterations)
    warp_instr(...) # Executed cooperatively by one warp
\end{verbatim}
}

\filbreak
\mainKey{Synchronization:}
We will \textit{not} be taking a fork-join approach here; this is too restrictive to model the highly pipelined and asynchronous instructions needed to write tensor processing kernels for the A100 and especially the H100.

\filbreak
Instead, we will provide (and task) the user with inserting explicit \myKeyA{synchronization statements}.
In the simple case, the user can use an all-to-all sync (all threads in a certain collective wait for all others),
but we will also provide finer-grained arrive/await synchronization (e.g. threads 128-383 wait for threads 0-31).

\filbreak
\mainSub{Code Example Sketch}
{\color{lightttColor}
\begin{verbatim}
def my_proc(...):
    # CPU code here (actor kind: cpu)
    with CudaDeviceFunction(blockDim = 128):  # 128 threads per block
        # CUDA code here (actor kind: cuda_classic)
        # Lowered to CUDA device function
        #
        # ... distribute work across blocks (to be explained)
            for y in cuda_threads(0, 16, unit=8 * cuda_thread):
                # 16 "iterations", executing collective unit = 8 cuda threads
                # So block of 128 threads is subdivided to 16 groups of 8 threads each.
                for x in cuda_threads(0, 8, unit=cuda_thread):
                    # 8 "iterations", executing collective unit = 1 cuda thread
                    # We further subdivided the 8 threads into 8 single threads

\end{verbatim}
}

\filbreak
\mainSub{Core Idea: S/M Equivalence}

The key idea of this design is to keep the parallelism constructs orthogonal to the core language, as if they were pragmas that only control code lowering.
In particular, these constructs don't change the dataflow.
All rewrite operations (and probably Chexo) will continue to interpret procs under \myKeyA{S-semantics} (single-threaded semantics), where
\begin{enumerate}
  \item parallel-for loops are treated like sequential loops
  \item async blocks are treated like \lighttt{if True}
\end{enumerate}

\filbreak
Corollary: current rewrites don't need to change, and any new rewrites that parallelize loops, add async blocks, or modify synchronization don't need any checking.

\filbreak
The final lowered proc will exhibit \myKeyB{M-semantics} (multi-threaded semantics), where parallel/async Exo constructs are lowered to actual parallel/async code.
To complete the chain-of-equivalence \textbf{(figure \ref{fig:chain})} between the original proc and the lowered CUDA code, we just have to prove that the \textit{final} proc interpreted under S-semantics gives the same result as said proc interpreted under M-semantics.
In other words, prove that sufficient synchronization exists to guarantee that the parallel program produces the same
output as-if it were forcibly executed sequentially (interpreted under \myKeyA{S-semantics}).

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=6mm]
\node(proc0) [normalnode] {$proc_0$\\\myKeyA{S-semantics}};
\node(proc1) [normalnode, right=of proc0] {$proc_1$\\\myKeyA{S-semantics}};
\node(procNS) [normalnode, right=of proc1] {$proc_N$\\\myKeyA{S-semantics}};
\node(procNM) [normalnode, right=of procNS] {$proc_N$\\\myKeyB{M-semantics}};
\node(cuda) [smallnode, right=of procNM] {CUDA C++};

\draw [arrow] (proc0) -- (proc1);
\draw [arrow, dotted] (proc1) to node(rewrites)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=7cm, minimum width=7cm, below=of rewrites, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg, xshift=-2cm] {Existing Exo Rewrites + \textbf{parallelism rewrites} (ignored in S-semantics)};
\node(sync) [normalnode, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Synchronization Checking};
\node(spork) [normalnode, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Codegen};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg] {Physically same proc, \textbf{different interpretation}};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (proc1);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\end{tikzpicture}
\caption{Chain-of-equivalence (S/M equivalence)} \label{fig:chain}
\end{figure*}

\filbreak
\myTitle{Parallelism \& Memory}

\mainSub{CUDA Thread \& Memory Hierarchy}

My core observation of the CUDA programminng model is that the mapping of the algorithm to the parallel hardware is very \myKeyA{explicit}, at least compared to HLS and other ``clever'' quasi-C compilers.
The CUDA programmer is tasked with choosing the number of threads to launch, programming individual threads (workload distribution), and ensuring correct synchronization.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=3.5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKeyA{GMEM}};
\node (gmeminfo) [widenode, right=of gmem] {\myKeyA{GMEM:} Global memory. \myKeyA{``slow''}, 10s of GB. Any thread in grid may access.};
\draw[line] (grid) -- (gmem);

\node (block0) [smallishnode, fill=violetBoxBg, below=of grid, xshift=-15mm, yshift=-16mm] {block};
\node (block1) [smallishnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallishnode, right=of block1, xshift=1.6cm] {block};

\node (smem0) [smallishsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallishsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallishsmemnode, above=of block2] {\myKeyB{SMEM}};
\node(smeminfo) [widenode, right=of smem2, yshift=-6mm] {\myKeyB{SMEM:} Shared memory. Per-CTA memory (L1 cache carveout). \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math.};

\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=100] ($(block2.west)+(0,0.6)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=330,in=100] (gridDim.north);

\node (thread0) [smallishnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallishnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallishnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (threadinfo) [widenode, right=of thread2, yshift=2mm, fill=violetBoxBg] {Threads in the same block (CTA) may synchronize explicitly with each other.};
\node (rmem0) [smallishnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallishnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallishnode, below=of thread2] {\textbf{RMEM}};
\node (rmeminfo) [widenode, right=of rmem2] {\textbf{RMEM:}\\Register ``memory''. 255 per thread.};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=150] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\caption{CUDA thread and memory hierarchy} \label{fig:hierarchy}
\end{figure*}

\filbreak
\mainKey{Kernel Launch:} CUDA threads are launched from the CPU, organized as a grid of \lighttt{gridDim}-many blocks (\myKeyB{CTA}: cooperative thread array), each consisting of \lighttt{blockDim}-many threads.
Both parameters are set by the programmer.

\filbreak
\mainKey{Work Distribution:} For the most part, from CUDA, each thread is programmed as if it were a scalar processor.
The programmer assigns work to each thread.

\filbreak
\mainKey{Memory Types:} The programmer manually manages moving data between different memory types in CUDA.
These form a hierarchy roughly parallel to the thread hierarchy \textbf{(figure \ref{fig:hierarchy})}.
Each thread block has its own private shared memory (\myKeyB{SMEM}), whose size and usage is controlled by the programmer.
This is typically used as a manual cache, or to communicate values between threads.

\filbreak
\mainKey{Programmer-managed Synchronization:}
  The programmer manages synchronization between threads.
  \textit{Within} a single block, there are constructs for synchronizing all or a subset of threads with another set of threads.
  \textit{Across} blocks, for the most part, there are no native synchronization mechanisms besides atomic reductions.
  Thus, typical algorithms have \myKeyB{frequent} communication \myKeyB{within} blocks, and \myKeyA{minimal} communication \myKeyA{between} blocks.
  With the H100 (Hopper generation), CTAs are now grouped into clusters, which allow some limited cross-CTA communication.

\filbreak
\minorSub{Extra Info: Thread Block Clusters}

Prior to the H100, shared memory was truly local to a single block.
The H100 introduces clusters of 2, 4, 8, or 16 thread blocks.
Threads within the same cluster support synchronizing with each other and can view each others' shared memory (\myKeyA{distributed shared memory}), with restrictions.
For the most part, this feature is set up assuming that all CTAs' shared memory layout is the same, and multicasting patterns are used.
Direct access to another within-cluster CTA's shared memory is possible, but expensive.

\filbreak
\mainSub{Kernel Launch}

\filbreak
\mainSub{Parallel Loops}

\filbreak
\mainSub{Memory Allocation}

\filbreak
\minorSub{Extra Info: Collective Units}

\filbreak
\myTitle{Async Instructions \& Actor Kind}

\filbreak
\myTitle{Synchronization}

\end{document}
