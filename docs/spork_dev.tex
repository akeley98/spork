\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColor, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

{ \LARGE Exo Dev 2025-01-15 \hfill \textbf{\textsf{Project Spork: EXO GPU}}}

\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage

\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myTitle{Project Goals}  % Moved into minipage due to lack of space

\begin{itemize}
\item Support generating mixed CPU and CUDA code from Exo
\item ``Minimal change'' to the core Exo language; avoid huge changes to analysis and rewrite
\item Support basic synchronization, e.g. \texttt{\_\_syncthreads} (all threads in a CTA wait for all other threads in a CTA).
\begin{itemize}
  \item Fork/join
\end{itemize}
\item ... but also support split barriers: set of threads waiting for an event triggered by a different set of threads or different program location
\begin{itemize}
  \item Move beyond fork/join model
\end{itemize}
\item Support asynchronous CUDA accelerator instructions (async copy, async tensor core matrix multiply-accumulate)
\item Not-too-conservative sync checking
\end{itemize}
\end{minipage} %
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Elevator Pitch, Programming Model:}
\begin{itemize}
  \item \texttt{with} = open device (CUDA) code block
  \begin{itemize}
    \item Kenneth: we need to discuss overloading
  \end{itemize}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = distribute work across threads/warps/etc.
  \item Add new \texttt{Fence}, \texttt{Arrive}, \texttt{Await} statements and \texttt{barrier} type for synchronization.
  \item Need to extend \texttt{Memory} and \texttt{@instr} to handle new CUDA memory types and async instructions.
\end{itemize}

\mySub{Elevator Pitch, Rewrites \& Safety:}

Key idea: most of Exo will still treat the code \textit{as if it were not parallelized}

\begin{itemize}
  \item \texttt{with} = \texttt{if True:}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = sequential for loop
  \item \texttt{Fence}, \texttt{Arrive}, \texttt{Await} = nothing
  \item Async \texttt{instr} = sync \texttt{instr}
\end{itemize}

Parallelism checks are part of the code lowering process.

\end{minipage} %
\newpage

\myTitle{Basic CUDA Features}

\begin{minipage}[t]{0.5\textwidth}\fixminipage

CPU code launches ``grid'': hierarchy of threads
\begin{itemize}
  \item warp: 32 threads
  \item warpgroup: 128 threads (new in H100)
  \item CTA (block): \texttt{blockDim} threads (user-set size)
  \item cluster: 1-16 cooperating blocks (user-set)
  \item grid: \texttt{gridDim} blocks (user-set size)
\end{itemize}
I call these \myKey{``collective units''} (includes base case, one thread), abusing Cutlass vocabulary

Core ``unit of parallelism'': CTA
\begin{itemize}
  \item Easy to synchronize within CTAs; hard between CTAs (Exo won't model this)
  \item Sub-collectives (thread, warp, warpgroup) within CTA can split and coalesce easily
  \item Different collective unit needed for different operations; programmer handles manually!
  \item \myKeyB{Frequent} communication \myKeyB{within} blocks
  \item \myKey{Minimal} communication \myKey{between} blocks
  \item Limited cross-block cooperation with atomics
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myKey{GMEM} (Global Memory): \myKey{``slow''}, 10s of GB
\begin{itemize}
  \item any thread in grid may access
\end{itemize}
\myKeyB{SMEM} (Shared Memory): 100s of KiB
\begin{itemize}
  \item per-CTA memory (L1 cache carveout)
  \item \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math
\end{itemize}
\textbf{RMEM} (Register ``Memory'') -- 255 per thread

\begin{tikzpicture}[node distance=3.5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKey{GMEM}};
\draw[line] (grid) -- (gmem);

\node (block0) [smallishnode, fill=violetBoxBg, below=of grid, xshift=-15mm, yshift=-16mm] {block};
\node (block1) [smallishnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallishnode, right=of block1, xshift=1.6cm] {block};

\node (smem0) [smallishsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallishsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallishsmemnode, above=of block2] {\myKeyB{SMEM}};

\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=100] ($(block2.west)+(0,0.6)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=330,in=100] (gridDim.north);

\node (thread0) [smallishnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallishnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallishnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (rmem0) [smallishnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallishnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallishnode, below=of thread2] {\textbf{RMEM}};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=150] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\end{minipage}

\newpage
\myTitle{CUDA Async Instructions}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
New accelerator instructions with the NVIDIA H100 (Hopper) are asynchronous.
They are issued by CUDA threads or warpgroups, but execution continues without waiting.
\begin{itemize}
  \item TMA: copy tensor tiles \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
  \item wgmma: tiled matrix multiply-accumulate
\end{itemize}
This means the following won't work if the highlighted statements are async:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$B \leftarrow Y$}
  \item $C \leftarrow C + AB$ (incorrectly assumes $A,B$ written to)
\end{enumerate}
Two async instructions issued by the same thread/warpgroup don't even complete in the same order relative to each other, e.g., the following is a data race:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$A \leftarrow Y$}
\end{enumerate}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
These also ignore all ``ordinary'' synchronization.
\begin{itemize}
\item Special barriers needed
\item Exception: still protected by ``stop the world'' barriers between grid launches on the same stream (I think?)
\end{itemize}

\mySub{Async Proxy}

In CUDA terminology, TMA and wgmma are considered to operate in the \myKey{async proxy}
\begin{itemize}
  \item \myKey{generic proxy}: most other instructions
  \item \myKey{tensorMap proxy}: we'll ignore this
  \item Memory not visible by default across proxies
\end{itemize}
PTX docs has \textit{tomes} about this; boils down to
\begin{itemize}
  \item Need a \texttt{fence.proxy.async} for generic$\to$async proxy data flow
  \item \textit{Nothing} needed for async$\to$generic
  \begin{itemize}
    \item (detail: the fence is required in both directions, but built in to the ``wait for async instruction'' machinery)
  \end{itemize}
\end{itemize}

\end{minipage}
\newpage
\myTitle{wgmma: Warpgroup Matrix Multiply-accumulate Async}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, this is really simple.
Each \texttt{wgmma.mma\_async} instruction is issued by a warpgroup (\myKey{128 aligned threads}), and computes
\begin{itemize}
  \item $D \leftarrow AB$ or \hfill (scale-d = 0)
  \item $D \leftarrow AB + D$ \hfill (scale-d = 1)
\end{itemize}
where
\begin{itemize}
  \item $D$ matrix tile is in \textbf{registers (RMEM)}
  \begin{itemize}
    \item Note: $D$ is implicitly synchronized for consecutive wgmma.mma!
  \end{itemize}
  \item $B$ matrix tile is in \myKeyB{shared memory (SMEM)}
  \item $A$ matrix tile is stored in either format
  \item Usually, scale-d = 0 only for the first iteration
\end{itemize}

The complexity for this feature comes from
\begin{itemize}
  \item Input/output format details \myKey{(huuuge mess)}
  \item Synchronization
\end{itemize}
%I'll just address the latter for now.
%Although Exo will eventually have to figure out how to model the full set of matrix formats.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ wgmma}

For \textbf{registers}, issue \texttt{wgmma.fence}

For \myKeyB{shared memory}, issue an async proxy fence
\begin{itemize}
  \item Not needed if TMA filled SMEM
\end{itemize}

\mySub{Synchronization wgmma $\to$ generic}

The completion mechanism is \myKey{``commit group''}

wgmma synchronization only occurs within a single warpgroup, with MMAs pipelined:

\begin{itemize}
\item \texttt{wgmma.fence} begins a ``pipeline stage''
\item \texttt{wgmma.mma} must appear within pipeline stage
\item \texttt{wgmma.commit\_group} ends a pipeline stage
\item ptxas f's you if it can't recognize the pattern  % fun fact: "f's" is used because no other verb fits without forcing a line break
\end{itemize}
\texttt{wgmma.wait\_group N} waits for the MMAs of the $N^{th}$ prior pipeline stage (0-indexed)

\end{minipage}

\begin{tikzpicture}[node distance=5mm]
\node (fence0) [smallnode] {fence};
\node (mma00) [smallishnode, right=of fence0, fill=violetBoxBg] {MMA\\scale-d=0};
\node (mma01) [smallnode, right=of mma00, fill=violetBoxBg] {MMA};
\node (commit0) [smallnode, right=of mma01, fill=violetBoxBg] {commit\\group};
\node (fence1) [smallnode, right=of commit0] {fence};
\node (mma10) [smallnode, right=of fence1] {MMA};
\node (mma11) [smallnode, right=of mma10] {MMA};
\node (commit1) [smallnode, right=of mma11] {commit\\group};
\node (wait1) [smallishnode, right=of commit1, fill=violetBoxBg] {wait\_group \textbf{1}};
\draw[arrow] (mma00.east) to (mma01.west);
\draw[arrow] (mma01.east) to (commit0.west);
\draw[arrow] (commit0.south) to[out=350,in=190] (wait1.south);
\end{tikzpicture}

\newpage
\myTitle{TMA: Tensor Memory Accelerator}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Handled with \texttt{cp.async.bulk.tensor} instructions.

\begin{itemize}
\item Async 1D-5D tile copy \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
\item \myKeyB{SMEM} tile: densely packed C-order matrix
\item \myKey{GMEM} tile: tile from big C-order matrix
\begin{itemize}
  \item \myKey{Predicated} and strided (16B aligned)
\end{itemize}
\item 16 byte aligned; cannot stride innermost dim
\end{itemize}

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (gmembig) [bignode, right=of smem, yshift=-3mm] {};
\node (gmem) [gmemnode, right=of smem, xshift=9mm] {\myKey{GMEM tile}};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);
\end{tikzpicture}

Need to encode GMEM matrix as \myKey{\texttt{CUtensorMap}}
\begin{itemize}
  \item \myKey{``fat pointer''}, GMEM pointer + info
  \item Encodes GMEM matrix size and strides
  \item Encodes SMEM tile size (\& swizzle mode)
  \item SMEM pointer NOT encoded
  \item Used on device, but must be encoded on the host CPU
  \begin{itemize}
    \item not 100\% true, look up tensorMap proxy if you want
  \end{itemize}
\end{itemize}

%Note: TMA can also be used without a \texttt{CUtensorMap} for literal array to array copies.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ TMA}

Async proxy fence required to see memory written by generic instructions.

\mySub{Synchronization TMA $\to$ generic}

If the TMA copies SMEM to GMEM, the completion mechanism is \myKey{commit\_group}

If the TMA copies GMEM to SMEM, the completion mechanism is \myKey{mbarrier} (split barrier)

\begin{itemize}
  \item Initialized in SMEM with arrive-count $A$
  \item Any number of threads can wait until both of the following complete:
  \begin{itemize}
    \item $A$-many threads arrive
    \item \texttt{tx-count}-many bytes copied by TMA
  \end{itemize}
  \item Re-usable (detail: requires parity tracking)
\end{itemize}
Usually, we nominate only \myKey{1 thread} to issue the TMA instruction.
Hence, we use a 1-to-many mbarrier to synchronize ($A = 1$).

NB mbarrier is usable without TMA (\texttt{tx-count=0})

\end{minipage}
\newpage
\myTitle{TMA Reduction}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
TMA takes an optional ``reduce'' operand when copying \myKeyB{SMEM}$\to$\myKey{GMEM}

Replaces \texttt{GMEM[\textit{slice}] = SMEM}\\
with \texttt{GMEM[\textit{slice}] \violetBox{+=} SMEM} (or another reduce op)

This reduction is \myKey{atomic} (\texttt{relaxed.gpu}) per element

Extremely OP Feature!

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (op) [normalnode, right=of smem, fill=violetBoxBg] {AtomicOp};
\draw [arrow] (smem) -- (op);
\node (gmembig) [bignode, below=of op] {};
\node (gmem) [gmemnode, below=of op, yshift=-6mm] {\myKey{GMEM tile}};
\draw [arrow] (gmem.east) to[out=0,in=0] (op.east);
\draw [arrow] (op.south) to[out=250,in=110] (gmem.north);
\end{tikzpicture}
\begin{verbatim}
def tma_reduce(gmem, smem, boxDim1...boxDimN,
               coord1...coordN):
    for i1 in seq(0, boxDim1):
        ... # N=1,2,3,4,5
        for iN in seq(0, boxDimN):
            gmem[i1 + coord1...
                iN + coordN] += smem[i1...iN]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Application: Split-$k$}

With TMA, we can parallelize a gemm on $k$ by assigning multiple thread blocks to each output tile,
assigning different $k$ ranges to each block, and using TMA to reduce into the output tile.

\mySub{Application: Backwards Pass}

The backwards pass reverses the direction of information flow.
Each original ``input'' tile $A_i,B_j$ receives gradient contributions from multiple ``output'' tiles $C_{i,j}$.
Parallelizing blocks on $C$ tiles requires reductions between thread blocks.

\begin{tikzpicture}[node distance=2mm]
\node(b0) [smallnode] {$B_0$};
\node(b1) [smallnode, right=of b0] {$B_1$};
\node(b2) [smallnode, right=of b1, fill=blueBoxBg] {$B_2$};
\node(bN) [smallnode, right=of b2, xshift=4mm] {$B_{no-1}$};
\draw [dotted] (b2) -- (bN);
\node(c00) [smallnode, below=of b0, yshift=-4mm] {$C_{0,0}$};
\node(c01) [smallnode, below=of b1, yshift=-4mm] {$C_{0,1}$};
\node(c02) [smallnode, below=of b2, yshift=-4mm, fill=blueBoxBg] {$C_{0,2}$};
\node(c0N) [smallnode, below=of bN, yshift=-4mm] {$C_{0,no-1}$};

\node(a0) [smallnode, left=of c00, xshift=-4mm] {$A_0$};
\node(a1) [smallnode, below=of a0, fill=redBoxBg] {$A_1$};
\node(aM) [smallnode, below=of a1, yshift=-4mm] {$A_{mo-1}$};
\draw [dotted] (a1) -- (aM);

\node(c10) [smallnode, below=of c00, fill=redBoxBg] {$C_{1,0}$};
\node(c11) [smallnode, below=of c01, fill=redBoxBg] {$C_{1,1}$};
\node(c12) [smallnode, below=of c02, fill=violetBoxBg] {$C_{1,2}$};
\node(c1N) [smallnode, below=of c0N, fill=redBoxBg] {$C_{1,no-1}$};

\node(cM0) [smallnode, below=of c10, yshift=-4mm] {$C_{mo-1,0}$};
\node(cM1) [smallnode, below=of c11, yshift=-4mm] {$C_{mo-1,1}$};
\node(cM2) [smallnode, below=of c12, yshift=-4mm, fill=blueBoxBg] {$C_{mo-1,2}$};
\node(cMN) [smallnode, below=of c1N, yshift=-4mm] {$C_{...}$};

\draw [arrow] (c00) -- (a0);
\draw [arrow] (c10) -- (a1);
\draw [arrow] (cM0) -- (aM);

\draw [dotted] (c10) -- (cM0);
\draw [dotted] (c11) -- (cM1);
\draw [dotted] (c12) -- (cM2);
\draw [dotted] (c1N) -- (cMN);

\draw [dotted] (c02) -- (c0N);
\draw [dotted] (c12) -- (c1N);
\draw [dotted] (cM2) -- (cMN);
\draw [dotted] (c12) -- (cMN);

\draw [arrow] (c00) -- (b0);
\draw [arrow] (c01) -- (b1);
\draw [arrow] (c02) -- (b2);
\draw [arrow] (c0N) -- (bN);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- Async Block, Parallel For}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Open an ``async block'' to switch from compiling CPU code to accelerator code.
\begin{itemize}
  \item Syntax: \texttt{with \textit{async\_config}: \textit{body}}
  \item Nomenclature?\\Idea is this block is ``asynchronous'' in some way relative to the parent code
\end{itemize}
\vspace{12mm}
Top level (outside async block): CPU code

Middle level: \myKey{synchronous} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaDeviceFunction(blockDim,...):}
  \item \myKey{Parallel for} allowed at this level
\end{itemize}

Bottom level: \myKey{async} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaAsync(...):}
  \item more detail later!
\end{itemize}
\vspace{12mm}
Future work could build on this syntax to target non-CUDA accelerators.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
We define parallel for loops for each level of the CUDA thread hierarchy;
\texttt{for \_ in cuda\_\{units\}:}
\begin{itemize}
  \item i.e. each \myKey{collective unit}
  \item \texttt{\{units\}} = \texttt{clusters}, \texttt{blocks}, \texttt{warpgroups}, \texttt{warps}, \texttt{threads}
  \item Generalize \texttt{seq}, \texttt{par} to \myKey{loop mode}
\end{itemize}
Must be \myKey{strictly nested}
\begin{itemize}
  \item levels except \texttt{blocks} may be skipped
  \item exception: \myKey{multidimensional} iteration\\(defined by directly nested same-unit loops)
\end{itemize}
Each iteration executed by a \myKey{collective lane}

Parallel for loops + hardware resources define \myKey{collective scope} (formerly parlane, parscope)

\begin{verbatim}
with CudaDeviceFunction(blockDim=256):
    for b in cuda_blocks(...):
        for y in cuda_threads(0, 16):
            for x in cuda_threads(0, 16):
                # 16 x 16 iteration space
                # mapped to 256 threads
                with CudaAsync(...):
                    # Async instructions
\end{verbatim}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- SpecializeCollective (warp specialization)}

Parallel for loops map the work over all available hardware resources of the parent collective lane:
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        # collective scope is block_count-many blocks
        # collective lane is 1 block of blockDim = 5 * 128 threads

        for wg in cuda_warpgroups(0, 5):
            # 5 iterations mapped to 5 warpgroups available in block
            # collective lane is now a warpgroup

            for w in cuda_warps(0, 4):
                # 4 iterations mapped to 4 warps available in warpgroup
\end{verbatim}
Nest inside \myKey{\texttt{with SpecializeCollective}} to override this; assign \myKey{different work} to \myKey{different warps}
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        with SpecializeCollective(cuda_warpgroup, 0, 1):        # Overloaded with statement
            for wg in cuda_warpgroups(0, 1):
                # This code mapped to warpgroup 0
        with SpecializeCollective(cuda_warpgroup, 1, 5):        # Overloaded with statement
            for n_wg in cuda_warpgroup(0, 2):
                for m_wg in cuda_warpgroup(0, 2):
                    # 2 x 2 space mapped to warpgroups 1, 2, 3, 4
\end{verbatim}

\newpage
\myTitle{Exo Syntax -- Synchronization 1/2}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
The simplest category of barrier is what I term a \myKey{Fence}; this specifies that all threads in the executing \myKey{collective lane} wait for each other.

\begin{verbatim}
with CudaDeviceFunction(...):
    for b in cuda_blocks(...):
        # Executing collective lane is block

        for t in cuda_threads(...):
            do_something_A()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)

        for w in cuda_warps(...):
            for t in cuda_threads(0, 32):
                do_something_B()
            # Executing collective lane is warp
            # so this Fence is __syncwarp()
            Fence(cuda_sync, cuda_generic)
            do_something_warp()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
Ignore \texttt{cuda\_sync, cuda\_generic} for now (actor kinds, to be explained)

In Exo, we are \myKey{explicit} about which operations are done by which \myKey{collective units} (e.g. block / coalesced warp), unlike in CUDA C++ where it is the \myKey{user's responsibility} to do this correctly, else get hangs or UB.

This can cause one block of CUDA code to map to multiple blocks of Exo code, e.g. the code on the left is equivalent to CUDA code
\begin{verbatim}
__global__ void <function name>()
{
    do_something_A();   // unit =     thread
    __syncthreads();    // unit = block
    do_something_B();   // unit =     thread
    __syncwarp();       // unit =   warp
    do_something_warp();// unit =   warp
    __syncthreads();    // unit = block
}
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Exo Syntax -- Synchronization 2/2, Actor Kind}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, an async block instructs the compiler to lower to a \myKey{different category} of instructions with \myKey{different synchronization} requirements

For now, informally, this categorization is the \myKey{actor kind}, which is one of the following:

\vspace{12mm}

Top level code:
\begin{itemize}
  \item \texttt{cpu}
\end{itemize}

\texttt{with CudaDeviceFunction(...):}
\begin{itemize}
  \item \texttt{cuda\_sync}
  \begin{itemize}
    \item Synchronous CUDA instructions
  \end{itemize}
\end{itemize}

\texttt{with CudaAsync(\textit{actor\_kind}):}
\begin{itemize}
  \item \texttt{non\_bulk\_cp\_async}
  \begin{itemize}
    \item Ampere async copy\\(uses generic proxy; not discussed)
  \end{itemize}
  \item \texttt{tma\_gmem\_to\_smem\_async}
  \item \texttt{tma\_smem\_to\_gmem\_async}
  \item \texttt{wgmma\_async}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Fence}

Parameterize with actor kind
\begin{verbatim}
  Fence(A1, A2)
\end{verbatim}
Within the executing collective lane, prior actions of actor kind \texttt{A1} happen before subsequent actions of actor kind \texttt{A2}

\mySub{Split Barrier}

Declare variable of \texttt{barrier} type
\begin{verbatim}
  <barrier var> : barrier
  Arrive(A1, <barrier var>)
  Await(<barrier var>, A2)
\end{verbatim}

Actions of actor kind \texttt{A1} prior to the $N^{th}$ arrive happen before actions of actor kind \texttt{A2} subsequent to the $N^{th}$ await.

We use orthogonal syntax but \myKey{not all} collective unit + actor kind combinations supported.

\mySub{Synthetic Actor Kind}

Additional actor kinds only used to parameterize synchronization,
e.g. \texttt{cuda\_all} (all CUDA instructions), \texttt{cuda\_generic} (generic proxy)

\end{minipage}

\newpage
\myTitle{Exo Syntax -- CudaAsync Block}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
As mentioned, nested within \texttt{CudaDeviceFunction} async block (actor kind \texttt{cuda\_sync})

Syntax, \texttt{with CudaAsync(\textit{actor\_kind})}

Often, there is \myKey{nontrivial interaction} between the async instructions and related synchronization
\begin{itemize}
\item Example: wgmma ``pipeline stage'' must appear in fixed pattern (that gets ptxas's vaunted stamp of approval)
\item Example: TMA \texttt{expect-tx} needs to be set to the number of bytes copied
\end{itemize}
\vspace{12mm}
For certain actor kinds, we \myKey{could} require
\begin{itemize}
\item \myKey{prologue synchronization}: first body stmt
\item \myKey{epilogue synchronization}: last body stmt
\end{itemize}
which the backend will compile non-trivially with respect to the async block's contents.

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{wgmma pipeline stage}
\begin{verbatim}
with CudaAsync(wgmma_async):
    # Prologue synchronization
    # wgmma.fence
    # NB wgmma_rmem is a synthetic actor kind
    Fence(wgmma_rmem, wgmma_rmem_async)

    # ... MMA instr ...

    # Epilogue synchronization
    # wgmma.commit_group
    Arrive(wgmma_async, <barrier var>)
\end{verbatim}
\mySub{TMA GMEM$\to$SMEM}
\begin{verbatim}
with CudaAsync(tma_to_smem_async):
    # ... TMA instr ...

    # Epilogue synchronization
    # Calculates expect-tx = bytes copied
    # (futher surprise, in the generated
    # CUDA C++, the Arrive is moved before
    # the TMA instructions!!!)
    Arrive(tma_to_smem_async,
           <barrier var>)
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Syntax Example}
\begin{verbatim}
with CudaDeviceFunction(blockDim=384):
    # blockDim 384 = 3 warpgroups

    for yo in cuda_blocks(0, 24):
        for xo in cuda_blocks(0, 36):
        # Grid of 36 x 24 blocks (CTAs)

        with SpecializeCollective(cuda_warpgroup, 1, 3):
            for w in cuda_warpgroups(0, 2):
                # Skipped warpgroup 0 (specialize); 2 iters mapped to warpgroups 1 and 2
                bar: Barrier

                for yi in cuda_threads(0, 16):
                      for xi in cuda_threads(0, 8):
                          # 8 x 16 iteration space, each iter mapped to 1 of 128 threads

                with CudaAsync(wgmma_async):
                    # Now targetting wgmma; this is at warpgroup scope
                    # since wgmma requires coalesced warpgroups
                    Fence(wgmma_rmem, wgmma_async_rmem)  # prologue synchronization
                    for k in seq(0, 8):                  # ordinary sequential loop
                        wgmma_mma(...)
                    Arrive(bar, wgmma_async)             # epilogue synchronization
\end{verbatim}

\newpage
\myTitle{Contrast with prior work: Triton}

\begin{minipage}[t]{0.7\textwidth}\fixminipage
Triton is another Python AST $\to$ CUDA language

In my terminology, the ``collective unit'' for a Triton function is a block.
\begin{itemize}
  \item One block executes one ``call'' of a Triton function.
\end{itemize}

Triton's responsibility: \textbf{automatically} distribute the work \myKeyB{within} one block to sub-block units (mostly threads) and automatically synchronize.
\begin{itemize}
  \item Recall: synchronization within a block is \myKeyB{frequent}
  \item Minimal control over memory movement, register/SMEM allocation
  \item Honestly, not a bad model though
\end{itemize}

User responsibility: manually distribute work \myKey{between} blocks.
\begin{itemize}
  \item Recall: synchronization between blocks is \myKey{minimal}
  \item Often one output tile assigned per block
\end{itemize}

With Exo, we're trying to build something much more \textbf{imperative}
\begin{itemize}
  \item Control (and predict!) memory and register pressure
  \item Fine grained control over each level of CUDA thread hierarchy
  \item Explicit use of accelerator instructions
\end{itemize}
\end{minipage}

\vspace{6mm}
\hfill(Also, Triton generates IR and Exo GPU will generate C and CUDA C++)

\newpage
\myTitle{10 Miles Up: Synchronization Checking}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Internally, Exo will continue to be a fundamentally sequential programming system
\begin{itemize}
  \item Expose parallel for, etc. to the user
  \item ... but rewrite rules, etc. will accept the correctness of these \myKey{without proof}
\end{itemize}
Each Exo proc may be interpreted in two ways:
\begin{itemize}
  \item \myKey{S-semantics} (Single-threaded)\\parallel for = sequential for,\\ignore async blocks
  \item \myKeyB{M-semantics} (Multi-threaded)\\Treat parallel for, async blocks correctly
\end{itemize}
We will interpret procs under \myKey{S-semantics}\\until the very end, at code lowering time

Prove interpretation under \myKey{S-semantics}\\and \myKeyB{M-semantics} will give the same results

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
Strat: reduce parallelism to checking assertions on conceptually \myKey{sequential} programs

Interpretation
\begin{itemize}
  \item $\Sigma$: environment: values of variables
  \item $\Sigma^S$: synchronization environment (\myKey{SyncEnv})
\end{itemize}
\vspace{8mm}
The SyncEnv will track the ``visibility'' (which threads can see) for \textit{each} read and write done to each variable -- this changes with synchronization

Prove that sufficient synchronization exists such that each read under \myKeyB{M-semantics} pairs with the same write as under \myKey{S-semantics}
\end{minipage}
\vfill
\begin{tikzpicture}[node distance=8mm]
\node(proc0) [normalnode] {$proc_0$\\\myKey{S-semantics}};
\node(proc1) [normalnode, right=of proc0] {$proc_1$\\\myKey{S-semantics}};
\node(procNS) [normalnode, right=of proc1, xshift=+6mm] {$proc_N$\\\myKey{S-semantics}};
\node(procNM) [normalnode, right=of procNS] {$proc_N$\\\myKeyB{M-semantics}};
\node(cuda) [smallnode, right=of procNM] {CUDA C++};

\draw [arrow] (proc0) -- (proc1);
\draw [arrow, dotted] (proc1) to node(rewrites)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, below=of rewrites, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Existing Exo Rewrites};
\node(sync) [normalnode, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Synchronization Checking};
\node(spork) [normalnode, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Compiler};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (proc1);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\end{tikzpicture}

\newpage
% Title moved out of desperation
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\vspace{-5mm}
\myTitle{Actor Kind, Actor Signature}

\vspace{4mm}
Imagine a dynamic trace of an Exo-GPU proc
\begin{itemize}
  \item Sequential, since core Exo is sequential
  \item ... but we need to annotate which thread would do each action in M-semantics
  \item ... and if it's an async instruction?
  \item ... to prove synchronization correctness
\end{itemize}
\myKey{Thread Index} = ``who'' did the access

\myKey{Actor Signature} = ``how'' the access was done

\myKey{Actor Kind}
\begin{itemize}
  \item Informal: categorization of instructions
  \item Formal: \myKey{set} of allowed \myKey{actor signatures}
  \item 1-to-many: a single accelerator can perform accesses with different sync requirements
\end{itemize}

Example: actor kind \texttt{wgmma\_async}
\begin{itemize}
  \item \texttt{sig\_wgmma\_smem}: \myKeyB{SMEM} ($B$, maybe $A$)
  \item \texttt{sig\_wgmma\_rmem\_a}: \textbf{RMEM} $A$
  \item \texttt{sig\_wgmma\_rmem\_d}: \textbf{RMEM} $D$
  \begin{itemize}
    \item Only $D$ (accumulator) implicitly sync'd
  \end{itemize}
\end{itemize}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myKey{sigthread} = (thread index, actor signature) pair
\begin{itemize}
  \item Conceptual sub-thread of each thread
  \item ``signs'' each action done in a dynamic trace
\end{itemize}
\vspace{2mm}
\begin{verbatim}
with CudaAsync(tma_to_smem_async):
    # Only thread 0 does TMA (pseudocode)
    for tid in cuda_threads(0, 1):
        SMEM[0:N] = GMEM[c:c+N]
# ...
for tid in cuda_threads(0, N):
    Y[tid] = SMEM[tid]
\end{verbatim}
\vspace{-4mm}
\begin{align*}
    & {\color{blueBoxFg}\textit{[[ thread 0 kicks off TMA ]]}} \\
    \text{SMEM[$0$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+0$]} \\
    \text{SMEM[$1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+1$]} \\
    & ... \\
    \text{SMEM[$N-1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+N-1$]} \\
    & {\color{blueBoxFg}\textit{[[ thread $x$ reads SMEM[$x$] ]]}} \\
    \text{Y[$0$]} & \xleftarrow{\text{(0, sig\_cuda\_sync)}} \text{SMEM[$0$]} \\
    \text{Y[$1$]} & \xleftarrow{\text{(1, sig\_cuda\_sync)}} \text{SMEM[$1$]} \\
    & ... \\
    \text{Y[$N-1$]} & \xleftarrow{\text{(N-1, sig\_cuda\_sync)}} \text{SMEM[$N-1$]} \\
\end{align*}
\end{minipage}
\newpage
\myTitle{SyncEnv \hfill Correctness concept, not runtime state}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
The SyncEnv associates \myKey{visibility records} with variables, containing state
\begin{itemize}
  \item $V_S$: sync visibility set: set of \myKey{sigthreads}
  \begin{itemize}
    \item sigthreads that can ``see'' the read/write
  \end{itemize}
  \item $V_A$: async visibility set: set of \myKey{sigthreads}
  \begin{itemize}
    \item to model async instructions, later
  \end{itemize}
  \item $A_O$: original \myKey{actor signature}
\end{itemize}

One write visibility record for last write, and one read visibility record added for \myKey{each} previous read

With $(t: \mathsf{thread\_index}, a: \mathsf{actor\_signature})$ being the sigthread doing the access, \myKey{initialize}
\begin{itemize}
  \item $V_S$ = $\{(t,a)\}$ if the instruction is synchronous, otherwise $\{\}$
  \item $V_A$ = $\{(t,a)\}$
  \item $A_O$ = $a$
  \item Invariant: $V_S \subseteq V_A$
\end{itemize}
TODO
\begin{itemize}
  \item support for atomic reductions
  \item deleting ``old'' visibility records
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\ \\ % formatting is screwed without this
\begin{tikzpicture}[node distance=6mm]
\node(SyncEnv) [normalnode] {$\Sigma^S$: SyncEnv};
\node(vars) [normalnode, below=of SyncEnv] {variable};
\node(reads) [normalnode, below=of vars, xshift=-25mm, minimum width=4cm, text width=4cm, draw=redBoxFg, text=redBoxFg, fill=redBoxBg] {read visibility records\\\textit{red $\equiv$ ``read''}};
\node(writes) [normalnode, below=of vars, xshift=+25mm, minimum width=4cm, text width=4cm, draw=blueBoxFg, text=blueBoxFg, fill=blueBoxBg] {write visibility record\\\textit{blue $\equiv$ ``not read''}};
\node(record) [normalnode, below=of reads, xshift=+25mm] {visibility record};
\node(VA) [smallnode, below=of record] {$V_A$};
\node(VS) [smallnode, left=of VA] {$V_S$};
\node(AO) [smallnode, right=of VA] {$A_O$};
\node(sigthread) [normalnode, below=of VS] {sigthread};
\node(tid) [normalnode, below=of sigthread] {thread index};
\node(signature) [normalnode, right=of tid] {actor signature};

\draw [arrow, draw=keyColorB] ($(SyncEnv.south)$) -- ($(vars.north)$);
\draw [arrow, draw=keyColorB] ($(SyncEnv.south) - (0.2, 0)$) -- ($(vars.north) - (0.2, 0)$);
\draw [arrow, draw=keyColorB] ($(SyncEnv.south) + (0.2, 0)$) -- ($(vars.north) + (0.2, 0)$);

\draw [arrow] (vars) -- (reads);
\draw [arrow] (vars) -- (writes);

\draw [arrow, draw=keyColorB] ($(reads.south) - (0.4, 0)$) -- ($(record.north) - (1.2, 0)$);
\draw [arrow, draw=keyColorB] ($(reads.south) + (0.4, 0)$) -- ($(record.north) - (0.4, 0)$);
\draw [arrow, draw=keyColorB] ($(reads.south) + (1.2, 0)$) -- ($(record.north) + (0.4, 0)$);
\draw [arrow] ($(writes.south) - (0.4, 0)$) -- ($(record.north) + (1.2, 0)$);

\draw [arrow] (record) -- (VS);
\draw [arrow] (record) -- (VA);
\draw [arrow] (record) -- (AO);

\draw [arrow, draw=keyColorB] ($(VS.south) - (0.2, 0)$) -- ($(sigthread.north) - (0.2, 0)$);
\draw [arrow, draw=keyColorB] ($(VS.south)$) -- ($(sigthread.north)$);
\draw [arrow, draw=keyColorB] ($(VS.south) + (0.2, 0)$) -- ($(sigthread.north) + (0.2, 0)$);

\draw [arrow, draw=keyColorB] ($(VA.south) - (0.5, 0)$) -- ($(sigthread.north) - (0.5, 0)$);
\draw [arrow, draw=keyColorB] ($(VA.south)$) -- ($(sigthread.north)$);
\draw [arrow, draw=keyColorB] ($(VA.south) + (0.5, 0)$) -- ($(sigthread.north) + (0.5, 0)$);

\draw [arrow] (sigthread.south) to (tid);
\draw [arrow] (sigthread.south) to[out=315, in=135] ($(signature.north) - (0.1, 0)$);
\draw [arrow] (AO) to[out=270, in=90] ($(signature.north) + (0.1, 0)$);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{SyncEnv Barrier Visibility Sets}

\begin{minipage}[t]{0.48\textwidth}\fixminipage

Each \texttt{Fence($A_1$:\textsf{actor-kind}, $A_2$:\textsf{actor-kind})} statement is executed by a \myKey{collective lane}, itself defined by a range of threads $T$

\begin{itemize}
  \item e.g. warpgroup 2 may have $T = [256, 383]$
  \item Define \myKey{first visibility set}: $V_1 = T \times A_1$
  \begin{itemize}
    \item set of sigthreads ``signalling the fence''
  \end{itemize}
  \item Define \myKey{second visibility set}: $V_2 = T \times A_2$
  \begin{itemize}
    \item set of sigthreads ``waiting for the fence''
  \end{itemize}
\end{itemize}

A fence can be \myKey{transitive} or \myKey{non-transitive} (TODO elaborate)

Informally, the \myKeyB{synchronizes-with} relation associates prior reads/writes (\myKey{visibility records}) with the fences that ``\myKey{protect}'' them.

Formally, when we interpret a \texttt{Fence} statement, we say that a visibility record \myKeyB{synchronizes-with}
\begin{itemize}
  \item transitive fences: if \violetBox{$V_A \cap V_1$} non-empty
  \item non-transitive fences:\\if \violetBox{$V_A \cap V_1 \cap (\mathbb{N} \times A_O)$} non-empty
  \begin{itemize}
    \item Recall, $A_O$: original actor signature
  \end{itemize}
\end{itemize}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
%% Example, suppose warpgroup 2 of block 0 executes \texttt{wgmma.fence}
%% \begin{itemize}
%%   \item \texttt{Fence(wgmma\_rmem, wgmma\_async\_rmem)}\\(in Exo)
%%   \item $T = [256, 383]$
%%   \item $V_1 = T \times \texttt{wgmma\_rmem}$
%%   \item $V_2 = T \times \texttt{wgmma\_async\_rmem}$
%% \end{itemize}

\mySub{Side Notes}

Recall that actor kinds are a set of actor signatures, hence the typing is correct
\begin{itemize}
  \item $T$: set of thread indices
  \item $A_i$: set of actor signatures
  \item $T \times A_i$: set of\\(thread index, actor signature): \myKey{sigthread}
\end{itemize}

\vspace{6mm}
Ignore split barriers (\texttt{Arrive}, \texttt{Await}) today
\begin{itemize}
  \item See spork.pdf
  \item Essentially, \texttt{Arrive} gives $V_1$, \texttt{Await} gives $V_2$
\end{itemize}
\end{minipage}
\newpage
\begin{minipage}[t]{0.42\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{SyncEnv Barrier Effects}

\texttt{Fence} effect: do, \myKey{for each} visibility record in $\Sigma^S$ that \myKeyB{synchronizes-with} the fence,
\begin{itemize}
  \item $V_A \leftarrow V_A \cup V_2$
  \item $V_S \leftarrow V_S \cup V_2$
\end{itemize}
Really expensive if interpreted literally!

\mySub{Example Code}

\myKey{Block 0}: threads $T = [0, 255]$

\myKey{Block 1}: threads $T = [256, 511]$

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        smem : f32[256] @ CudaSmem
        for t in cuda_threads(0, 256):
            smem[t] = gmemRead[t]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
        Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}

\myKey{S-semantics}: interpret \texttt{b}-loop sequentially
\begin{itemize}
  \item Suppose $b=0$ already done
\end{itemize}

For block 1 ($b = 1$), let's sketch before and after
\violetBox{\texttt{Fence(cuda\_sync, cuda\_generic)}}
\begin{itemize}
  \item $V_1 = [256, 511] \times \texttt{cuda\_sync}$
  \item $V_2 = [256, 511] \times \texttt{cuda\_generic}$
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\textwidth}\fixminipage
\mySub{$\Sigma^S$ Before}

\begin{tikzpicture}[node distance=2mm]
\node(gmem0) [normalnode] {\texttt{gmemRead[0]}};
\node(gmem255) [normalnode, below=of gmem0, yshift=-3mm] {\texttt{gmemRead[255]}};
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, below=of smem0, yshift=-3mm] {\texttt{smem[255]}};

\node(b0) [widenode, right=of gmem0, xshift=6mm, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [0,255] \times \texttt{cuda\_generic}$\\\textit{reads from previous iter $b=0$}};
\node(r256) [widenode, below=of b0, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = \{ 256, \texttt{sig\_cuda\_sync} \}$};
\node(r511) [widenode, below=of r256, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = \{ 511, \texttt{sig\_cuda\_sync} \}$};
\node(w256) [widenode, below=of r511, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = \{ 256, \texttt{sig\_cuda\_sync} \}$};
\node(w511) [widenode, below=of w256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = \{ 511, \texttt{sig\_cuda\_sync} \}$};

\draw[dotted] (gmem0) -- (gmem255);
\draw[dotted] (smem0) -- (smem255);
\draw[line, draw=redBoxFg] (gmem0.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem0.east) -- (r256.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (r511.west);
\draw[line, draw=blueBoxFg] (smem0.east) --(w256.west);
\draw[line, draw=blueBoxFg] (smem255.east) --(w511.west);
\end{tikzpicture}

\mySub{$\Sigma^S$ After}

\begin{tikzpicture}[node distance=2mm]
\node(gmem0) [normalnode] {\texttt{gmemRead[0]}};
\node(gmem255) [normalnode, below=of gmem0, yshift=-3mm] {\texttt{gmemRead[255]}};
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, below=of smem0, yshift=-3mm] {\texttt{smem[255]}};

\node(b0) [widenode, right=of gmem0, xshift=6mm, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [0,255] \times \texttt{cuda\_generic}$\\\textit{unchanged as $V_A \cap V_1$ empty}};
\node(r256) [widenode, below=of b0, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic}$\\\textit{read visibility set unioned with $V_2$}};
\node(w256) [widenode, below=of r256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic}$\\\textit{write visibility set unioned with $V_2$}};

\draw [dotted] (gmem0) -- (gmem255);
\draw [dotted] (smem0) -- (smem255);
\draw[line, draw=redBoxFg] (gmem0.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem0.east) -- (r256.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (r256.west);
\draw[line, draw=blueBoxFg] (smem0.east) --(w256.west);
\draw[line, draw=blueBoxFg] (smem255.east) --(w256.west);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{SyncEnv Barrier Example 2}

\begin{minipage}[t]{0.55\textwidth}\fixminipage
Suppose the block continues on to uses \texttt{smem} to stage a\\
\myKeyB{SMEM}$\to$\myKey{GMEM} TMA copy

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        smem : f32[256] @ CudaSmem
        for t in cuda_threads(0, 256):
            smem[t] = gmemRead[t]
        Fence(cuda_sync, cuda_generic)

\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
        # Only 0th thread per block does TMA
        for t in cuda_threads(0, 1):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            # fence.proxy.async
            Fence(cuda_sync, tma_to_gmem_async)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
            with CudaAsync(tma_to_gmem_async):
                # (placeholder) TMA instr
                gmemWrite[...] = smem[0:256]
\end{verbatim}
\end{mdframed}

For block \texttt{b=1}, \texttt{blockDim=256},\\
\violetBox{\texttt{Fence(cuda\_sync, tma\_to\_gmem\_async)}} has
\begin{itemize}
  \item $V_1 = \{(256, \texttt{sig\_cuda\_sync})\}$
  \item $V_2 = \{(\mathbf{256}, \textbf{\texttt{sig\_tma\_to\_gmem}})\}$
  \item Note, \texttt{sig\_cuda\_sync} $\in$ \texttt{cuda\_generic}
\end{itemize}
\blueBox{$(256, \texttt{sig\_cuda\_sync}) \in [256, 511] \times \texttt{cuda\_generic}$}
\end{minipage}
\hfill
\begin{minipage}[t]{0.43\textwidth}\fixminipage
\vspace{-8mm}
\begin{tikzpicture}[node distance=2mm]
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, right=of smem0, xshift=8mm] {\texttt{smem[255]}};
\draw[dotted] (smem0) to node(smemdots){} (smem255);

\node(rtma) [widenode, below=of smemdots, fill=redBoxBg, draw=redBoxFg, text=redBoxFg, yshift=-10mm] {$V_S = \{\}$ \textit{(TMA \textbf{async} read)}\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem})\}$};
\node(w256) [widenode, minimum width=8cm, text width=8cm, below=of rtma, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic} \cup \{(\mathbf{256}, \textbf{\texttt{sig\_tma\_to\_gmem}})\}$\\\textit{unioned with $V_2$ (bold part)}};
\node(wtma) [widenode, below=of w256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg, yshift=-8mm] {$V_S = \{\}$ \textit{(TMA \textbf{async} write)}\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem})\}$};
\node(gmemWrite) [normalnode, below=of wtma, yshift=-8mm] {\texttt{gmemWrite[...]}};

\draw[line, draw=redBoxFg] (smem0) -- (rtma);
\draw[line, draw=redBoxFg] (smem255) -- (rtma);
\draw[line, draw=blueBoxFg] (smem0) to[out=215,in=105] ($(w256.north) - (3.5, 0)$);
\draw[line, draw=blueBoxFg] (smem255) to[out=325,in=75] ($(w256.north) + (3.5, 0)$);
\draw[line, draw=blueBoxFg] (gmemWrite) to[] (wtma);
\end{tikzpicture}

Recall,
\begin{itemize}
\item $V_S$: \myKey{sync visibility set}
\item $V_A$: \myKey{async visibility set}
\end{itemize}
\end{minipage}

\newpage
\myTitle{Synchronization Checking}

Each read/write access is done with a certain sigthread $(t, a)$

The checks are done \myKey{prior to any changes} to $\Sigma^S$ (SyncEnv)

For a read/write visibility record $x$, denote its \myKey{sync visibility set} as $V_S^x$

\mySub{Checking on Read}

\myKey{RAW} check: Check $(t,a) \in V_S^w$, where $w$ is the \blueBox{write visibility record}.\\
If so, we say that the write recorded by $w$ is \myKeyB{visible-to} sigthread $(t,a)$.

\mySub{Checking on Write}

\myKey{WAW} check: Check write $w$ is \myKeyB{visible-to} $(t,a)$

\myKey{WAR} check: For each \redBox{read visibility record} $r$, check that $(t,a) \in V_S^r$.\\
If so, we say that the read recorded by $r$ is \myKeyB{visible-to} sigthread $(t,a)$.

\mySub{\textbf{IMPORTANT:}} These are \textbf{NOT} ``allowed to read'' and ``allowed to write'' sets.\\
It's more complicated than that.

\hfill
\begin{minipage}[t]{0.7\textwidth}\fixminipage
\vspace{6mm}
\textit{Deeper side note, not for live presentation: \myKey{WAR} checking is a ``conjunction of disjunctions''; with the disjunction being how each read visibility set $V_S$ gets independently augmented by synchronization ($V_S \leftarrow V_S \cup V_2$), and the conjunction being $\forall r, t \in V_S^r$.
This is not really simplifiable to a single ``allowed to write'' set.}
\end{minipage}
\newpage
% Title again moved out of desperation
\begin{minipage}[t]{0.55\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{Synchronization Checking Example}

Suppose immediately after the TMA, we issue another \violetBox{\texttt{Fence}}.
Then, all threads in the block write to \texttt{smem} again.

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        # ...
        for t in cuda_threads(0, 1):
            # fence.proxy.async
            Fence(cuda_sync, tma_to_gmem_async)
            with CudaAsync(tma_to_gmem_async):
                # (placeholder) TMA instr
                gmemWrite[...] = smem[0:256]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            # Inadequate Fence! (1 thread only)
            Fence(tma_to_gmem_async, cuda_sync)
\end{verbatim}
\end{mdframed}
\hfill\textit{$\Sigma^S$ shown for this location}
\vspace{2mm}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
        for t in cuda_threads(0, 256):
            smem[t] = ...
\end{verbatim}
\end{mdframed}

This \myKey{should fail} synchronization checking, since all but one thread fails to wait for the TMA.

\begin{itemize}
  \item $V_1 = \{(256, \texttt{sig\_tma\_to\_gmem})\}$
  \item $V_2 = \{(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$
\end{itemize}
Recall we're analyzing block $b=1$, threads $T = [256, 511]$

\end{minipage}
\hfill
\begin{minipage}[t]{0.43\textwidth}\fixminipage
\vspace{-8mm}
\begin{tikzpicture}[node distance=2mm]
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, right=of smem0, xshift=8mm] {\texttt{smem[255]}};
\draw[dotted] (smem0) to node(smemdots){} (smem255);

\node(incoming256) [normalnode, above=of smem0, yshift=6mm, fill=greenBoxBg] {Incoming write\\$(t,a) = (256,$\\\texttt{sig\_cuda\_sync})};
\node(incoming511) [normalnode, above=of smem255, yshift=6mm, fill=greenBoxBg] {Incoming write\\$(t,a) = (511,$\\\texttt{sig\_cuda\_sync})};

\draw[dotted] (incoming256) to (incoming511);
\draw[arrow] (incoming256) to (smem0);
\draw[arrow] (incoming511) to (smem255);

\node(rtma) [widenode, below=of smemdots, fill=redBoxBg, draw=redBoxFg, text=redBoxFg, yshift=-10mm] {$V_S = \{(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem}),$ $(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$};
\node(w256) [widenode, minimum width=8cm, text width=8cm, below=of rtma, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic} \cup \{(256, \texttt{sig\_tma\_to\_gmem})\}$};

\draw[line, draw=redBoxFg] (smem0) -- (rtma);
\draw[line, draw=redBoxFg] (smem255) -- (rtma);
\draw[line, draw=blueBoxFg] (smem0) to[out=215,in=105] ($(w256.north) - (3.5, 0)$);
\draw[line, draw=blueBoxFg] (smem255) to[out=325,in=75] ($(w256.north) + (3.5, 0)$);
\end{tikzpicture}

\myKey{WAW check pass}

$\forall t \in [256, 511]$,\\\greenBox{$(t,a)$} $\in$ \blueBox{$[256, 511] \times \texttt{cuda\_generic} \cup ...$}\\
(write visibility record's $V_S$)

\myKey{WAR check fail}

Fails except for \texttt{smem[0]}, i.e, $t \ge 257$

\greenBox{$(t,a)$} $\notin$ \redBox{$\{(256, \texttt{sig\_cuda\_sync})\}$}\\
(read visibility record's $V_S$)
\end{minipage}

%% Challenges
%%
%% * Kernel launch (fat pointer, CPU-to-GPU "grid constant" ... could grid constant be a memory property?)
%%
%% * Rewrite rule problems, treat 0 specially (zero-on-first use for wgmma, predication)
%%
%% * Memory Types (memory "properties", memory layout / swizzling)
%%
%% * Distributed Memory (collective assignment? cross-collective communication?)
%%
%% * collective assignment also for parallel for loops especially grid launch
%%     * collective dimension, non-collective dimension
%%     * ... also for parview concept, later
%%     * "partial" parview also allowed?

\end{document}
