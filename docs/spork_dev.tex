\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColor, fill=white]
\tikzstyle{smallsmemnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

{ \LARGE Exo Dev 2025-01-15 \hfill \textbf{\textsf{Project Spork: EXO GPU}}}

\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage

\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myTitle{Project Goals}  % Moved into minipage due to lack of space

\begin{itemize}
\item Support generating mixed CPU and CUDA code from Exo
\item ``Minimal change'' to the core Exo language; avoid huge changes to analysis and rewrite
\item Support basic synchronization, e.g. \texttt{\_\_syncthreads} (all threads in a CTA wait for all other threads in a CTA).
\begin{itemize}
  \item Fork/join
\end{itemize}
\item ... but also support split barriers: set of threads waiting for an event triggered by a different set of threads or different program location
\begin{itemize}
  \item Move beyond fork/join model
\end{itemize}
\item Support asynchronous CUDA accelerator instructions (async copy, async tensor core matrix multiply-accumulate)
\item Not-too-conservative sync checking
\end{itemize}
\end{minipage} %
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Elevator Pitch, Programming Model:}
\begin{itemize}
  \item \texttt{with} = open device (CUDA) code block
  \begin{itemize}
    \item Kenneth: we need to discuss overloading
  \end{itemize}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = distribute work across threads/warps/etc.
  \item Add new \texttt{Fence}, \texttt{Arrive}, \texttt{Await} statements and \texttt{barrier} type for synchronization.
  \item Need to extend \texttt{Memory} and \texttt{@instr} to handle new CUDA memory types and async instructions.
\end{itemize}

\mySub{Elevator Pitch, Rewrites \& Safety:}

Key idea: most of Exo will still treat the code \textit{as if it were not parallelized}

\begin{itemize}
  \item \texttt{with} = \texttt{if True:}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = sequential for loop
  \item \texttt{Fence}, \texttt{Arrive}, \texttt{Await} = nothing
  \item Async \texttt{instr} = sync \texttt{instr}
\end{itemize}

Parallelism checks are part of the code lowering process.

\end{minipage} %
\newpage

\myTitle{Basic CUDA Features}

\begin{minipage}[t]{0.5\textwidth}\fixminipage

CPU code launches ``grid'': hierarchy of threads
\begin{itemize}
  \item warp: 32 threads
  \item warpgroup: 128 threads (new in H100)
  \item CTA (block): \texttt{blockDim} threads (user-set size)
  \item cluster: 1-16 cooperating blocks (user-set)
  \item grid: \texttt{gridDim} blocks (user-set size)
\end{itemize}
I call these \myKey{``collective units''} (includes base case, one thread), abusing Cutlass vocabulary

Core ``unit of parallelism'': CTA
\begin{itemize}
  \item Easy to synchronize within CTAs; hard between CTAs (Exo won't model this)
  \item Sub-collectives (thread, warp, warpgroup) within CTA can split and coalesce easily
  \item Different collective unit needed for different operations; programmer handles manually!
  \item \myKeyB{Frequent} communication \myKeyB{within} blocks
  \item \myKey{Minimal} communication \myKey{between} blocks
  \item Limited cross-block cooperation with atomics
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myKey{GMEM} (Global Memory): \myKey{``slow''}, 10s of GB
\begin{itemize}
  \item any thread in grid may access
\end{itemize}
\myKeyB{SMEM} (Shared Memory): 100s of KiB
\begin{itemize}
  \item per-CTA memory (L1 cache carveout)
  \item \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math
\end{itemize}
\textbf{RMEM} (Register ``Memory'') -- 255 per thread

\begin{tikzpicture}[node distance=5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKey{GMEM}};
\draw[line] (grid) -- (gmem);

\node (block0) [smallnode, fill=violetBoxBg, below=of grid, xshift=-1cm, yshift=-16mm] {block};
\node (block1) [smallnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallnode, right=of block1, xshift=2cm] {block};
\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=180] ($(block2.west)+(0,0.2)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=310,in=145] (gridDim);

\node (thread0) [smallnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (rmem0) [smallnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallnode, below=of thread2] {\textbf{RMEM}};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\node (smem0) [smallsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallsmemnode, above=of block2] {\myKeyB{SMEM}};
\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=130] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\end{minipage}

\newpage
\myTitle{CUDA Async Instructions}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
New accelerator instructions with the NVIDIA H100 (Hopper) are asynchronous.
They are issued by CUDA threads or warpgroups, but execution continues without waiting.
\begin{itemize}
  \item TMA: copy tensor tiles \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
  \item wgmma: tiled matrix multiply-accumulate
\end{itemize}
This means the following won't work if the highlighted statements are async:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$B \leftarrow Y$}
  \item $C \leftarrow C + AB$ (incorrectly assumes $A,B$ written to)
\end{enumerate}
Two async instructions issued by the same thread/warpgroup don't even complete in the same order relative to each other, e.g., the following is a data race:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$A \leftarrow Y$}
\end{enumerate}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
These also ignore all ``ordinary'' synchronization.
\begin{itemize}
\item Special barriers needed
\item Exception: still protected by ``stop the world'' barriers between grid launches on the same stream (I think?)
\end{itemize}

\mySub{Async Proxy}

In CUDA terminology, TMA and wgmma are considered to operate in the \myKey{async proxy}
\begin{itemize}
  \item \myKey{generic proxy}: most other instructions
  \item \myKey{tensorMap proxy}: we'll ignore this
  \item Memory not visible by default across proxies
\end{itemize}
PTX docs has \textit{tomes} about this; boils down to
\begin{itemize}
  \item Need a \texttt{fence.proxy.async} for generic$\to$async proxy data flow
  \item \textit{Nothing} needed for async$\to$generic
  \begin{itemize}
    \item (detail: the fence is required in both directions, but built in to the ``wait for async instruction'' machinery)
  \end{itemize}
\end{itemize}

\end{minipage}
\newpage
\myTitle{wgmma: Warpgroup Matrix Multiply-accumulate Async}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, this is really simple.
Each \texttt{wgmma.mma\_async} instruction is issued by a warpgroup (\myKey{128 aligned threads}), and computes
\begin{itemize}
  \item $D \leftarrow AB$ or \hfill (scale-d = 0)
  \item $D \leftarrow AB + D$ \hfill (scale-d = 1)
\end{itemize}
where
\begin{itemize}
  \item $D$ matrix tile is in \textbf{registers (RMEM)}
  \begin{itemize}
    \item Note: $D$ is implicitly synchronized for consecutive wgmma.mma!
  \end{itemize}
  \item $B$ matrix tile is in \myKeyB{shared memory (SMEM)}
  \item $A$ matrix tile is stored in either format
  \item Usually, scale-d = 0 only for the first iteration
\end{itemize}

The complexity for this feature comes from
\begin{itemize}
  \item Input/output format details \myKey{(huuuge mess)}
  \item Synchronization
\end{itemize}
%I'll just address the latter for now.
%Although Exo will eventually have to figure out how to model the full set of matrix formats.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ wgmma}

For \textbf{registers}, issue \texttt{wgmma.fence}

For \myKeyB{shared memory}, issue an async proxy fence
\begin{itemize}
  \item Not needed if TMA filled SMEM
\end{itemize}

\mySub{Synchronization wgmma $\to$ generic}

The completion mechanism is \myKey{``commit group''}

wgmma synchronization only occurs within a single warpgroup, with MMAs pipelined:

\begin{itemize}
\item \texttt{wgmma.fence} begins a ``pipeline stage''
\item \texttt{wgmma.mma} must appear within pipeline stage
\item \texttt{wgmma.commit\_group} ends a pipeline stage
\item ptxas f's you if it can't recognize the pattern
\end{itemize}
\texttt{wgmma.wait\_group N} waits for the MMAs of the $N^{th}$ prior pipeline stage (0-indexed)

\end{minipage}

\begin{tikzpicture}[node distance=5mm]
\node (fence0) [smallnode] {fence};
\node (mma00) [smallishnode, right=of fence0, fill=violetBoxBg] {MMA\\scale-d=0};
\node (mma01) [smallnode, right=of mma00, fill=violetBoxBg] {MMA};
\node (commit0) [smallnode, right=of mma01, fill=violetBoxBg] {commit\\group};
\node (fence1) [smallnode, right=of commit0] {fence};
\node (mma10) [smallnode, right=of fence1] {MMA};
\node (mma11) [smallnode, right=of mma10] {MMA};
\node (commit1) [smallnode, right=of mma11] {commit\\group};
\node (wait1) [smallishnode, right=of commit1, fill=violetBoxBg] {wait\_group \textbf{1}};
\draw[arrow] (mma00.east) to (mma01.west);
\draw[arrow] (mma01.east) to (commit0.west);
\draw[arrow] (commit0.south) to[out=350,in=190] (wait1.south);
\end{tikzpicture}

\newpage
\myTitle{TMA: Tensor Memory Accelerator}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Handled with \texttt{cp.async.bulk.tensor} instructions.

\begin{itemize}
\item Async 1D-5D tile copy \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
\item \myKeyB{SMEM} tile: densely packed C-order matrix
\item \myKey{GMEM} tile: tile from big C-order matrix
\begin{itemize}
  \item \myKey{Predicated} and strided (16B aligned)
\end{itemize}
\item 16 byte aligned; cannot stride innermost dim
\end{itemize}

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (gmembig) [bignode, right=of smem, yshift=-3mm] {};
\node (gmem) [gmemnode, right=of smem, xshift=9mm] {\myKey{GMEM tile}};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);
\end{tikzpicture}

Need to encode GMEM matrix as \myKey{\texttt{CUtensorMap}}
\begin{itemize}
  \item \myKey{``fat pointer''}, GMEM pointer + info
  \item Encodes GMEM matrix size and strides
  \item Encodes SMEM tile size (\& swizzle mode)
  \item SMEM pointer NOT encoded
  \item Used on device, but must be encoded on the host CPU
  \begin{itemize}
    \item not 100\% true, look up tensorMap proxy if you want
  \end{itemize}
\end{itemize}

%Note: TMA can also be used without a \texttt{CUtensorMap} for literal array to array copies.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ TMA}

Async proxy fence required to see memory written by generic instructions.

\mySub{Synchronization TMA $\to$ generic}

If the TMA copies SMEM to GMEM, the completion mechanism is \myKey{commit\_group}

If the TMA copies GMEM to SMEM, the completion mechanism is \myKey{mbarrier} (split barrier)

\begin{itemize}
  \item Initialized in SMEM with arrive-count $A$
  \item Any number of threads can wait until both of the following complete:
  \begin{itemize}
    \item $A$-many threads arrive
    \item \texttt{tx-count}-many bytes copied by TMA
  \end{itemize}
  \item Re-usable (detail: requires parity tracking)
\end{itemize}
Usually, we nominate only \myKey{1 thread} to issue the TMA instruction.
Hence, we use a 1-to-many mbarrier to synchronize ($A = 1$).

NB mbarrier is usable without TMA (\texttt{tx-count=0})

\end{minipage}
\newpage
\myTitle{TMA Reduction}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
TMA takes an optional ``reduce'' operand when copying \myKeyB{SMEM}$\to$\myKey{GMEM}

Replaces \texttt{GMEM[\textit{slice}] = SMEM}\\
with \texttt{GMEM[\textit{slice}] \violetBox{+=} SMEM} (or another reduce op)

This reduction is \myKey{atomic} (\texttt{relaxed.gpu}) per element

Extremely OP Feature!

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (op) [normalnode, right=of smem, fill=violetBoxBg] {AtomicOp};
\draw [arrow] (smem) -- (op);
\node (gmembig) [bignode, below=of op] {};
\node (gmem) [gmemnode, below=of op, yshift=-6mm] {\myKey{GMEM tile}};
\draw [arrow] (gmem.east) to[out=0,in=0] (op.east);
\draw [arrow] (op.south) to[out=250,in=110] (gmem.north);
\end{tikzpicture}
\begin{verbatim}
def tma_reduce(gmem, smem, boxDim1...boxDimN,
               coord1...coordN):
    for i1 in seq(0, boxDim1):
        ... # N=1,2,3,4,5
        for iN in seq(0, boxDimN):
            gmem[i1 + coord1...
                iN + coordN] += smem[i1...iN]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Application: Split-$k$}

With TMA, we can parallelize a gemm on $k$ by assigning multiple thread blocks to each output tile,
assigning different $k$ ranges to each block, and using TMA to reduce into the output tile.

\mySub{Application: Backwards Pass}

The backwards pass reverses the direction of information flow.
Each original ``input'' tile $A_i,B_j$ receives gradient contributions from multiple ``output'' tiles $C_{i,j}$.
Parallelizing blocks on $C$ tiles requires reductions between thread blocks.

\begin{tikzpicture}[node distance=2mm]
\node(b0) [smallnode] {$B_0$};
\node(b1) [smallnode, right=of b0] {$B_1$};
\node(b2) [smallnode, right=of b1, fill=blueBoxBg] {$B_2$};
\node(bN) [smallnode, right=of b2, xshift=4mm] {$B_{no-1}$};
\draw [dotted] (b2) -- (bN);
\node(c00) [smallnode, below=of b0, yshift=-4mm] {$C_{0,0}$};
\node(c01) [smallnode, below=of b1, yshift=-4mm] {$C_{0,1}$};
\node(c02) [smallnode, below=of b2, yshift=-4mm, fill=blueBoxBg] {$C_{0,2}$};
\node(c0N) [smallnode, below=of bN, yshift=-4mm] {$C_{0,no-1}$};

\node(a0) [smallnode, left=of c00, xshift=-4mm] {$A_0$};
\node(a1) [smallnode, below=of a0, fill=redBoxBg] {$A_1$};
\node(aM) [smallnode, below=of a1, yshift=-4mm] {$A_{mo-1}$};
\draw [dotted] (a1) -- (aM);

\node(c10) [smallnode, below=of c00, fill=redBoxBg] {$C_{1,0}$};
\node(c11) [smallnode, below=of c01, fill=redBoxBg] {$C_{1,1}$};
\node(c12) [smallnode, below=of c02, fill=violetBoxBg] {$C_{1,2}$};
\node(c1N) [smallnode, below=of c0N, fill=redBoxBg] {$C_{1,no-1}$};

\node(cM0) [smallnode, below=of c10, yshift=-4mm] {$C_{mo-1,0}$};
\node(cM1) [smallnode, below=of c11, yshift=-4mm] {$C_{mo-1,1}$};
\node(cM2) [smallnode, below=of c12, yshift=-4mm, fill=blueBoxBg] {$C_{mo-1,2}$};
\node(cMN) [smallnode, below=of c1N, yshift=-4mm] {$C_{...}$};

\draw [arrow] (c00) -- (a0);
\draw [arrow] (c10) -- (a1);
\draw [arrow] (cM0) -- (aM);

\draw [dotted] (c10) -- (cM0);
\draw [dotted] (c11) -- (cM1);
\draw [dotted] (c12) -- (cM2);
\draw [dotted] (c1N) -- (cMN);

\draw [dotted] (c02) -- (c0N);
\draw [dotted] (c12) -- (c1N);
\draw [dotted] (cM2) -- (cMN);
\draw [dotted] (c12) -- (cMN);

\draw [arrow] (c00) -- (b0);
\draw [arrow] (c01) -- (b1);
\draw [arrow] (c02) -- (b2);
\draw [arrow] (c0N) -- (bN);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- Async Block, Parallel For}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Open an ``async block'' to switch from compiling CPU code to accelerator code.
\begin{itemize}
  \item Syntax: \texttt{with \textit{async\_config}: \textit{body}}
  \item Nomenclature?\\Idea is this block is ``asynchronous'' in some way relative to the parent code
\end{itemize}
\vspace{12mm}
Top level (outside async block): CPU code

Middle level: \myKey{synchronous} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaDeviceFunction(blockDim,...):}
  \item \myKey{Parallel for} allowed at this level
\end{itemize}

Bottom level: \myKey{async} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaAsync(...):}
  \item more detail later!
\end{itemize}
\vspace{12mm}
Future work could build on this syntax to target non-CUDA accelerators.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
We define parallel for loops for each level of the CUDA thread hierarchy;
\texttt{for \_ in cuda\_\{units\}:}
\begin{itemize}
  \item i.e. each \myKey{collective unit}
  \item \texttt{\{units\}} = \texttt{clusters}, \texttt{blocks}, \texttt{warpgroups}, \texttt{warps}, \texttt{threads}
\end{itemize}
Must be \myKey{strictly nested}
\begin{itemize}
  \item levels except \texttt{blocks} may be skipped
  \item exception: \myKey{multidimensional} iteration\\(defined by directly nested same-unit loops)
\end{itemize}
Each iteration executed by a \myKey{collective lane}

Parallel for loops + hardware resources define \myKey{collective scope} (formerly parlane, parscope)

\begin{verbatim}
with CudaDeviceFunction(blockDim=256):
    for b in cuda_blocks(...):
        for y in cuda_threads(0, 16):
            for x in cuda_threads(0, 16):
                # 16 x 16 iteration space
                # mapped to 256 threads

                with CudaAsync(...):
                    # Async instructions
\end{verbatim}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- SpecializeCollective (warp specialization)}

Parallel for loops map the work over all available hardware resources of the parent collective lane:
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        # collective scope is block_count-many blocks
        # collective lane is 1 block of blockDim = 5 * 128 threads

        for wg in cuda_warpgroups(0, 5):
            # 5 iterations mapped to 5 warpgroups available in block
            # collective lane is now a warpgroup

            for w in cuda_warps(0, 4):
                # 4 iterations mapped to 4 warps available in warpgroup
\end{verbatim}
Nest inside \myKey{\texttt{with SpecializeCollective}} to override this; assign \myKey{different work} to \myKey{different warps}
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        with SpecializeCollective(cuda_warpgroup, 0, 1):        # Overloaded with statement
            for wg in cuda_warpgroups(0, 1):
                # This code mapped to warpgroup 0
        with SpecializeCollective(cuda_warpgroup, 1, 5):        # Overloaded with statement
            for n_wg in cuda_warpgroup(0, 2):
                for m_wg in cuda_warpgroup(0, 2):
                    # 2 x 2 space mapped to warpgroups 1, 2, 3, 4
\end{verbatim}

\newpage
\myTitle{Exo Syntax -- Synchronization 1/2}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
The simplest category of barrier is what I term a \myKey{Fence}; this specifies that all threads in the executing \myKey{collective lane} wait for each other.

\begin{verbatim}
with CudaDeviceFunction(...):
    for b in cuda_blocks(...):
        # Executing collective lane is block

        for t in cuda_threads(...):
            do_something_A()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)

        for w in cuda_warps(...):
            for t in cuda_threads(0, 32):
                do_something_B()
            # Executing collective lane is warp
            # so this Fence is __syncwarp()
            Fence(cuda_sync, cuda_generic)
            do_something_warp()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
Ignore \texttt{cuda\_sync, cuda\_generic} for now (actor kinds, to be explained)

In Exo, we are \myKey{explicit} about which operations are done by which \myKey{collective units} (e.g. block / coalesced warp), unlike in CUDA C++ where it is the \myKey{user's responsibility} to do this correctly, else get hangs or UB.

This can cause one block of CUDA code to map to multiple blocks of Exo code, e.g. the code on the left is equivalent to CUDA code
\begin{verbatim}
__global__ void <function name>()
{
    do_something_A();   // unit =     thread
    __syncthreads();    // unit = block
    do_something_B();   // unit =     thread
    __syncwarp();       // unit =   warp
    do_something_warp();// unit =   warp
    __syncthreads();    // unit = block
}
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Exo Syntax -- Synchronization 2/2, Actor Kind}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, an async block instructs the compiler to lower to a \myKey{different category of instructions} with \myKey{different synchronization requirements}

For now, informally, this categorization is the \myKey{actor kind}, which is one of the following:

\vspace{12mm}

Top level code:
\begin{itemize}
  \item \texttt{cpu}
\end{itemize}

\texttt{with CudaDeviceFunction(...):}
\begin{itemize}
  \item \texttt{cuda\_sync}
  \begin{itemize}
    \item Synchronous CUDA instructions
  \end{itemize}
\end{itemize}

\texttt{with CudaAsync(\textit{actor\_kind}):}
\begin{itemize}
  \item \texttt{non\_bulk\_cp\_async}
  \begin{itemize}
    \item Ampere async copy\\(uses generic proxy; not discussed)
  \end{itemize}
  \item \texttt{tma\_gmem\_to\_smem\_async}
  \item \texttt{tma\_smem\_to\_gmem\_async}
  \item \texttt{wgmma\_async}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Fence}

Parameterize with actor kind
\begin{verbatim}
  Fence(A1, A2)
\end{verbatim}
Within the executing collective lane, prior actions of actor kind \texttt{A1} happen before subsequent actions of actor kind \texttt{A2}

\mySub{Split Barrier}

Declare variable of \texttt{barrier} type
\begin{verbatim}
  <barrier var> : barrier
  Arrive(A1, <barrier var>)
  Await(<barrier var>, A2)
\end{verbatim}

Actions of actor kind \texttt{A1} prior to the $N^{th}$ arrive happen before actions of actor kind \texttt{A2} subsequent to the $N^{th}$ await.

We use orthogonal syntax but \myKey{not all} collective unit + actor kind combinations supported.

\mySub{Synthetic Actor Kind}

Additional actor kinds only used to parameterize synchronization,
e.g. \texttt{cuda\_all} (all CUDA instructions), \texttt{cuda\_generic} (generic proxy)

\end{minipage}

\newpage
\myTitle{Exo Syntax -- CudaAsync Block}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
As mentioned, nested within \texttt{CudaDeviceFunction} async block (actor kind \texttt{cuda\_sync})

Syntax, \texttt{with CudaAsync(\textit{actor\_kind})}

Often, there is \myKey{nontrivial interaction} between the async instructions and related synchronization
\begin{itemize}
\item Example: wgmma ``pipeline stage'' must appear in fixed pattern (that gets ptxas's vaunted stamp of approval)
\item Example: TMA \texttt{expect-tx} needs to be set to the number of bytes copied
\end{itemize}
\vspace{12mm}
For certain actor kinds, we \myKey{could} require
\begin{itemize}
\item \myKey{prologue synchronization}: first body stmt
\item \myKey{epilogue synchronization}: last body stmt
\end{itemize}
which the backend will compile non-trivially with respect to the async block's contents.

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{wgmma pipeline stage}
\begin{verbatim}
with CudaAsync(wgmma_async):
    # Prologue synchronization
    # wgmma.fence
    # NB wgmma_rmem is a synthetic actor kind
    Fence(wgmma_rmem, wgmma_rmem_async)

    # ... MMA instr ...

    # Epilogue synchronization
    # wgmma.commit_group
    Arrive(wgmma_async, <barrier var>)
\end{verbatim}
\mySub{TMA GMEM$\to$SMEM}
\begin{verbatim}
with CudaAsync(tma_gmem_to_smem):
    # ... TMA instr ...

    # Epilogue synchronization
    # Calculates expect-tx = bytes copied
    # (futher surprise, in the generated
    # CUDA C++, the Arrive is moved before
    # the TMA instructions!!!)
    Arrive(tma_gmem_to_smem, <barrier var>)
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Syntax Example}
\begin{verbatim}
with CudaDeviceFunction(blockDim=384):
    # blockDim 384 = 3 warpgroups

    for yo in cuda_blocks(0, 24):
        for xo in cuda_blocks(0, 36):
        # Grid of 36 x 24 blocks (CTAs)

        with SpecializeCollective(cuda_warpgroup, 1, 3):
            for w in cuda_warpgroups(0, 2):
                # Skipped warpgroup 0 (specialize); 2 iters mapped to warpgroups 1 and 2
                bar: Barrier

                for yi in cuda_threads(0, 16):
                      for xi in cuda_threads(0, 8):
                          # 8 x 16 iteration space, each iter mapped to 1 of 128 threads

                with CudaAsync(wgmma_async):
                    # Now targetting wgmma; this is at warpgroup scope
                    # since wgmma requires coalesced warpgroups
                    Fence(wgmma_rmem, wgmma_async_rmem)  # prologue synchronization
                    for k in seq(0, 8):                  # ordinary sequential loop
                        wgmma_mma(...)
                    Arrive(bar, wgmma_async)             # epilogue synchronization
\end{verbatim}

\newpage
\myTitle{Contrast with prior work: Triton}

\begin{minipage}[t]{0.7\textwidth}\fixminipage
Triton is another Python AST $\to$ CUDA language

In my terminology, the ``collective unit'' for a Triton function is a block.
\begin{itemize}
  \item One block executes one ``call'' of a Triton function.
\end{itemize}

Triton's responsibility: \textbf{automatically} distribute the work \myKeyB{within} one block to sub-block units (mostly threads) and automatically synchronize.
\begin{itemize}
  \item Recall: synchronization within a block is \myKeyB{frequent}
  \item Minimal control over memory movement, register/SMEM allocation
  \item Honestly, not a bad model though
\end{itemize}

User responsibility: manually distribute work \myKey{between} blocks.
\begin{itemize}
  \item Recall: synchronization between blocks is \myKey{minimal}
  \item Often one output tile assigned per block
\end{itemize}

With Exo, we're trying to build something much more \textbf{imperative}
\begin{itemize}
  \item Control (and predict!) memory and register pressure
  \item Fine grained control over each level of CUDA thread hierarchy
  \item Explicit use of accelerator instructions
\end{itemize}
\end{minipage}

\vspace{6mm}
\hfill(Also, Triton generates IR and Exo GPU will generate C and CUDA C++)
\end{document}
