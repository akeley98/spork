\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColor, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

{ \LARGE Exo Dev 2025-01-15 \hfill \textbf{\textsf{Project Spork: EXO GPU}}}

\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage

\begin{minipage}[t]{0.48\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{Project Goals}  % Moved into minipage due to lack of space
\vspace{6mm}
\begin{itemize}
\item Generate \myKey{mixed CPU and CUDA} code
\item Support asynchronous CUDA accelerator instructions
\item \myKey{Minimal change} to the core Exo language; avoid huge changes to analysis and rewrite
\item Support basic synchronization, e.g. \texttt{\_\_syncthreads}
\begin{itemize}
  \item (scoped all $\leftrightarrow$ all threads sync)
  \item similar to fork/join
\end{itemize}
\item ... but also support \myKey{split barriers}
\begin{itemize}
  \item (thread set $B$ waits for thread set $A$)
  \item move beyond fork/join model
\end{itemize}
\begin{itemize}
  \item async memory copies
  \item ``tensor core'' -- tiled MMA
\end{itemize}
\item Not-too-conservative sync checking
\end{itemize}
\end{minipage} %
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Elevator Pitch, Programming Model:}
\begin{itemize}
  \item \texttt{with} = open asynchronous block
  \begin{itemize}
    \item Open CUDA block in CPU code
    \item Open async CUDA block
    \item Kenneth: we need to discuss overloading
  \end{itemize}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = distribute work across threads/warps/etc.
  \item Add new \texttt{Fence}, \texttt{Arrive}, \texttt{Await} statements and \texttt{barrier} type for synchronization.
  \item Need to extend \texttt{Memory} and \texttt{@instr} to handle new CUDA memory types and async instructions.
\end{itemize}

\mySub{Elevator Pitch, Rewrites \& Safety:}

Key idea: most of Exo will still treat the code \textit{as if it were not parallelized}

\begin{itemize}
  \item \texttt{with} = \texttt{if True:}
  \item \texttt{for i in cuda\_\{thread/warp/etc.\}(lo,hi)}\\ = sequential for loop
  \item \texttt{Fence}, \texttt{Arrive}, \texttt{Await} = nothing
  \item Async \texttt{instr} = sync \texttt{instr}
\end{itemize}

Check synchronization in code lowering process.

\end{minipage} %
\newpage

\myTitle{Basic CUDA Features}

\begin{minipage}[t]{0.5\textwidth}\fixminipage

CPU code launches ``grid'': hierarchy of threads
\begin{itemize}
  \item warp: exactly 32 threads
  \item warpgroup: exactly 128 threads (new in H100)
  \item \myKeyB{CTA (block)}: \texttt{blockDim} (user-set) threads
  \item cluster: 1-16 (user-set) cooperating blocks
  \item grid: \texttt{gridDim} (user-set) blocks
\end{itemize}
I call these \myKey{``collective units''} (includes base case, one thread), abusing Cutlass vocabulary

Core ``unit of parallelism'': \myKeyB{CTA}
\begin{itemize}
  \item Easy to synchronize within CTAs; hard between CTAs (Exo won't model this)
  \item Sub-collectives (thread, warp, warpgroup) within CTA can split and coalesce easily
  \item Different collective unit needed for different operations; programmer handles manually!
  \item \myKeyB{Frequent} communication \myKeyB{within} blocks
  \item \myKey{Minimal} communication \myKey{between} blocks
  \item Limited cross-block cooperation with atomics
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myKey{GMEM} (Global Memory): \myKey{``slow''}, 10s of GB
\begin{itemize}
  \item any thread in grid may access
\end{itemize}
\myKeyB{SMEM} (Shared Memory): 100s of KiB
\begin{itemize}
  \item per-CTA memory (L1 cache carveout)
  \item \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math
\end{itemize}
\textbf{RMEM} (Register ``Memory'') -- 255 per thread

\begin{tikzpicture}[node distance=3.5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKey{GMEM}};
\draw[line] (grid) -- (gmem);

\node (block0) [smallishnode, fill=violetBoxBg, below=of grid, xshift=-15mm, yshift=-16mm] {block};
\node (block1) [smallishnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallishnode, right=of block1, xshift=1.6cm] {block};

\node (smem0) [smallishsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallishsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallishsmemnode, above=of block2] {\myKeyB{SMEM}};

\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=290,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=100] ($(block2.west)+(0,0.6)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node(gridDim)[]{gridDim} ($(block2.west) - (0, 0.3)$);
\draw[arrow,dotted] (grid.south) to [out=330,in=100] (gridDim.north);

\node (thread0) [smallishnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallishnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallishnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (rmem0) [smallishnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallishnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallishnode, below=of thread2] {\textbf{RMEM}};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=290,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=150] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node(blockDim)[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\draw[arrow,dotted] (block0.south) to [out=310,in=130] (blockDim.north);
\end{tikzpicture}
\end{minipage}

\newpage
\myTitle{CUDA Async Instructions}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
New accelerator instructions with the NVIDIA H100 (Hopper) are asynchronous.
They are issued by CUDA threads or warpgroups, but execution continues without waiting.
\begin{itemize}
  \item TMA: copy tensor tiles \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
  \item wgmma: tiled matrix multiply-accumulate
\end{itemize}
This means the following won't work if the highlighted statements are async:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$B \leftarrow Y$}
  \item $C \leftarrow C + AB$ (incorrectly assumes $A,B$ written to)
\end{enumerate}
Two async instructions issued by the same thread/warpgroup don't even complete in the same order relative to each other, e.g., the following is a data race:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$A \leftarrow Y$}
\end{enumerate}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
These also ignore all ``ordinary'' synchronization.
\begin{itemize}
\item Special barriers needed
\item Exception: still protected by ``stop the world'' barriers between grid launches on the same stream (I think?)
\end{itemize}

\mySub{Async Proxy}

In CUDA terminology, TMA and wgmma are considered to operate in the \myKey{async proxy}
\begin{itemize}
  \item \myKey{generic proxy}: most other instructions
  \item \myKey{tensorMap proxy}: we'll ignore this
  \item Memory not visible by default across proxies
\end{itemize}
PTX docs has \textit{tomes} about this; boils down to
\begin{itemize}
  \item Need a \texttt{fence.proxy.async} for generic$\to$async proxy data flow
  \item \textit{Nothing} needed for async$\to$generic
  \begin{itemize}
    \item (detail: the fence is required in both directions, but built in to the ``wait for async instruction'' machinery)
  \end{itemize}
\end{itemize}

\end{minipage}
\newpage
\myTitle{wgmma: Warpgroup Matrix Multiply-accumulate Async}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, this is really simple.
Each \texttt{wgmma.mma\_async} instruction is issued by a warpgroup (\myKey{128 aligned threads}), and computes
\begin{itemize}
  \item $D \leftarrow AB$ or \hfill (scale-d = 0)
  \item $D \leftarrow AB + D$ \hfill (scale-d = 1)
\end{itemize}
where
\begin{itemize}
  \item $D$ matrix tile is in \textbf{registers (RMEM)}
  \begin{itemize}
    \item Note: $D$ is implicitly synchronized for consecutive \texttt{wgmma.mma}!
  \end{itemize}
  \item $B$ matrix tile is in \myKeyB{shared memory (SMEM)}
  \item $A$ matrix tile is stored in either format
  \item Usually, scale-d = 0 only for the first iteration
\end{itemize}

The complexity for this feature comes from
\begin{itemize}
  \item Input/output format details \myKey{(huuuge mess)}
  \item Synchronization
\end{itemize}
%I'll just address the latter for now.
%Although Exo will eventually have to figure out how to model the full set of matrix formats.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ wgmma}

For \textbf{registers}, issue \texttt{wgmma.fence}

For \myKeyB{shared memory}, issue an async proxy fence
\begin{itemize}
  \item Not needed if TMA filled SMEM
\end{itemize}

\mySub{Synchronization wgmma $\to$ generic}

The completion mechanism is \myKey{``commit group''}

wgmma synchronization only occurs within a single warpgroup, with MMAs pipelined:

\begin{itemize}
\item \texttt{wgmma.fence} begins a ``pipeline stage''
\item \texttt{wgmma.mma} must appear within pipeline stage
\item \texttt{wgmma.commit\_group} ends a pipeline stage
\item ptxas f's you if it can't recognize the pattern  % fun fact: "f's" is used because no other verb fits without forcing a line break
\end{itemize}
\texttt{wgmma.wait\_group N} waits for the MMAs of the $N^{th}$ prior pipeline stage (0-indexed)

\end{minipage}

\begin{tikzpicture}[node distance=5mm]
\node (fence0) [smallnode] {fence};
\node (mma00) [smallishnode, right=of fence0, fill=violetBoxBg] {MMA\\scale-d=0};
\node (mma01) [smallnode, right=of mma00, fill=violetBoxBg] {MMA};
\node (commit0) [smallnode, right=of mma01, fill=violetBoxBg] {commit\\group};
\node (fence1) [smallnode, right=of commit0] {fence};
\node (mma10) [smallnode, right=of fence1] {MMA};
\node (mma11) [smallnode, right=of mma10] {MMA};
\node (commit1) [smallnode, right=of mma11] {commit\\group};
\node (wait1) [smallishnode, right=of commit1, fill=violetBoxBg] {wait\_group \textbf{1}};
\draw[arrow] (mma00.east) to (mma01.west);
\draw[arrow] (mma01.east) to (commit0.west);
\draw[arrow] (commit0.south) to[out=350,in=190] (wait1.south);
\end{tikzpicture}

\newpage
\myTitle{TMA: Tensor Memory Accelerator}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Handled with \texttt{cp.async.bulk.tensor} instructions.

\begin{itemize}
\item Async 1D-5D tile copy \myKey{GMEM}$\leftrightarrow$\myKeyB{SMEM}
\item \myKeyB{SMEM} tile: densely packed C-order matrix
\item \myKey{GMEM} tile: tile from big C-order matrix
\begin{itemize}
  \item \myKey{Predicated} and strided (16B aligned)
\end{itemize}
\item 16 byte aligned; cannot stride innermost dim
\end{itemize}

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (gmembig) [bignode, right=of smem, yshift=-3mm] {};
\node (gmem) [gmemnode, right=of smem, xshift=9mm] {\myKey{GMEM tile}};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);
\end{tikzpicture}

Need to encode GMEM matrix as \myKey{\texttt{CUtensorMap}}
\begin{itemize}
  \item \myKey{``fat pointer''}, GMEM pointer + info
  \item Encodes GMEM matrix size and strides
  \item Encodes SMEM tile size (\& swizzle mode)
  \item SMEM pointer NOT encoded
  \item Used on device, but must be encoded on the host CPU
  \begin{itemize}
    \item not 100\% true, look up tensorMap proxy if you want
  \end{itemize}
\end{itemize}

%Note: TMA can also be used without a \texttt{CUtensorMap} for literal array to array copies.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ TMA}

Async proxy fence required to see memory written by generic instructions.

\mySub{Synchronization TMA $\to$ generic}

If the TMA copies \myKeyB{SMEM}$\to$\myKey{GMEM}, the completion mechanism is \textbf{commit\_group}

If the TMA copies \myKey{GMEM}$\to$\myKeyB{SMEM}, the completion mechanism is \textbf{mbarrier} (split barrier)

\begin{itemize}
  \item Initialized in SMEM with arrive-count $A$
  \item Any number of threads can wait until both of the following complete:
  \begin{itemize}
    \item $A$-many threads arrive
    \item \texttt{tx-count}-many bytes copied by TMA
  \end{itemize}
  \item Re-usable (detail: requires parity tracking)
\end{itemize}
Usually, we nominate only \myKey{1 thread} to issue the TMA instruction.
Hence, we use a 1-to-many mbarrier to synchronize ($A = 1$).

NB mbarrier is usable without TMA (\texttt{tx-count=0})

\end{minipage}
\newpage
\myTitle{TMA Reduction}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
TMA takes an optional ``reduce'' operand when copying \myKeyB{SMEM}$\to$\myKey{GMEM}

Replaces \texttt{GMEM[\textit{slice}] = SMEM}\\
with \texttt{GMEM[\textit{slice}] \violetBox{+=} SMEM} (or another reduce op)

This reduction is \myKey{atomic} (\texttt{relaxed.gpu}) per element

Extremely OP Feature!

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (op) [normalnode, right=of smem, fill=violetBoxBg] {AtomicOp};
\draw [arrow] (smem) -- (op);
\node (gmembig) [bignode, below=of op] {};
\node (gmem) [gmemnode, below=of op, yshift=-6mm] {\myKey{GMEM tile}};
\draw [arrow] (gmem.east) to[out=0,in=0] (op.east);
\draw [arrow] (op.south) to[out=250,in=110] (gmem.north);
\end{tikzpicture}
\begin{verbatim}
def tma_reduce(gmem, smem, boxDim1...boxDimN,
               coord1...coordN):
    for i1 in seq(0, boxDim1):
        ... # N=1,2,3,4,5
        for iN in seq(0, boxDimN):
            gmem[i1 + coord1...
                iN + coordN] += smem[i1...iN]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Application: Split-$k$}

With TMA, we can parallelize a gemm on $k$ by assigning multiple thread blocks to each output tile,
assigning different $k$ ranges to each block, and using TMA to reduce into the output tile.

\mySub{Application: Backwards Pass}

The backwards pass reverses the direction of information flow.
Each original ``input'' tile $A_i,B_j$ receives gradient contributions from multiple ``output'' tiles $C_{i,j}$.
Parallelizing blocks on $C$ tiles requires reductions between thread blocks.

\begin{tikzpicture}[node distance=2mm]
\node(b0) [smallnode] {$B_0$};
\node(b1) [smallnode, right=of b0] {$B_1$};
\node(b2) [smallnode, right=of b1, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$B_2$};
\node(bN) [smallnode, right=of b2, xshift=4mm] {$B_{no-1}$};
\draw [dotted] (b2) -- (bN);
\node(c00) [smallnode, below=of b0, yshift=-4mm] {$C_{0,0}$};
\node(c01) [smallnode, below=of b1, yshift=-4mm] {$C_{0,1}$};
\node(c02) [smallnode, below=of b2, yshift=-4mm, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$C_{0,2}$};
\node(c0N) [smallnode, below=of bN, yshift=-4mm] {$C_{0,no-1}$};

\node(a0) [smallnode, left=of c00, xshift=-4mm] {$A_0$};
\node(a1) [smallnode, below=of a0, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$A_1$};
\node(aM) [smallnode, below=of a1, yshift=-4mm] {$A_{mo-1}$};
\draw [dotted] (a1) -- (aM);

\node(c10) [smallnode, below=of c00, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$C_{1,0}$};
\node(c11) [smallnode, below=of c01, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$C_{1,1}$};
\node(c12) [smallnode, below=of c02, fill=violetBoxBg, draw=violet, text=violet] {$C_{1,2}$};  % cheating at my color scheme a bit
\node(c1N) [smallnode, below=of c0N, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$C_{1,no-1}$};

\node(cM0) [smallnode, below=of c10, yshift=-4mm] {$C_{mo-1,0}$};
\node(cM1) [smallnode, below=of c11, yshift=-4mm] {$C_{mo-1,1}$};
\node(cM2) [smallnode, below=of c12, yshift=-4mm, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$C_{mo-1,2}$};
\node(cMN) [smallnode, below=of c1N, yshift=-4mm] {$C_{...}$};

\draw [arrow] (c00) -- (a0);
\draw [arrow] (c10) -- (a1);
\draw [arrow] (cM0) -- (aM);

\draw [dotted] (c10) -- (cM0);
\draw [dotted] (c11) -- (cM1);
\draw [dotted] (c12) -- (cM2);
\draw [dotted] (c1N) -- (cMN);

\draw [dotted] (c02) -- (c0N);
\draw [dotted] (c12) -- (c1N);
\draw [dotted] (cM2) -- (cMN);
\draw [dotted] (c12) -- (cMN);

\draw [arrow] (c00) -- (b0);
\draw [arrow] (c01) -- (b1);
\draw [arrow] (c02) -- (b2);
\draw [arrow] (c0N) -- (bN);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- Async Block, Parallel For}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Open an ``async block'' to switch from compiling CPU code to accelerator code
\begin{itemize}
  \item Syntax: \texttt{with \textit{async\_config}: \textit{body}}
  \item Nomenclature?\\Idea is this block is ``asynchronous'' in some way relative to the parent code
\end{itemize}
\vspace{12mm}
Top level (outside async block): CPU code

Middle level: \myKey{synchronous} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaDeviceFunction(blockDim,...):}
  \item \myKey{Parallel for} allowed at this level
\end{itemize}

Bottom level: \myKey{async} CUDA instructions
\begin{itemize}
  \item \texttt{with CudaAsync(...):}
  \item more detail later!
\end{itemize}
\vspace{12mm}
Future work could build on this syntax to target non-CUDA accelerators.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
We define parallel for loops for each level of the CUDA thread hierarchy;
\texttt{for \_ in cuda\_\{units\}:}
\begin{itemize}
  \item i.e. each \myKey{collective unit}
  \item \texttt{\{units\}} = \texttt{clusters}, \texttt{blocks}, \texttt{warpgroups}, \texttt{warps}, \texttt{threads}
  \item Generalize \texttt{seq}, \texttt{par} to \myKey{loop mode}
\end{itemize}
Must be \myKey{strictly nested}
\begin{itemize}
  \item levels except \texttt{blocks} may be skipped
  \item exception: \myKey{multidimensional} iteration\\(defined by directly nested same-unit loops)
\end{itemize}
Each iteration executed by a \myKey{collective lane}

Parallel for loops + hardware resources define \myKey{collective scope} (formerly parlane, parscope)

\begin{verbatim}
with CudaDeviceFunction(blockDim=256):
    for b in cuda_blocks(...):
        for y in cuda_threads(0, 16):
            for x in cuda_threads(0, 16):
                # 16 x 16 iteration space
                # mapped to 256 threads
                with CudaAsync(...):
                    # Async instructions
\end{verbatim}
\end{minipage}
\newpage
\myTitle{Exo Syntax -- SpecializeCollective (warp specialization)}

Parallel for loops map the work over all available hardware resources of the parent collective lane:
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        # collective scope is block_count-many blocks
        # collective lane is 1 block of blockDim = 5 * 128 threads

        for wg in cuda_warpgroups(0, 5):
            # 5 iterations mapped to 5 warpgroups available in block
            # collective lane is now a warpgroup

            for w in cuda_warps(0, 4):
                # 4 iterations mapped to 4 warps available in warpgroup
\end{verbatim}
Nest inside \myKey{\texttt{with SpecializeCollective}} to override this; assign \myKey{different work} to \myKey{different warps}
\begin{verbatim}
with CudaDeviceFunction(blockDim = 5 * 128):
    for b in cuda_blocks(0, block_count):
        with SpecializeCollective(cuda_warpgroup, 0, 1):        # Overloaded with statement
            for wg in cuda_warpgroups(0, 1):
                # This code mapped to warpgroup 0
        with SpecializeCollective(cuda_warpgroup, 1, 5):        # Overloaded with statement
            for n_wg in cuda_warpgroup(0, 2):
                for m_wg in cuda_warpgroup(0, 2):
                    # 2 x 2 space mapped to warpgroups 1, 2, 3, 4
\end{verbatim}

\newpage
\myTitle{Exo Syntax -- Synchronization 1/2}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
The simplest category of barrier is what I term a \myKey{Fence}; this specifies that all threads in the executing \myKey{collective lane} wait for each other.

\begin{verbatim}
with CudaDeviceFunction(...):
    for b in cuda_blocks(...):
        # Executing collective lane is block

        for t in cuda_threads(...):
            do_something_A()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)

        for w in cuda_warps(...):
            for t in cuda_threads(0, 32):
                do_something_B()
            # Executing collective lane is warp
            # so this Fence is __syncwarp()
            Fence(cuda_sync, cuda_generic)
            do_something_warp()

        # __syncthreads()
        Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
Ignore \texttt{cuda\_sync, cuda\_generic} for now (actor kinds, to be explained)

One block of CUDA C++ could map to multiple blocks of Exo
\begin{itemize}
\item Exo: we are \myKey{explicit} about the collective unit used (parallel loop nesting)
\item Cuda C++: expected collective unit is \myKey{implicit}, user error = deadlock or UB
\end{itemize}

\mySub{CUDA C++ translation}

\begin{verbatim}
__global__ void <function name>()
{
    do_something_A();   // unit =     thread
    __syncthreads();    // unit = block
    do_something_B();   // unit =     thread
    __syncwarp();       // unit =   warp
    do_something_warp();// unit =   warp
    __syncthreads();    // unit = block
}
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Exo Syntax -- Synchronization 2/2, Actor Kind}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Conceptually, an async block instructs the compiler to lower to a \myKey{different class} of instructions with \myKey{different synchronization} requirements

For now, informally, this classification is the \myKey{actor kind}, which is one of the following:

\vspace{4mm}

Top level code:
\begin{itemize}
  \item \texttt{cpu}
\end{itemize}
\vspace{4mm}

\texttt{with CudaDeviceFunction(...):}
\begin{itemize}
  \item \texttt{cuda\_sync}
  \begin{itemize}
    \item Synchronous CUDA instructions
  \end{itemize}
\end{itemize}
\vspace{4mm}

\texttt{with CudaAsync(\textit{actor\_kind}):}
\begin{itemize}
  \item \texttt{non\_bulk\_cp\_async}
  \begin{itemize}
    \item Ampere async copy\\(uses generic proxy; not discussed)
  \end{itemize}
  \item \texttt{tma\_gmem\_to\_smem\_async}
  \item \texttt{tma\_smem\_to\_gmem\_async}
  \item \texttt{wgmma\_async}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Fence}

Parameterize with actor kind
\begin{verbatim}
  Fence(A1, A2)
\end{verbatim}
Within the executing collective lane, prior actions of actor kind \texttt{A1} happen before subsequent actions of actor kind \texttt{A2}

\mySub{Split Barrier}

Declare variable of \texttt{barrier} type
\begin{verbatim}
  <barrier var> : barrier
  Arrive(A1, <barrier var>)
  Await(<barrier var>, A2)
\end{verbatim}

Actions of actor kind \texttt{A1} prior to the $N^{th}$ arrive happen before actions of actor kind \texttt{A2} subsequent to the $N^{th}$ await.

We use orthogonal syntax but \myKey{not all} collective unit + actor kind combinations supported.

\mySub{Synthetic Actor Kind}

Additional actor kinds only used to parameterize synchronization,
e.g. \texttt{cuda\_all} (all CUDA instructions), \texttt{cuda\_generic} (generic proxy)

\end{minipage}

\newpage
\myTitle{Exo Syntax -- CudaAsync Block}

\begin{minipage}[t]{0.48\textwidth}\fixminipage

Syntax, \texttt{with CudaAsync(\textit{actor\_kind}): \textit{body}}

\begin{itemize}
  \item Nested in \texttt{with CudaDeviceFunction} block
  \item Changes actor kind from \texttt{cuda\_sync} to \texttt{\textit{actor\_kind}}
\end{itemize}
Often, there is \myKey{nontrivial interaction} between the async instructions and related synchronization
\begin{itemize}
\item Example: wgmma ``pipeline stage'' must appear in fixed pattern (that gets ptxas's vaunted stamp of approval)
\item Example: TMA \texttt{expect-tx} needs to be set to the number of bytes copied
\end{itemize}
\vspace{12mm}
For certain actor kinds, we \myKey{could} require
\begin{itemize}
\item \myKey{prologue synchronization}: first body stmt
\item \myKey{epilogue synchronization}: last body stmt
\end{itemize}
which the backend will compile non-trivially with respect to the async block's contents.

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{wgmma pipeline stage}
\begin{verbatim}
with CudaAsync(wgmma_async):
    # Prologue synchronization
    # wgmma.fence
    # NB wgmma_rmem is a synthetic actor kind
    Fence(wgmma_rmem, wgmma_rmem_async)

    # ... MMA instr ...

    # Epilogue synchronization
    # wgmma.commit_group
    Arrive(wgmma_async, <barrier var>)
\end{verbatim}
\mySub{TMA GMEM$\to$SMEM}
\begin{verbatim}
with CudaAsync(tma_to_smem_async):
    # ... TMA instr ...

    # Epilogue synchronization
    # Calculates expect-tx = bytes copied
    # (futher surprise, in the generated
    # CUDA C++, the Arrive is moved before
    # the TMA instructions!!!)
    Arrive(tma_to_smem_async,
           <barrier var>)
\end{verbatim}
\end{minipage}

\newpage
\myTitle{Syntax Example}
\begin{verbatim}
with CudaDeviceFunction(blockDim=384):
    # blockDim 384 = 3 warpgroups

    for yo in cuda_blocks(0, 24):
        for xo in cuda_blocks(0, 36):
        # Grid of 36 x 24 blocks (CTAs)

        with SpecializeCollective(cuda_warpgroup, 1, 3):
            for w in cuda_warpgroups(0, 2):
                # Skipped warpgroup 0 (specialize); 2 iters mapped to warpgroups 1 and 2
                bar: Barrier

                for yi in cuda_threads(0, 16):
                      for xi in cuda_threads(0, 8):
                          # 8 x 16 iteration space, each iter mapped to 1 of 128 threads

                with CudaAsync(wgmma_async):
                    # Now targetting wgmma; this is at warpgroup scope
                    # since wgmma requires coalesced warpgroups
                    Fence(wgmma_rmem, wgmma_async_rmem)  # prologue synchronization
                    for k in seq(0, 8):                  # ordinary sequential loop
                        wgmma_mma(...)
                    Arrive(bar, wgmma_async)             # epilogue synchronization
\end{verbatim}

\newpage
\myTitle{Contrast with prior work: Triton}

\begin{minipage}[t]{0.7\textwidth}\fixminipage
Triton is another Python AST $\to$ CUDA language

In my terminology, the ``collective unit'' for a Triton function is a block.
\begin{itemize}
  \item One block executes one ``call'' of a Triton function.
\end{itemize}

Triton's responsibility: \textbf{automatically} distribute the work \myKeyB{within} one block to sub-block units (mostly threads) and automatically synchronize.
\begin{itemize}
  \item Recall: synchronization within a block is \myKeyB{frequent}
  \item Minimal control over memory movement, register/SMEM allocation
  \item Honestly, not a bad model though
\end{itemize}

User responsibility: manually distribute work \myKey{between} blocks.
\begin{itemize}
  \item Recall: synchronization between blocks is \myKey{minimal}
  \item Often one output tile assigned per block
\end{itemize}

With Exo, we're trying to build something much more \textbf{imperative}
\begin{itemize}
  \item Control (and predict!) memory and register pressure
  \item Fine grained control over each level of CUDA thread hierarchy
  \item Explicit use of accelerator instructions
\end{itemize}
\end{minipage}

\vspace{6mm}
\hfill(Also, Triton generates IR and Exo GPU will generate C and CUDA C++)

\newpage
\myTitle{10 Miles Up: Synchronization Checking}

\begin{minipage}[t]{0.46\textwidth}\fixminipage
Each Exo proc may be interpreted in two ways:
\begin{itemize}
  \item \myKey{S-semantics} (Single-threaded)\\parallel for = sequential for,\\ignore async blocks
  \item \myKeyB{M-semantics} (Multi-threaded)\\Treat parallel for, async blocks correctly
\end{itemize}

Internally, Exo will continue to be a fundamentally sequential programming system
\begin{itemize}
  \item Expose parallel for, etc. to the user
  \item ... but rewrite rules, etc. will accept the correctness of these \myKey{without proof}
\end{itemize}

We will interpret procs under \myKey{S-semantics}\\until the very end, at code lowering time

\end{minipage}
\hfill
\begin{minipage}[t]{0.52\textwidth}\fixminipage
Prove interpretation under \myKey{S-semantics}\\and \myKeyB{M-semantics} will give the same results

Strat: reduce parallelism to checking assertions on conceptually \myKey{sequential} programs

Interpretation
\begin{itemize}
  \item $\Sigma$: environment: \myKey{mutable} values of variables
  \item $\Sigma^S$: synchronization environment (\myKey{SyncEnv}) -- \myKey{mutable} ``access history''
\end{itemize}
$\Sigma^S$ will track, for each variable
\begin{itemize}
  \item \myKey{History} of accesses (reads \& writes) done
  \item Provable \myKey{visibility} of past accesses to other threads
\end{itemize}

\end{minipage}
\vfill
\begin{tikzpicture}[node distance=8mm]
\node(proc0) [normalnode] {$proc_0$\\\myKey{S-semantics}};
\node(proc1) [normalnode, right=of proc0] {$proc_1$\\\myKey{S-semantics}};
\node(procNS) [normalnode, right=of proc1, xshift=+6mm] {$proc_N$\\\myKey{S-semantics}};
\node(procNM) [normalnode, right=of procNS] {$proc_N$\\\myKeyB{M-semantics}};
\node(cuda) [smallnode, right=of procNM] {CUDA C++};

\draw [arrow] (proc0) -- (proc1);
\draw [arrow, dotted] (proc1) to node(rewrites)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=7cm, minimum width=7cm, below=of rewrites, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg, xshift=-2cm] {Existing Exo Rewrites + \textbf{parallelism rewrites} (ignored in S-semantics)};
\node(sync) [normalnode, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Synchronization Checking};
\node(spork) [normalnode, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Compiler};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg] {Physically same proc, \textbf{different interpretation}};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (proc1);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\end{tikzpicture}

\newpage
% Title moved out of desperation
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\vspace{-5mm}
\myTitle{Actor Kind, Actor Signature}

\vspace{4mm}
Imagine a dynamic trace of an Exo-GPU proc
\begin{itemize}
  \item Sequential, since core Exo is sequential
  \item ... but we need to annotate which thread would do each action in M-semantics
  \item ... and whether it's an async instruction?
  \item ... to prove synchronization correctness
\end{itemize}
\myKey{Thread Index} = ``who'' did the access

\myKey{Actor Signature} = ``how'' the access was done

\myKey{Actor Kind}
\begin{itemize}
  \item Informal: classification of instructions
  \item Formal: \myKey{set} of allowed \myKey{actor signatures}
  \item 1-to-many: a single accelerator can perform accesses with different sync requirements
\end{itemize}

Example: actor kind \texttt{wgmma\_async}
\begin{itemize}
  \item \texttt{sig\_wgmma\_smem}: \myKeyB{SMEM} ($B$, maybe $A$)
  \item \texttt{sig\_wgmma\_rmem\_a}: \textbf{RMEM} $A$
  \item \texttt{sig\_wgmma\_rmem\_d}: \textbf{RMEM} $D$
  \begin{itemize}
    \item Only $D$ (accumulator) implicitly sync'd
  \end{itemize}
\end{itemize}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\myKey{sigthread} = (thread index, actor signature) pair
\begin{itemize}
  \item Conceptual sub-thread of each thread
  \item ``Signs'' each action done in a dynamic trace
\end{itemize}
\vspace{2mm}
\begin{verbatim}
for tid in cuda_threads(0, 1):
    # Only thread 0 does TMA (pseudocode)
    with CudaAsync(tma_to_smem_async):
        SMEM[0:N] = GMEM[c:c+N]
# ...
for tid in cuda_threads(0, N):
    Y[tid] = SMEM[tid]
\end{verbatim}
\vspace{-4mm}
\begin{align*}
    & {\color{blueBoxFg}\textit{[[ thread 0 kicks off TMA ]]}} \\
    \text{SMEM[$0$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+0$]} \\
    \text{SMEM[$1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+1$]} \\
    & ... \\
    \text{SMEM[$N-1$]} & \xleftarrow{\text{(0, sig\_tma\_to\_smem)}} \text{GMEM[$c+N-1$]} \\
    & {\color{blueBoxFg}\textit{[[ thread $x$ reads SMEM[$x$] ]]}} \\
    \text{Y[$0$]} & \xleftarrow{\text{(0, sig\_cuda\_sync)}} \text{SMEM[$0$]} \\
    \text{Y[$1$]} & \xleftarrow{\text{(1, sig\_cuda\_sync)}} \text{SMEM[$1$]} \\
    & ... \\
    \text{Y[$N-1$]} & \xleftarrow{\text{(N-1, sig\_cuda\_sync)}} \text{SMEM[$N-1$]} \\
\end{align*}
\end{minipage}
\newpage
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{SyncEnv Mental Model}  % Desperate for space again
\vspace{6mm}

\mySub{For Each Variable}

\begin{itemize}
  \item Remember the most recent write
  \item Remember \myKey{all prior reads}
  \item Track their visibility to sigthreads
\end{itemize}

\mySub{$\Sigma^S$ mutable}
\begin{itemize}
  \item Changes on both reads and writes
  \begin{itemize}
    \item Reads are \myKey{highly stateful}
  \end{itemize}
  \item Initial visibility = \myKey{sigthread} doing access
  \item Changes on barriers: visibility \myKey{grows}
\end{itemize}

\mySub{Intuition -- Simulation Mindset}
\begin{itemize}
  \item Simulate for loops sequentially
  \begin{itemize}
    \item This is a valid ``interleaving'' of threads\\
    (see code inset if confused)
  \end{itemize}
  \item Assert visibility prior to access
  \begin{itemize}
    \item On read: prior writes must be visibile
    \item On write: prior reads and writes `` ''
    \item Guarantees any interleaving works
  \end{itemize}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Rationale}

``Sequential'' analysis, but threads exist?
\begin{itemize}
  \item {[sig]}threads = metadata in sequential proc
  \item threads aren't real -- just a word
\end{itemize}
Stateful, simulation mindset?
\begin{itemize}
  \item Honestly, matches my intuition
  \item Easy soundness \& completeness$^{\textsf{[citation needed]}}$
  \item May or may not be basis for static analysis
  \begin{itemize}
    \item deduce $\Sigma^S$ at each program point
    \item can compromise on completeness
  \end{itemize}
\end{itemize}

\mySub{Code Inset}

\begin{verbatim}
for b in cuda_blocks(...):
    for i in cuda_threads(...):
        stuff_before_fence()
    # __syncthreads()
    Fence(cuda_sync, cuda_generic)
    for i in cuda_threads(...):
        stuff_after_fence()
\end{verbatim}
The \texttt{Fence} is outside the \texttt{cuda\_threads} loops; ensures S-semantics gives a valid interleaving
\end{minipage}

\newpage
\myTitle{SyncEnv State \hfill Correctness concept, not runtime state}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
The SyncEnv associates \myKey{visibility records} with variables, containing state
\begin{itemize}
  \item $V_S$: sync visibility set: set of \myKey{sigthreads}
  \begin{itemize}
    \item sigthreads that can ``see'' the read/write
  \end{itemize}
  \item $V_A$: async visibility set: set of \myKey{sigthreads}
  \begin{itemize}
    \item to model async instructions, later
  \end{itemize}
  \item $A_O$: original \myKey{actor signature}
\end{itemize}

\begin{itemize}
  \item one \blueBox{write} visibility record for last write
  \item one \redBox{read} visibility record for \myKey{each} read
\end{itemize}

With $(t: \mathsf{thread\_index}, a: \mathsf{actor\_signature})$ being the sigthread doing the access, \myKey{initialize}
\begin{itemize}
  \item $V_S$ = $\{(t,a)\}$ if the instruction is synchronous, otherwise $\{\}$
  \item $V_A$ = $\{(t,a)\}$
  \item $A_O$ = $a$
  \item Invariant: $V_S \subseteq V_A$
\end{itemize}
TODO
\begin{itemize}
  \item support for atomic reductions
  \item deleting ``old'' visibility records
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\ \\ % formatting is screwed without this
\begin{tikzpicture}[node distance=6mm]
\node(SyncEnv) [normalnode] {$\Sigma^S$: SyncEnv};
\node(vars) [normalnode, below=of SyncEnv] {variable};
\node(reads) [normalnode, below=of vars, xshift=-25mm, minimum width=4cm, text width=4cm, draw=redBoxFg, text=redBoxFg, fill=redBoxBg] {read visibility records\\\textit{red $\equiv$ ``read''}};
\node(writes) [normalnode, below=of vars, xshift=+25mm, minimum width=4cm, text width=4cm, draw=blueBoxFg, text=blueBoxFg, fill=blueBoxBg] {write visibility record\\\textit{blue $\equiv$ ``not read''}};
\node(record) [normalnode, below=of reads, xshift=+25mm] {visibility record};
\node(VA) [smallnode, below=of record] {$V_A$};
\node(VS) [smallnode, left=of VA] {$V_S$};
\node(AO) [smallnode, right=of VA] {$A_O$};
\node(sigthread) [normalnode, below=of VS] {sigthread};
\node(tid) [normalnode, below=of sigthread] {thread index};
\node(signature) [normalnode, right=of tid] {actor signature};

\draw [arrow, draw=keyColorB] ($(SyncEnv.south)$) -- ($(vars.north)$);
\draw [arrow, draw=keyColorB] ($(SyncEnv.south) - (0.2, 0)$) -- ($(vars.north) - (0.2, 0)$);
\draw [arrow, draw=keyColorB] ($(SyncEnv.south) + (0.2, 0)$) -- ($(vars.north) + (0.2, 0)$);

\draw [arrow] (vars) to[out=240,in=90] (reads);
\draw [arrow] (vars) to[out=300,in=90] (writes);

\draw [arrow, draw=keyColorB] ($(reads.south) - (0.2, 0)$) to[out=270, in=90] ($(record.north) - (0.3, 0)$);
\draw [arrow, draw=keyColorB] ($(reads.south) + (0.0, 0)$) to[out=270, in=90] ($(record.north) - (0.1, 0)$);
\draw [arrow, draw=keyColorB] ($(reads.south) + (0.2, 0)$) to[out=270, in=90] ($(record.north) + (0.1, 0)$);
\draw [arrow] ($(writes.south) - (0, 0)$) to[out=270, in=90] ($(record.north) + (0.3, 0)$);

\draw [arrow] (record) to[out=240, in=90] (VS);
\draw [arrow] (record) to[out=270, in=90] (VA);
\draw [arrow] (record) to[out=300, in=90] (AO);

\draw [arrow, draw=keyColorB] ($(VS.south) - (0.2, 0)$) to[out=270, in=90] ($(sigthread.north) - (0.5, 0)$);
\draw [arrow, draw=keyColorB] ($(VS.south)$) to[out=270, in=90] ($(sigthread.north) - (0.3, 0)$);
\draw [arrow, draw=keyColorB] ($(VS.south) + (0.2, 0)$) to[out=270, in=90] ($(sigthread.north) - (0.1, 0)$);

\draw [arrow, draw=keyColorB] ($(VA.south) - (0.2, 0)$) to[out=270, in=90] ($(sigthread.north) + (0.1, 0)$);
\draw [arrow, draw=keyColorB] ($(VA.south)$) to[out=270, in=90] ($(sigthread.north) + (0.3, 0)$);
\draw [arrow, draw=keyColorB] ($(VA.south) + (0.2, 0)$) to[out=270, in=90] ($(sigthread.north) + (0.5, 0)$);

\draw [arrow] (sigthread.south) to (tid);
\draw [arrow] (sigthread.south) to[out=315, in=135] ($(signature.north) - (0.1, 0)$);
\draw [arrow] (AO) to[out=270, in=90] ($(signature.north) + (0.1, 0)$);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{SyncEnv Barrier Visibility Sets}

\begin{minipage}[t]{0.48\textwidth}\fixminipage

Each \texttt{Fence($A_1$:\textsf{actor-kind}, $A_2$:\textsf{actor-kind})} statement is executed by a \myKey{collective lane}, itself defined by a range of threads $T$

\begin{itemize}
  \item e.g. warpgroup 2 may have $T = [256, 383]$
  \item Define \myKey{first visibility set}: $V_1 = T \times A_1$
  \begin{itemize}
    \item set of sigthreads ``signalling the fence''
  \end{itemize}
  \item Define \myKey{second visibility set}: $V_2 = T \times A_2$
  \begin{itemize}
    \item set of sigthreads ``waiting for the fence''
  \end{itemize}
\end{itemize}

A fence can be \myKey{transitive} or \myKey{non-transitive} (TODO elaborate)

Informally, the \myKeyB{synchronizes-with} relation associates prior reads/writes (\myKey{visibility records}) with the fences that ``\myKey{protect}'' them.

Formally, when we interpret a \texttt{Fence} statement, we say that a visibility record \myKeyB{synchronizes-with}
\begin{itemize}
  \item transitive fences: if \violetBox{$V_A \cap V_1$} non-empty
  \item non-transitive fences:\\if \violetBox{$V_A \cap V_1 \cap (\mathbb{N} \times A_O)$} non-empty
  \begin{itemize}
    \item $A_O$: original actor signature
  \end{itemize}
\end{itemize}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage

\mySub{Side Notes}

Recall that actor kinds are a set of actor signatures, hence the typing is correct
\begin{itemize}
  \item $T$: set of thread indices
  \item $A_i$: set of actor signatures
  \item $T \times A_i$: set of\\(thread index, actor signature): \myKey{sigthread}
\end{itemize}

\vspace{6mm}
\mySub{Split Barriers (Arrive/Await)}
\begin{itemize}
  \item See spork.pdf
  \item Essentially, \texttt{Arrive} gives $V_1$, \texttt{Await} gives $V_2$
\end{itemize}
\end{minipage}
\newpage
\begin{minipage}[t]{0.42\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{SyncEnv Barrier Effects}

\texttt{Fence} effect: do, \myKey{for each} visibility record in $\Sigma^S$ that \myKeyB{synchronizes-with} the fence,
\begin{itemize}
  \item $V_A \leftarrow V_A \cup V_2$
  \item $V_S \leftarrow V_S \cup V_2$
\end{itemize}
Really expensive if interpreted literally!

\mySub{Example Code}

\myKey{Block 0}: threads $T = [0, 255]$

\myKey{Block 1}: threads $T = [256, 511]$

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        smem : f32[256] @ CudaSmem
        for i in cuda_threads(0, 256):
            smem[i] = gmemRead[i]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
        Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}

\myKey{S-semantics}: interpret \texttt{b}-loop sequentially
\begin{itemize}
  \item Suppose $b=0$ already done
\end{itemize}

For block 1 ($b = 1$), let's sketch before and after
\violetBox{\texttt{Fence(cuda\_sync, cuda\_generic)}}
\begin{itemize}
  \item $V_1 = [256, 511] \times \texttt{cuda\_sync}$
  \item $V_2 = [256, 511] \times \texttt{cuda\_generic}$
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\textwidth}\fixminipage
\mySub{$\Sigma^S$ Before Fence}

\begin{tikzpicture}[node distance=2mm]
\node(gmem0) [normalnode] {\texttt{gmemRead[0]}};
\node(gmem255) [normalnode, below=of gmem0, yshift=-3mm] {\texttt{gmemRead[255]}};
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, below=of smem0, yshift=-3mm] {\texttt{smem[255]}};

\node(b0) [widenode, right=of gmem0, xshift=6mm, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [0,255] \times \texttt{cuda\_generic}$\\\textit{\color{lightttColor}reads from previous iter $b=0$}};
\node(r256) [widenode, below=of b0, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = \{ (256, \texttt{sig\_cuda\_sync}) \}$};
\node(r511) [widenode, below=of r256, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = \{ (511, \texttt{sig\_cuda\_sync}) \}$};
\node(w256) [widenode, below=of r511, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = \{ (256, \texttt{sig\_cuda\_sync}) \}$};
\node(w511) [widenode, below=of w256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = \{ (511, \texttt{sig\_cuda\_sync}) \}$};

\draw[dotted] (gmem0) -- (gmem255);
\draw[dotted] (smem0) -- (smem255);
\draw[line, draw=redBoxFg] (gmem0.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem0.east) -- (r256.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (r511.west);
\draw[line, draw=blueBoxFg] (smem0.east) --(w256.west);
\draw[line, draw=blueBoxFg] (smem255.east) --(w511.west);
\end{tikzpicture}

\mySub{$\Sigma^S$ After Fence}

\begin{tikzpicture}[node distance=2mm]
\node(gmem0) [normalnode] {\texttt{gmemRead[0]}};
\node(gmem255) [normalnode, below=of gmem0, yshift=-3mm] {\texttt{gmemRead[255]}};
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, below=of smem0, yshift=-3mm] {\texttt{smem[255]}};

\node(b0) [widenode, right=of gmem0, xshift=6mm, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [0,255] \times \texttt{cuda\_generic}$\\\textit{\color{lightttColor}unchanged as $V_A \cap V_1$ empty}};
\node(r256) [widenode, below=of b0, fill=redBoxBg, draw=redBoxFg, text=redBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic}$\\\textit{\color{lightttColor}read visibility set unioned with $V_2$}};
\node(w256) [widenode, below=of r256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic}$\\\textit{\color{lightttColor}write visibility set unioned with $V_2$}};

\draw [dotted] (gmem0) -- (gmem255);
\draw [dotted] (smem0) -- (smem255);
\draw[line, draw=redBoxFg] (gmem0.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (b0.west);
\draw[line, draw=redBoxFg] (gmem0.east) -- (r256.west);
\draw[line, draw=redBoxFg] (gmem255.east) -- (r256.west);
\draw[line, draw=blueBoxFg] (smem0.east) --(w256.west);
\draw[line, draw=blueBoxFg] (smem255.east) --(w256.west);
\end{tikzpicture}
\end{minipage}
\newpage
\myTitle{SyncEnv Barrier Example 2}

\begin{minipage}[t]{0.55\textwidth}\fixminipage
Suppose the block continues on to uses \texttt{smem} to stage a\\
\myKeyB{SMEM}$\to$\myKey{GMEM} TMA copy

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        smem : f32[256] @ CudaSmem
        for i in cuda_threads(0, 256):
            smem[i] = gmemRead[i]
        Fence(cuda_sync, cuda_generic)

\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
        # Only 0th thread per block does TMA
        for i in cuda_threads(0, 1):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            # fence.proxy.async
            Fence(cuda_sync, tma_to_gmem_async)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
            with CudaAsync(tma_to_gmem_async):
                # (placeholder) TMA instr
                gmemWrite[...] = smem[0:256]
\end{verbatim}
\end{mdframed}

For block \texttt{b=1}, \texttt{blockDim=256},\\
\violetBox{\texttt{Fence(cuda\_sync, tma\_to\_gmem\_async)}} has
\begin{itemize}
  \item $V_1 = \{(256, \texttt{sig\_cuda\_sync})\}$
  \item $V_2 = \{(\mathbf{256}, \textbf{\texttt{sig\_tma\_to\_gmem}})\}$
  \item Note, \texttt{sig\_cuda\_sync} $\in$ \texttt{cuda\_generic}
\end{itemize}
\blueBox{$(256, \texttt{sig\_cuda\_sync}) \in [256, 511] \times \texttt{cuda\_generic}$}
\end{minipage}
\hfill
\begin{minipage}[t]{0.43\textwidth}\fixminipage
\vspace{-8mm}
\begin{tikzpicture}[node distance=2mm]
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, right=of smem0, xshift=8mm] {\texttt{smem[255]}};
\draw[dotted] (smem0) to node(smemdots){} (smem255);

\node(rtma) [widenode, below=of smemdots, fill=redBoxBg, draw=redBoxFg, text=redBoxFg, yshift=-10mm] {$V_S = \{\}$ \textit{\color{lightttColor}(TMA \textbf{async} read)}\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem})\}$};
\node(w256) [widenode, minimum width=8cm, text width=8cm, below=of rtma, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic} \cup \{(\mathbf{256}, \textbf{\texttt{sig\_tma\_to\_gmem}})\}$\\\textit{\color{lightttColor}unioned with $V_2$ (bold part)}};
\node(wtma) [widenode, below=of w256, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg, yshift=-8mm] {$V_S = \{\}$ \textit{\color{lightttColor}(TMA \textbf{async} write)}\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem})\}$};
\node(gmemWrite) [normalnode, below=of wtma, yshift=-8mm] {\texttt{gmemWrite[...]}};

\draw[line, draw=redBoxFg] (smem0) -- (rtma);
\draw[line, draw=redBoxFg] (smem255) -- (rtma);
\draw[line, draw=blueBoxFg] (smem0) to[out=215,in=105] ($(w256.north) - (3.5, 0)$);
\draw[line, draw=blueBoxFg] (smem255) to[out=325,in=75] ($(w256.north) + (3.5, 0)$);
\draw[line, draw=blueBoxFg] (gmemWrite) to[] (wtma);
\end{tikzpicture}

Recall,
\begin{itemize}
\item $V_S$: \myKey{sync visibility set}
\item $V_A$: \myKey{async visibility set}
\end{itemize}
\end{minipage}

\newpage
\myTitle{Synchronization Checking}

Each read/write access is done with a certain sigthread $(t, a)$

The checks are done \myKey{prior to any changes} to $\Sigma^S$ (SyncEnv)

For a read/write visibility record $x$, denote its \myKey{sync visibility set} as $V_S^x$

\mySub{Checking on Read}

\myKey{RAW} check: Check $(t,a) \in V_S^w$, where $w$ is the \blueBox{write} visibility record.\\
If so, we say that the write recorded by $w$ is \myKeyB{visible-to} sigthread $(t,a)$.

\mySub{Checking on Write}

\myKey{WAW} check: Check write $w$ is \myKeyB{visible-to} $(t,a)$

\myKey{WAR} check: For each \redBox{read} visibility record $r$, check that $(t,a) \in V_S^r$.\\
If so, we say that the read recorded by $r$ is \myKeyB{visible-to} sigthread $(t,a)$.

\mySub{\textbf{IMPORTANT:}} These are \textbf{NOT} ``allowed to read'' and ``allowed to write'' sets.\\
It's more complicated than that.

\hfill
\begin{minipage}[t]{0.7\textwidth}\fixminipage
\vspace{6mm}
\textit{Deeper side note, not for live presentation: \myKey{WAR} checking is a ``conjunction of disjunctions''; with the disjunction being how each read visibility set $V_S$ gets independently augmented by synchronization ($V_S \leftarrow V_S \cup V_2$), and the conjunction being $\forall r, (t,a) \in V_S^r$.\\
This is not really simplifiable to a single ``allowed to write'' set.}
\end{minipage}
\newpage
% Title again moved out of desperation
\begin{minipage}[t]{0.55\textwidth}\fixminipage
\vspace{-6mm}
\myTitle{Synchronization Checking Example}

Suppose immediately after the TMA, we issue another \violetBox{\texttt{Fence}}.
Then, all threads in the block write to \texttt{smem} again.

\begin{verbatim}
with CudaDeviceFunction(256):
    for b in cuda_blocks(0, 2):
        # ...
        for i in cuda_threads(0, 1):
            # fence.proxy.async
            Fence(cuda_sync, tma_to_gmem_async)
            with CudaAsync(tma_to_gmem_async):
                # (placeholder) TMA instr
                gmemWrite[...] = smem[0:256]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            # Inadequate Fence! (1 thread only)
            Fence(tma_to_gmem_async, cuda_sync)
\end{verbatim}
\end{mdframed}
\hfill\textit{$\Sigma^S$ shown for this location}
\vspace{2mm}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
        for i in cuda_threads(0, 256):
            smem[i] = ...
\end{verbatim}
\end{mdframed}

This \myKey{should fail} synchronization checking, since all but one thread fails to wait for the TMA.

\begin{itemize}
  \item $V_1 = \{(256, \texttt{sig\_tma\_to\_gmem})\}$
  \item $V_2 = \{(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$
\end{itemize}
Recall we're analyzing block $b=1$, i.e. threads $[256, 511]$\\
(but $T = \{256\}$ alone for the thread-scoped Fence)

\end{minipage}
\hfill
\begin{minipage}[t]{0.43\textwidth}\fixminipage
\vspace{-8mm}
\begin{tikzpicture}[node distance=2mm]
\node(smem0) [normalnode, below=of gmem255] {\texttt{smem[0]}};
\node(smem255) [normalnode, right=of smem0, xshift=8mm] {\texttt{smem[255]}};
\draw[dotted] (smem0) to node(smemdots){} (smem255);

\node(incoming256) [normalnode, above=of smem0, yshift=6mm, fill=greenBoxBg] {Incoming write\\$(t,a) = (256,$\\\texttt{sig\_cuda\_sync})};
\node(incoming511) [normalnode, above=of smem255, yshift=6mm, fill=greenBoxBg] {Incoming write\\$(t,a) = (511,$\\\texttt{sig\_cuda\_sync})};

\draw[dotted] (incoming256) to (incoming511);
\draw[arrow] (incoming256) to (smem0);
\draw[arrow] (incoming511) to (smem255);

\node(rtma) [widenode, below=of smemdots, fill=redBoxBg, draw=redBoxFg, text=redBoxFg, yshift=-10mm] {$V_S = \{(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$\\$V_A=\{(256, \texttt{sig\_tma\_to\_gmem}),$ $(\mathbf{256}, \textbf{\texttt{sig\_cuda\_sync}})\}$\\\textit{\color{lightttColor}$V_A,V_S$ unioned with $V_2$}};
\node(w256) [widenode, minimum width=8cm, text width=8cm, below=of rtma, fill=blueBoxBg, draw=blueBoxFg, text=blueBoxFg] {$V_S,V_A = [256, 511] \times \texttt{cuda\_generic} \cup \{(256, \texttt{sig\_tma\_to\_gmem})\}$};

\draw[line, draw=redBoxFg] (smem0) -- (rtma);
\draw[line, draw=redBoxFg] (smem255) -- (rtma);
\draw[line, draw=blueBoxFg] (smem0) to[out=215,in=105] ($(w256.north) - (3.5, 0)$);
\draw[line, draw=blueBoxFg] (smem255) to[out=325,in=75] ($(w256.north) + (3.5, 0)$);
\end{tikzpicture}

\myKey{WAW check pass}

$\forall t \in [256, 511]$,\\\greenBox{$(t,a)$} $\in$ \blueBox{$[256, 511] \times \texttt{cuda\_generic} \cup ...$}\\
(write visibility record's $V_S$)

\myKey{WAR check fail}

Fails for $t \ge 257$

\greenBox{$(t,a)$} $\notin$ \redBox{$\{(256, \texttt{sig\_cuda\_sync})\}$}\\
(read visibility record's $V_S$)
\end{minipage}

\newpage
\myTitle{Conclusion: Exo GPU Usage}

\begin{itemize}
  \item Schedule Exo using existing rewrites -- parallelism ignored
  \item Wrap CUDA code with \myKey{\texttt{with CudaDeviceFunction}}
  \item Within CUDA code, change the \myKey{\texttt{LoopMode}} of loops to convert from \texttt{seq} to CUDA parallel for
  \begin{itemize}
    \item Top-level loops within \texttt{CudaDeviceFunction} block must be \myKey{\texttt{cuda\_blocks}} or \myKey{\texttt{cuda\_clusters}}
  \end{itemize}
  \item Replace memory types
  \item Wrap async code with \myKey{\texttt{with CudaAsync($A:\textsf{actor-kind}$)}} and substitute TMA or wgmma \texttt{instr}
  \begin{itemize}
    \item $A$ = \texttt{wgmma\_async}: wgmma (tiled matrix multiply-accumulate)
    \item $A$ = \texttt{tma\_to\_smem\_async}, \texttt{tma\_to\_gmem\_async}: TMA (tiled async matrix copy)
    \item $A$ = \texttt{non\_bulk\_cp\_async}: Ampere async memcopy
  \end{itemize}
  \item Insert sufficient synchronization statements: \texttt{Fence($A_1$, $A_2$)}, \texttt{Arrive($A_1$, bar)}, \texttt{Await(bar, $A_2$)}
  \begin{itemize}
    \item Substitute appropriate \myKey{actor kinds} $A_1$, $A_2$
  \end{itemize}
  \item Code lowering: Exo checks equivalence of \myKey{S-semantics} and \myKeyB{M-semantics}
  \begin{itemize}
    \item \myKey{S-semantics}: treat parallel for as sequential for, ignore async, ignore barriers
    \item \myKeyB{M-semantics}: authentic parallelism
    \item We could optionally insert simulation of the SyncEnv ($\Sigma^S$)
    \item TODO: figure out how to statically prove correctness with the $\Sigma^S$ model
  \end{itemize}
\end{itemize}

\newpage
\myTitleExtra{Extra Discussion Slide: Atomics in SyncEnv}

Synchronization checks currently not extended to atomic reductions
\begin{itemize}
  \item assumes read-only access can be concurrent
  \item assumes \myKey{mutable access} must be serialized (atomics break this assumption)
\end{itemize}

We cannot just assume atomics are ``trivially correct''
\begin{itemize}
  \item we want to prove \textit{equivalence} between \myKey{S-semantics} and \myKeyB{M-semantics}, not just race freedom
  \item TMA reduce still has the async proxy issue
\end{itemize}

Currently we will only model relaxed memory model atomics
\begin{itemize}
  \item i.e. Exo won't support clever homemade synchronization with acquire/release atomics
  \item besides this is what TMA provides anyway
\end{itemize}

Atomics are sort of like a read??? In that they can be concurrent.
\begin{itemize}
  \item more complex though, since there's also a ``\myKey{scope of atomicity}'', e.g. CTA-scoped atomics don't allow concurrent atomics across different CTAs; cross-proxy access is unclear.
\end{itemize}

Basically, we now have 3 ``categories'' of actions: reads, atomic reduce (ignore output), and writes
\begin{itemize}
  \item Different action categories must be serialized (e.g. read after atomic, write after read)
  \item Reads may always be concurrent; atomics can \textit{conditionally} be concurrent
\end{itemize}

\newpage
\myTitleExtra{Extra Discussion Slide: Memory Types \& Kernel Launch}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Exo \texttt{instr}s expect a specific memory type

Too inflexible for the GPU; generalize?

\mySub{Memory Property}

An \texttt{instr} requirement like \myKey{device visible} may apply to multiple memories, e.g. GMEM, SMEM, grid constant, pinned memory (CPU visible)

\mySub{Memory Layout}

Swizzle mode (wgmma) = \myKey{memory property}?

\begin{itemize}
  \item Cutlass/CuTe have deep systems for this but I think that's too out-of-scope
  \item Can't express swizzling as \texttt{divide\_dim}, etc.
  \item Should we stop hard-wiring C-style arrays?
  \item Also need to reason about alignment
\end{itemize}

\mySub{Memory Access}

\texttt{can\_read}, \texttt{write}, \texttt{reduce} needs change
\begin{itemize}
  \item Need to consider the \myKey{actor kind}
  \item Need to consider non-C memory layout, \myKey{distributed memory} (next slide)
  \item Prefer not to force using custom \texttt{instr}
\end{itemize}


\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Fat Pointers}

TMA access \myKey{GMEM} via a tensorMap, which hard wires the \myKeyB{SMEM} matrix (box) size
\begin{itemize}
  \item In Exo, the box size is defined by the TMA \texttt{instr}'s window dim
  \item Need to propagate this back to the CPU and prepare the tensorMap outside CUDA code
\end{itemize}

Similarities with other APIs (e.g. descriptor sets)
\begin{itemize}
  \item Externalizable? Hard, questionable payoff
\end{itemize}

\mySub{Grid Constant}

CUDA device function parameters \myKey{copied} from CPU to GPU -- scalars or small fixed-size arrays

Model as \texttt{GridConstant} \myKey{memory property}?
\begin{itemize}
  \item CPU read and write
  \item read-only on GPU
  \item no sync checking -- copy protects us
  \item backdoor-ey: special case for compiler
\end{itemize}
Also in other APIs (e.g. push constants)
\end{minipage}
\newpage
\myTitleExtra{Extra Discussion Slide: Distributed Memory}

\begin{minipage}[t]{0.48\textwidth}\fixminipage

A single logical array may have its values distributed across collective lanes, e.g.
\begin{itemize}
  \item wgmma $A$ or $D$ matrix -- distributed among warpgroups's 128 threads' registers
  \item user may also want to define their own tiles split across thread registers
  \item distributed shared memory -- cluster-scope allocation distributed among blocks
\end{itemize}

Poses correctness and implementation challenges
\begin{itemize}
  \item Must be transparent to \myKey{S-semantics}
  \begin{itemize}
    \item Ordinary array as far as it's concerned
  \end{itemize}
  \item Changes read/write syntax
  \item Need to check each collective lane doesn't access another lane's values
  \begin{itemize}
    \item \myKey{S-semantics}: most of Exo is blind to this, so hard to forbid this \textit{a priori}
  \end{itemize}
  \item Limited communication across collective lanes may be possible (shuffles, multicast)
\end{itemize}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\begin{itemize}
  \item Could make it a \myKey{memory property}?
  \item ... but user may want custom tiling?
  \item \myKey{collective assignment} (next slide)?
\end{itemize}

\mySub{Common Use Case}

Even without wgmma, we need distributed memory due to scoping and fences

\begin{verbatim}
# At block (CTA) scope
matrix_tile: f32[64, 64] @ CudaRmem
for ko in seq(0, num_k):
    for yi in cuda_threads(0, 16):
        for xi in cuda_threads(0, 16):
            # Stage SMEM
    Fence(cuda_sync, cuda_generic)
    for yi in cuda_threads(0, 16):
        for xi in cuda_threads(0, 16):
            # Accum matrix_tile
    Fence(cuda_sync, cuda_generic)
\end{verbatim}

The $64 \times 64$ tile could be split into $4 \times 4$ register tiles distributed into $16 \times 16$ threads

Can't just declare a $4 \times 4$ tile at \texttt{cuda\_threads} scope -- alloc must persist across block-scoped \texttt{Fence} stmts
\end{minipage}

\newpage
\myTitleExtra{Extra Discussion Slide: Collective Assignment}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
Crazy Idea: mapping between N-dimensional iteration space and 2-tuple:
\begin{itemize}
  \item (\myKeyB{collective coordinate},
  \item \myKey{non-collective coordinate})
\end{itemize}

(I didn't want to re-implement CuTe but humor me)

Concept appears in a variety of contexts:

\mySub{Distributed Memory}

Register tiles need a mapping between tensor coordinates and 2-tuples
\begin{itemize}
  \item (\myKeyB{collective coord}: relative thread index,
  \item \myKey{non-collective coord}: register index)
\end{itemize}

We restrict each thread to accessing only tensor coordinates with matching \myKeyB{collective coordinate}

Similar logic for other collective units (e.g. the \myKeyB{collective coordinate} for distributed shared memory is the cluster-relative block index)

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Parallel-for Work Distribution}

The order that logical per-block tasks are executed by physical blocks can make a \myKey{huge difference} to speed (L2 utilization)

Strategy depends on runtime matrix size -- hard to statically tile.
Map \texttt{cuda\_blocks} iteration space to
\begin{itemize}
  \item (\myKeyB{collective coord}: physical block index,
  \item \myKey{non-collective coord}: time)
\end{itemize}

\mySub{Parview -- Parallel View}

We may need to add some annotations to help static analysis (borrowed concept from \textit{Descend})

Could define a \textit{partial} mapping from tensor coordinates to
\begin{itemize}
  \item (\myKeyB{collective coord}: collective lane index,
  \item \myKey{non-collective coord}: unused)
\end{itemize}
Collective lanes may only write to tensor coordinates assigned to themselves

Collective lanes may only read from unmapped tensor coordinates
\end{minipage}

\newpage
\myTitleExtra{Extra Discussion Slide: Rewrite Rule Issues (special cases for value 0)}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{Predication}

\texttt{divide\_loop} if guard causes issues:

\begin{verbatim}
for io in cuda_blocks(0, ...):  # outer loop
    for ii in seq(0, TileSize): # inner loop
        if <boundscheck>:
            Read(...)
            Arithmetic(...)
            Write(...)
\end{verbatim}

With \greenBox{TMA} predication, we want to fission the inner loop (\texttt{ii}) and the bounds check, but then eliminate the bounds check from the \redBox{arithmetic}

\begin{verbatim}
for io in cuda_blocks(0, ...):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\begin{verbatim}
    for ii in seq(0, TileSize):
        if <boundscheck>:  # boundscheck
            Read(or 0...)  # in TMA instr
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=redBoxBg]
\color{redBoxFg}
\begin{verbatim}
    for ii in seq(0, TileSize):
        Arithmetic(...)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    for ii in seq(0, TileSize):
        if <boundscheck>:  # boundscheck
            Write(...)     # in TMA instr
\end{verbatim}
\end{mdframed}

This is only valid due to TMA giving 0 for out-of-bounds, and only for certain arithmetic

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
\mySub{wgmma 0-initialization}

The pattern with wgmma is \textbf{not} to 0-initialize the accumulator $D$, i.e. not this
\begin{verbatim}
D = 0
for ko in seq(0, K // 16):
    with CudaAsync(wgmma_async):
        D = AB + D  # pseudocode
\end{verbatim}
but rather
\begin{verbatim}
D = <uninitialized>
for ko in seq(0, K // 16):
    with CudaAsync(wgmma_async):
        # pseudocode
        D = ko == 0 ? AB : AB + D
\end{verbatim}
which is implemented with the \texttt{scale-d} parameter.

We will likely use \texttt{stage\_mem} with \texttt{accum=True} for such patterns, which implicitly inserts the 0-initialization.
This needs to be modified, or we need a follow-up rewrite rule.

(note, this assumes the loop runs at least once)
\end{minipage}
\end{document}
