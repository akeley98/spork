% xelatex </dev/null spork.tex
\input{whitepaper_common.tex}

\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{document}
\myTitle{Project Spork: EXO GPU}

Prototyping for how to extend Exo to safely support GPU accelerators (specifically what CUDA offers), including features like \lighttt{memcpy\_async} and wgmma, which I've argued are impossible to model using a fork/join approach while preserving maximum throughput. We will still be taking the approach of proving equivalence between ``sequential (single-threaded) semantics'' and ``parallel (multi-threaded) semantics'', where parallel-for loops and barriers are modelled as sequential-for loops and no-ops respectively under sequential semantics.

\mySub{Goals}

\myKey{Supported CUDA Features:} Hierarchical parallel-for over clusters, CTAs (blocks), warpgroups, warps, and threads; support simple barriers (e.g. \lighttt{\_\_syncthreads}); split barriers (\mbarrier); \lighttt{memcpy\_async} (a.k.a. \webText{cp.async and cp.async.bulk}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-asynchronous-copy}); \webText{wmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions} (warp matrix instructions); \wgmma\ (async warpgroup matrix instructions).

\filbreak
\myKey{Minimize synchronization overhead:} As pointed out by Hazy Research (and others), it's absolutely critical that the tensor cores are fed each cycle; you cannot later compensate for the performance lost for each missed cycle. Therefore for maximum performance, the new split barriers \textit{must} be used. You can't afford the bubble caused by \lighttt{\_\_syncthreads}. This eliminates fork-join as a viable model for maximally performance accelerators.

\filbreak
\myKey{Safety:} We need to prove race freedom, but moreover prove that each read gets the \textit{expected} previously-written value (or the initial input value). This is beyond what \textit{Descend} handles, which is only race freedom, i.e. in some code like

{\color{lightttColor}
\begin{verbatim}
Thread 0: x = 3;             x = 5;
Thread 1:         y = 10*x;
\end{verbatim}
}

the \textit{Descend} borrow-checking model only requires proving that thread 1's read of \lighttt{x} does not overlap with either of thread 0's assignments to \lighttt{x}, and so \lighttt{y = 10*}$x_0$, \lighttt{y = 30}, and \lighttt{y = 50} are all possible outcomes depending on how the user synchronized the two threads. Wheras for Exo we will check that the user's synchronization will guarantee the single outcome that matches that given by the equivalent code under single-threaded semantics.

\filbreak
\myKey{Ring Buffer:} We need to support ring buffer optimization under multi-threaded semantics as well.

\filbreak
\myKey{Array Race Analysis:} We may need a mechanism similar to the ``view'' concept in \textit{Descend} to make more complicated array access patterns tractible to analyze. i.e. we need to allow the user to prove that two threads' parallel writes to the same array won't cause a hazard by showing the access pattern leads to disjoint array indices used for each thread.

\filbreak
\myKey{Other Features:} I am thinking less about these features as they are not as relevant to the AI-centric use case for Hopper, but nevertheless in the background we should think about \webText{grid group synchronization}{https://docs.nvidia.com/cuda/cuda-c-programming-guide/\#grid-group} (pre-Hopper method for synchronizing across all threads in an entire grid), associative atomic reductions (add, min, max), and maybe modelling the extremely powerful \webText{cub prefix sum}{https://nvidia.github.io/cccl/cub/api/structcub_1_1DeviceScan.html} functions.

\filbreak
\mySub{Project Scope}

For the most part it seems like this work will be an add-on just prior to code generation, where we check sequential and parallel equivalence. I hope there's relatively few \hook{``hooks''} needed in the rest of Exo for this work. I say this because for now to make this project feasible, imo it's best to bake-in assumptions about CUDA's synchronization model, and not try to generalize that, the way we might for modeling shared memory (as a memory type) or specific accelerator functions. So ideally it would not be too much of a maintenence burden to remove the initial Exo GPU code if it turns out not to be the best approach long term.

\filbreak
For the more complicated synchronization, a more ``declarative'' model (where the user specifies what blocks of code they intend to synchronize, rather than working at the level of individual fence/barrier instructions) may be better for making proving correctness feasible.

\newpage
\myTitle{Background on GPU Features}

TODO: verify \flagged{flagged} claims.

The primary point of this is to take a census of the synchronization patterns we would have to model and how they interact with async copies \& wgmma.
Other details like swizzling etc. (except sparsity?) should be modellable with enough work using Exo's existing features.

\myKey{WARNING:} The links I have should take you to the correct subsection of the giant PTX documentation, but there is a bug where sometimes the Nvidia website warps you back to the top of the page. Select the address bar and press enter to fix this.

\filbreak
\mySub{Synchronization Summary}

CUDA has two different notions of ``async'': asynchronous instructions (simple enough), and the more complicated \webText{async proxy}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#proxies}. An instruction being async implies control continues to the next instruction without waiting for completion. If an async instruction further documents that it operates on the async proxy, the user (in addition to synchronizing execution order) must further include \fenceProxyAsync\ (and also maybe \wgmmaFence) to ensure visibility between the async proxy and the ``generic proxy'', which is what most CUDA instructions operate on. Essentially, this is exporting the responsibility for memory coherence to the user.

Note in the case of mbarrier that this incoherence includes observing the mbarrier object (in shared memory) itself: ``To make the initialized barrier visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used. This instruction ensures that subsequent bulk-asynchronous copy operations operate on the initialized barrier'' (\webText{source}{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#tensor-memory-access}).

\filbreak
Of the new instructions we want to model,

\begin{enumerate}
  \item non-bulk async copy (\cpAsync) are asynchronous instructions operating on the generic proxy;
  \item bulk async copy (\cpAsyncBulk; Hopper TMA) are asynchronous instructions operating on the async proxy;
  \item pre-Hopper tensor cores (\webText{wmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions-wmma-mma}) are not async at all;
  \item Hopper tensor cores (\wgmma) are asynchronous instructions operating on the async proxy.
\end{enumerate}

\filbreak
For the most part there seem to be four categories of synchronization primitives:

\filbreak
\myKey{All-to-all:} Like \lighttt{\_\_syncthreads} and cooperative group syncs; useful in the common case but these do nothing for asynchronous instructions.

\filbreak
\myKey{mbarrier:} Split barrier where consumer threads wait for a certain number of producer threads to arrive.
This may be used to synchronize ordinary non-async instructions, synchronize non-bulk \cpAsync\ (Ampere), and to Hopper \cpAsyncBulk\ (TMA) instructions that copy to shared memory.
The \mbarrier\ must be constructed in shared memory.
The \mbarrier\ can only be used to wait for the current (incomplete) set of arrives or the immediately prior complete set of arrives; thus, a ring-buffer of \mbarrier\ objects would be needed to implement pipelining.

\filbreak
\myKey{Async-group:} Supported with separate instructions for \webText{non-bulk async copy}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms}, bulk async copies to global memory, and for \webText{wgmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group}.
These don't require constructing state like mbarrier.
Each wgmma or async copy instruction is initially uncommitted, and is commited to a new async-group with a commit\_group instruction.
You may then wait for the Nth previous async-group of the same thread to finish with a wait\_group instruction.
This may implement deep pipelining but cannot be used to implement separate producer/consumer threads.

\filbreak
\myKey{Fences:} The previous primitives only synchronize execution order, and as mentioned, for the async proxy, a fence is needed in addition to this to ensure visibility.
wgmma and bulk copy instructions include an implicit fence afterwards, so only execution order synchronization is needed to see the outputs; the reverse is not true (i.e. generic proxy code generating inputs for async proxy instructions requires a fence).
wgmma also requires fences for \textit{registers}, to be described in the dedicated wgmma subsection.

\filbreak
\mySub{Asynchronous Memory Copy}

According to William, it's best to use inline PTX instead of the CUDA C++ \lighttt{cuda::memcpy\_async} function, as the C++ function is too prone to silently decaying to a regular memory copy. Async copies can be non-bulk (requires sm\_80 a.k.a. Ampere or higher) or bulk (sm\_90; Hopper).

\filbreak
\myKey{Ampere / Non-bulk:} We use the \cpAsync\ instruction to issue a single asynchronous copy of 4, 8, or 16 aligned bytes from global memory to shared memory. This is the only source+destination memory type supported. Presumably the CUDA C++ async copy functions for Ampere are implemented by distributing the copy over the participating threads.

All cp.async instructions issued are not implicitly ordered with each other, not even in the same thread, so write-after-write hazards are possible if two cp.async instructions share the same destination address. Since these operate on the generic proxy, we still do need synchronization after the \cpAsync\ instruction, but synchronization isn't needed for \cpAsync\ to see global memory writes issued by non-async instructions earlier in program order, as they also run in the generic proxy and ``asynchronous operations are ordered after prior instructions in the same thread'' (\webText{Asynchronous Operations}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#program-order-async-operations}).

\filbreak
\myKey{Hopper TMA / Bulk:} \cpAsyncBulk\ instructions allow you to instead specify a range of 16-byte-aligned memory to copy, and supports these src/dst memory types: global to cluster-shared, cluster-shared to CTA-shared, and CTA-shared to global. The memory types determine whether commit\_group or mbarrier must be used (completion mechanism). Unlike non-bulk async copy, the expected usage is to nominate just one thread to issue the instruction: ``...the TMA programming model is single-threaded, where a single thread in a warp is elected to issue an asynchronous TMA operation'' (\hopperBlog). As a reminder, this operates on the async proxy.

\filbreak
Copies \textit{to} global memory allow for an optional \lighttt{.reduce} op parameter (e.g. add, max, bitwise or).
If supplied, instead of each input \lighttt{src[i]} overwriting \lighttt{dst[i]}, we instead execute a parallel component-wise reduction \lighttt{dst[i] = reduceOp(dst[i], src[i])}.
I suspect this is used to implement some form of near-data processing.

\filbreak
This reduction (in global memory) is done \textbf{atomically}, with \lighttt{relaxed.gpu} semantics.
This is an incredible feature for split-$k$ and such, so we will need to model atomic operations.
Note for floating point numbers, this is non-deterministic, due to non-associativity, but the Exo rewrite model already ignores this so this should be OK.

\filbreak
The non-tensor versions of TMA instructions copy linear blocks of memory.
The tensor variant is much more interesting; it copies 1D-5D tiles of a fixed \textbf{box size}.

\begin{itemize}
\item SMEM tile: densely packed C-order matrix, sized by box size
\item GMEM tile: tile from big C-order matrix
\begin{itemize}
  \item \textbf{Predicated} and strided (16B aligned) -- 0-fill when reading out of bounds
\end{itemize}
\item 16 byte aligned; cannot stride innermost dim
\end{itemize}

\begin{tikzpicture}[node distance=5mm]
\node (smem) [rectangle, draw=black] {SMEM tile};
\node (gmembig) [rectangle, minimum height=12mm, minimum width=2.6cm, draw=black, right=of smem, yshift=1mm] {};
\node (gmem) [rectangle, draw=black, fill=white, right=of smem, xshift=9mm] {GMEM tile};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);
\end{tikzpicture}

This requires encoding a \lighttt{CUtensorMap}; for the most part, this is a ``fat pointer'' containing the GMEM matrix address and strides, but it also contains the \textbf{box size}.
Only the host CPU can encode a \lighttt{CUtensorMap}, which is then used on the CUDA device.
This is kind of annoying; when compiling a TMA instruction, the box size is logically given in the CUDA device code, but we have to propagate it back to the CPU in order to prepare the \lighttt{CUtensorMap} at the site of the CUDA kernel launch.

\filbreak
\myKey{commit\_group:} The commit\_group mechanism has a per-thread scope for bulk and non-bulk async copies. So it's rather unclear to me how this mechanism is useful in the common case where we cache stuff in shared memory for all the threads to read from. (Contrast with wgmma async-groups, which have per-warpgroup scope). I'll discuss the more complicated mbarrier in the next section.

Fortunately, non-bulk \cpAsync\ allows for \mbarrier\ as an alternative, and for TMA, the common case of copying \textit{to} shared memory is also handled by \mbarrier\ (copying from shared to global requires commit\_group).

\filbreak
\mySub{mbarrier}

\myKey{Split Barrier:} The \mbarrier\ may be used to implement split barriers. This opaque object is initialized in shared memory with an ``expected arrival count'' $n$, then, each \webText{``phase''}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-phase}=0,1,2,... of the mbarrier consists of

\begin{enumerate}
  \item $n$-many threads calling \webText{mbarrier.arrive}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-mbarrier-arrive} on the mbarrier.
  \item \textit{one} thread successfully calling \webText{mbarrier.test\_await}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait} or similar instruction on the barrier. This makes the mbarrier ready for the next phase.
\end{enumerate}

\filbreak
Instructions performed in the \textit{generic proxy} following the await operation observe changes made in the generic proxy by \textit{non-async} instructions in the arriving threads, prior to their arrival.

\filbreak
The ``one thread'' condition for advancing the phase complicates implementing multiple threads waiting on an mbarrier. The mechanism for \webText{this sort of  wait}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait} is either to pass the \lighttt{state} operand (from mbarrier.arrive) or specify the parity of the phase to wait for with the \lighttt{phaseParity} operand; either way, this allows the wait instruction to know whether to immediately return due to the previous phase being complete, or block waiting for the current phase.

\filbreak
It's required that no ``arrive'' operation for phase $n+1$ occurs until an ``await'' operation for phase $n$ is successful.
This requirement is easily met for self-synchronizing $n$-many threads (with useful work possible between the alternating arrives and waits), but is harder if implementing a producer/consumer threads model.
It's also not allowed to wait for phase $n-2$ or older (in constrast to async-group).
Hence my comment about ring-buffers of mbarriers.

\filbreak
\myKey{Non-bulk Async Copy mbarrier:} The \webText{cp.async.mbarrier.arrive}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive} instruction takes the place of mbarrier.arrive in the above description, causing the awaiting threads to observe changes performed by non-bulk \cpAsync\ instructions issued by the arriving threads, \textit{instead} of changes by non-async instructions issued.

Question: what use cases \lighttt{.noinc} has here.

\filbreak
\myKey{Bulk Async Copy mbarrier:} Since the expected model for bulk async copies is to nominate just one thread to issue the copy, the expected usage is to set expected arrival count $n = 1$, and use the additional tx-count feature: execute an \expectTxOperation\ (\webText{mbarrier.expect\_tx}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-expect-tx}) with the number of bytes expected to be copied, then use the \completeTxOperation\ built into \cpAsyncBulk\ to signal completion. (\webText{example code}{https://research.colfax-intl.com/tutorial-hopper-tma/})

\filbreak
\mySub{wmma}

Documentation: \webText{Warp-level matrix-multiply-accumulate}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions-wmma-mma}

These don't seem so bad at all. We are just distributing the storage for a ``matrix fragment'' (matrix tile) across the registers of 32 threads in a warp, with a wide variety of supported row-major and column-major formats, and computing multiply-add entirely in registers. There seem to be no synchronization requirements. TODO investigate sparse operations.

\filbreak
\mySub{wgmma}

Documentation: \webText{Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions}

These \webText{wgmma.mma\_async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-operation-wgmma-mma-async} instructions support $D = AB + D$ or $D = AB$ operations on matrix fragments. Unlike wmma, the work is distributed across warpgroups of 128 aligned threads; there are very complicated synchronization requirements; and fragment storage may be in shared memory or distributed across 128 threads' registers, with $A$ in either, $B$ in shared memory only, and the accumulator $D$ in registers only.

\filbreak
As mentioned, the execution order is handled by the fairly straightforward pipelined async-group mechanism: \webText{wgmma.commit\_group}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-multiply-and-accumulate-instruction-wgmma-commit-group} and \webText{wgmma.wait\_group}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-multiply-and-accumulate-instruction-wgmma-wait-group}.
In constrast to bulk async copy, register matrix values themselves may be modified asynchronously; hence, it's forbidden to modify registers holding $A$, or use any registers holding $D$, prior to the completion of the instruction (registers holding pointers, control codes, etc., don't exhibit this asynchronous behavior).

\filbreak
It's another story though for fencing the inputs, with separate mechanisms for shared memory and for matrix fragment registers.

\filbreak
\myKey{Shared Memory Fence:} The \webText{fence.proxy.async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-async-proxy} instruction must be used to make prior shared memory writes in the generic proxy to visible to subsequent wgmma.mma\_async instructions issued by the same warpgroup.
Supposedly this is not needed for inputs loaded by bulk async copy (TMA) according to \webText{this source}{https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/}, as TMA also operates in the async proxy: ``Since we use TMA load, we don’t need fence.proxy.async in our example, and indeed it doesn’t appear in the WGMMA tutorial code or in the mainloop of CUTLASS Hopper GEMM kernels''.
However, it appears in this case that we still need to synchronize the execution order between bulk async copies and wgmma operations ... only the memory visibility given by fence.proxy.async is not needed in this case.

\filbreak
\myKey{Register Fence:} The confusingly-similar-sounding \wgmmaFence\ instruction must be issued before the first wgmma.mma\_async instruction, and issued whenever we must make prior register writes to $A$ and $D$ visible to a subsequent wgmma.mma\_async instruction.
This includes between two wgmma.mma\_async instructions, with the \textbf{notable, common exception} of two wgmma.mma\_async instructions using the same registers for the accumulator $D$ where $D$ is using the same matrix fragment format.
After the wgmma.mma\_async instruction is issued and prior to its completion, the registers holding $A$ and/or $D$ can be read or modified asynchronously by the wgmma hardware.

\filbreak
In both cases, I don't see it explicitly stated, but it appears based on Nvidia's expected usage pattern that the wgmma.mma\_async instruction following either kind of fence \flagged{observes changes made by all four warps of the warpgroup} issuing the wgmma.mma\_async instruction, so there is some sort of weak execution order guarantee as well.

TODO investigate sparse operations.

\newpage
\mySub{Common Theme}

The common theme of all of this is that we will need to model waits that \textbf{only carry certain kinds of dependencies}: register only, shared memory only, registers with a common matrix format (for the two wgmma.mma\_async case), or special dependecies that only carry specific memory, e.g. the non-transitive \completeTxOperation: ``The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same asynchronous instruction, and in particular it does not transitively establish ordering with respect to prior instructions from the issuing thread.'' (\webText{Documentation}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#program-order-async-operations})

\filbreak
The non-transitivity (and to some extent, relaxed atomics, if/when we model this) is probably not behavior something like concurrent separation logic can reason about (at least this was Amanda's impression based on my brief description of the following contrieved example).

{
\color{lightttColor}
\begin{verbatim}
threads A do:
    Fill memory W
    Fill memory X = f(W)  // force causality
    fence.proxy.async
    TMA copy X -> Y, signal <barrier> when done

threads B do:
    Wait for <barrier>
    Read Y
    if someProperty(Y):  // force causality
        Read W  // WRONG, despite that W preceded X.
                // Even if W is part of the causality chain that caused X to be filled!!!
\end{verbatim}
}

\newpage
\myTitle{Sketch of New Language Features}

\myKey{Async Block:} All code is lowered to the CPU by default.
Code within an ``async block'' gets lowered to a different accelerator, or more formally, executed with a different \textbf{actor kind} (detailed later).

Syntax:
{\color{lightttColor}
\begin{verbatim}
@proc
def some_proc(...):
    # Top-level CPU code -- actor kind cpu

    with CudaDeviceFunction(blockDim=...):
        # CUDA code, kernel with specified blockDim
        # actor kind cuda_sync

        with CudaAsync(<actor-kind>):
            # CUDA code targetting async instructions
            # actor kind as given by the user
\end{verbatim}
}

\flagged{TODO} ``async'' is severely overloaded, consider different name?

\filbreak
\myKey{Parallel-for:} Allow replacing ``\lighttt{for \_ in seq(...):}'' with a hierarchy of parallel for loops:

\filbreak
{\color{lightttColor}
\begin{verbatim}
for _ in cuda_clusters(lo, hi):
for _ in cuda_blocks(lo, hi):
for _ in cuda_warpgroups(lo, hi):
for _ in cuda_warps(lo, hi):
for _ in cuda_threads(lo, hi):
\end{verbatim}
}

\filbreak
Such loops are considered to have a different \lighttt{LoopMode} (generalization of \lighttt{seq}/\lighttt{par}).
They may only appear within a \lighttt{CudaDeviceFunction} async block, and not within a \lighttt{CudaAsync} async block.

\filbreak
Different levels of the hierarchy need to be strictly nested, except that as a convenience, immediately-nested parallel for loops of the same level may be used to define a multidimensional shape for the parallel iteration space.
Skipping the cluster, warpgroup, or warp level is acceptable.

\filbreak
Each level of parallel-for defines a new collective scope, with each iteration executed by its own collective lane; both will be detailed in the concepts section.
Each collective lane is indexed by the [multidimensional] parallel-for loop index and comprises all the GPU threads of the loop level's collective unit (e.g. each collective lane of a \lighttt{cuda\_warps} loop consists of 32 threads).

\filbreak
Each level of parallel-for corresponds to a \textbf{collective unit}.
Child statements of a parallel-for loop are executed by entire collective units (e.g. within a \lighttt{cuda\_warps} loop, each iteration is executed by a full warp), not counting code within more deeply nested parallel-for loops (e.g. \lighttt{cuda\_threads} loop within \lighttt{cuda\_warps} loop.

\filbreak
\myKey{Collective Specialization Statement:} We need to provide a mechanism for warp specialization, i.e., scheduling a parallel-for loop over only a subset of available resources.
This is implemented using the syntax \lighttt{with SpecializeCollective(\textit{collective\_unit}, lo, hi): body}.
For example, the following means to schedule a $16 \times 8$ parallel-for loop over warps 4, 5, 6, and 7 (128 threads):

{\color{lightttColor}
\begin{verbatim}
with SpecializeCollective(cuda_thread, 128, 256):
    for yi in cuda_threads(0, 8):
        for xi in cuda_threads(0, 16):
            ...
\end{verbatim}
}

\filbreak
\myKey{Actor Kind, Actor Signature:} In Exo currently, we assume all code is executed on the CPU, or with accelerator functions that operate \textit{synchronously} with the CPU.
In other words, there's no concept of memory availability or visibility: A happens-before B is enough to ensure B sees the effects of A's action.

\filbreak
With Exo-GPU, we have to keep track of ``who'' is executing a certain instruction and take into account memory visibility.
At a minimum, this ``who'' is either the host CPU or the CUDA device; furthermore, we need specialized ``whos'' that represent different categories of asynchronous CUDA operations (async memcpy, wgmma).
Furthermore, it may matter ``how'' the value was accessed by a particular accelerator (e.g. wgmma has different handling for accumulators, input-only registers, and shared memory).
For now I'm proposing that we use the term ``actor kind'' for the ``who'' and ``actor signature'' for the ``how'', since in a dynamic trace of a program, we're ``signing'' each \textit{action} with the kind of \textit{actor} that executed it.

\filbreak
Each read or write is performed with a specific ``actor signature''.
An ``actor kind'' is formally a set of actor signatures and a $V_1$-transitive flag (see SyncEnv); the actor kinds are used to annotate blocks of code (see async block), \lighttt{@instr} procs, and synchronization statements.
For async blocks and \lighttt{@instr}, the actor kind specifies the allowed set of actor signatures for reads and writes performed.
For synchronization statements, the actor kind acts as a filter of the reads/writes that the statement is capable of synchronizing (filter by matching actor signature).

\filbreak
\hook{Hook:} Will need to be able to identify the actor kind for any LoopIR node (traverse root to node), account for this in LoopIR simplification and accelerator instructions, and account for this when checking if a memory type can be read/written/reduced.

\filbreak
\myKey{List of actor signatures:} The separate actor signatures for the two types of wgmma register accesses are needed in order to handle the special implicit synchronization for back-to-back wgmma writes to accumulators.
\begin{align*}
\lighttt{sig\_cpu: } & \text{Default CPU}\\
\lighttt{sig\_cuda\_sync: } & \text{Default CUDA (synchronous CUDA instructions)}\\
\lighttt{sig\_non\_bulk\_cp\_async: } & \text{Ampere \lighttt{cp.async} (async memcpy in generic proxy)}\\
\lighttt{sig\_tma\_to\_smem: } & \text{Hopper TMA \lighttt{cp.bulk.async} to shared memory}\\
\lighttt{sig\_tma\_to\_gmem: } & \text{Hopper TMA \lighttt{cp\{.reduce\}.bulk.async} to global memory}\\
\lighttt{sig\_wgmma\_rmem\_a: } & \text{wgmma access to `A' parameter when stored in registers}\\
\lighttt{sig\_wgmma\_rmem\_d: } & \text{wgmma access to `D' parameter (accumulator) when stored in registers}\\
\lighttt{sig\_wgmma\_smem: } & \text{wgmma access to shared memory (either `A' or `B' parameter)}
\end{align*}

\filbreak
\myKey{List of actor kinds:} Greyed-out actor kinds are ``synthetic'' and cannot be used to label actual code or \lighttt{instr}s.
They are valid only for synchronization statements (later), and represent subsets or supersets of other actor kinds.
\begin{align*}
\texttt{cpu: } & \{\lighttt{sig\_cpu}\}\\
\graytt{cuda\_all: } & \text{All actor signatures except \lighttt{sig\_cpu}}\\
\texttt{cuda\_sync: } & \text{Default CUDA; \{\lighttt{sig\_cuda\_sync}\}}\\
\texttt{non\_bulk\_cp\_async: } & \{\lighttt{sig\_non\_bulk\_cp\_async}\}\\
\graytt{cuda\_generic: } & \text{Generic proxy; \{\lighttt{sig\_cuda\_sync}, \lighttt{sig\_non\_bulk\_cp\_async}\}}\\
\graytt{cuda\_async\_proxy: } & \text{Async proxy; \{\lighttt{sig\_tma\_*}, \lighttt{sig\_wgmma\_smem}\}}\\
\texttt{tma\_to\_smem\_async: } & \text{\{\lighttt{sig\_tma\_to\_smem}\}}\\
\texttt{tma\_to\_gmem\_async: } & \text{\{\lighttt{sig\_tma\_to\_gmem}\}}\\
\texttt{wgmma\_async: } & \text{wgmma instructions; \{\lighttt{sig\_wgmma\_*}\}}\\
\graytt{wgmma\_async\_smem: } & \text{actions on shared memory by wgmma instructions; \{\lighttt{sig\_wgmma\_smem}\}}\\
\graytt{wgmma\_fence\_1: } & \text{\{\lighttt{sig\_cuda\_sync}, \lighttt{sig\_wgmma\_rmem\_*}\}}\\
\graytt{wgmma\_fence\_2: } & \text{actions on wgmma register tiles by wgmma instructions; \{\lighttt{sig\_wgmma\_rmem\_*}\}}\\
\end{align*}

\filbreak
\myKey{Parallel View (parview):} (Incomplete) borrowing a concept from \textit{Descend}.
A parview consists of an array variable and a mapping function $f$ mapping array indices to collective lane indices.
If an array is accessed through a parview at index $i$, it is a promise that the access is being performed by the collective lane with index $f(i)$.
Simple patterns (e.g. striping) may be statically checked as in \textit{Descend}, but note it would not be too difficult to defer checking to runtime by compiling the given mapping function to an \lighttt{assert} statement.

I haven't figured this out yet, but we probably need to extend this to handle asserting that reads and writes won't conflict ``temporally'' i.e. proving that different loop iterations only use disjoint subsets of some array.
This would be needed for ring buffer optimization to work for deep pipelining.

\hook{Hook:} Need to teach Exo to statically verify the promise, or pass through the mapping function to codegen in order to generate the assert.
I'm probably going to put off this feature until much later.

\flagged{TODO} could be an extension of ``collective assignment''.

\filbreak
\myKey{Synchronization Statement:} We need to support arrive, await, and non-split barriers (e.g. plain old \lighttt{\_\_syncthreads}), which act as a combined arrive-await statement.
These define a synchronization relation between all GPU threads of the executing collective lane.

\lighttt{Fence(A1, A2)}: non-split barrier parameterized with a ``first actor kind'' \lighttt{A1} and a ``second actor kind'' \lighttt{A2}; instructions with actor kind \lighttt{A2} issued after the barrier see the actions with actor kind \lighttt{A1} performed before the barrier.

\lighttt{Arrive(A1 : ActorKind, B : barrier)}: arrive

\lighttt{Await(B : barrier, A2 : ActorKind)}: await; instructions with the second actor kind (\lighttt{A2}) issued after the await see the actions with first actor kind (\lighttt{A1}) performed before the matched arrive.
(The Nth arrive performed on a given barrier variable is matched with the Nth await for the same barrier).

\filbreak
Example:

{\color{lightttColor}
\begin{verbatim}
for blockIdx in cuda_blocks(lo, hi):
    for threadIdx in cuda_threads(0, 256):
        ...
    # __syncthreads() as the executing collective lane here is a full block
    Fence(cuda_sync, cuda_generic)

    for warpIdx in cuda_warps(0, 8):
        for threadIdx in cuda_threads(32*warpIdx, 32*(warpIdx + 1)):
            ...
        # __syncwarp() as the executing collective lane here is a warp
        Fence(cuda_sync, cuda_generic)
        for threadIdx in cuda_threads(32*warpIdx, 32*(warpIdx + 1)):
            ...
\end{verbatim}
}

By requiring synchronization statements to be lifted to block or warp level, we trivially enforce convergence requirements.

\filbreak
I'm not sure this is really the right approach to take, but my gut feeling is it's best to expose a higher-level synchronization interface and compile to the appropriate CUDA synchronization primitive depending on the actor kinds (e.g. a ring buffer of \mbarrier), rather than expose the complexity of basic synchronization primitives and having to analyze that they are used correctly.

\filbreak
Not all combinations of first and second actor kinds are supported.
For split barriers, I'm expecting mostly that we will support \lighttt{cuda\_all} to \lighttt{cpu} barriers, asynchronous CUDA to \lighttt{cuda\_sync} barriers, and \lighttt{tma\_*} to/from \lighttt{wgmma} split barriers.

\filbreak
For non-split barriers, we support the following:

\texttt{Fence(cuda\_sync, cuda\_generic)}: \lighttt{\_\_syncthreads()} and similar barriers

\texttt{Fence(wgmma\_fence\_1, wgmma\_fence\_2)}: PTX \wgmmaFence

\texttt{Fence(cuda\_sync, wgmma\_async\_smem)}: \fenceProxyAsync\ at warpgroup scope

\texttt{Fence(cuda\_sync, tma\_to\_gmem\_async)}: \fenceProxyAsync\ at thread scope\footnote{Not sure about this one ... the underlying instruction is the same as the previous but \cpAsyncBulk\ instructions are issued per-thread, not per-warpgroup, so the interaction between cooperative CUDA synchronous code and TMA is quite different compared to wgmma.}

\texttt{Fence(cuda\_all, cpu)}: \lighttt{cudaStreamSynchronize()} (CPU waits for CUDA kernels)

\filbreak
Note the first two GPU barriers have non-equal first and second actor kinds to account for the fact that the barriers can synchronize prior synchronous instructions with subsequent asynchronous instructions, but not the reverse.

\filbreak
\hook{Hook:} For the analysis and codegen to be feasible, I'm expecting language restrictions that make it possible to statically verify that for each split barrier constructed, we issue matching pairs of arrive and await, with the same collective lane used for all arrives and the same collective lane used for all awaits.
We need to verify that the iteration spaces for the arrive and awaits are the same.

\filbreak
\myKey{Kernel parameters:} We need to take a census of the kernel parameters needed similar to how we determine whether pointers are const or non-const.
This is needed to generate the prototype of the device function (kernel), and generate the parameters for the CUDA kernel launch on the CPU.

\filbreak
\flagged{TODO} fat pointers e.g. \lighttt{CUtensorMap}.
Need to be prepared on the CPU prior to kernel launch and passed to the kernel.
So we need to take a ``census'' of fat pointers needed on the kernel and generate CPU code to prepare them.

\filbreak
\flagged{TODO} ``grid constant'' scalars or small tensors that are copied from the CPU to the CUDA device via kernel parameters.
Read/write on the CPU. Read only on the GPU. Skip synchronization checking.

\filbreak
\myKey{Distributed Memory (GPU Registers):} CUDA registers need to be modelled as ``distributed memory'', where we understand the storage for an array as being distributed across lanes (i.e. threads; wmma and wgmma registers may instead be distributed across warps/warpgroups) and such that the $n^{th}$ array entry can (mostly) only be accessed by the $n^{th}$ lane.
The reason for this is the barrier lifting requirement, so we cannot declare cuda register variables at thread-scope if we want them to survive across synchronization statements, e.g. something like the following won't work:

\filbreak
{\color{lightttColor}
\begin{verbatim}
for blockIdx in cuda_blocks(lo, hi):
    for threadIdx in cuda_threads(0, 256):
        x : f32 @ CudaRmem
        x = foo()
    Fence(cuda_sync, cuda_generic)  # __syncthreads() executed convergently by 1 block
    for threadIdx in cuda_threads(0, 256):
        y : f32 @ CudaRmem
        y = doSth(x)  # uh oh, x was freed above
\end{verbatim}
}

\filbreak
Instead we need this:
{\color{lightttColor}
\begin{verbatim}
for blockIdx in cuda_blocks(lo, hi):
    x : f32[256] @ CudaRmem
    for threadIdx in cuda_threads(0, 256):
        x[threadIdx] = foo()
    Fence(cuda_sync, cuda_generic)  # __syncthreads() executed convergently by 1 block
    for threadIdx in cuda_threads(0, 256):
        y : f32 @ CudaRmem
        y = doSth(x[threadIdx])
\end{verbatim}
}

\flagged{TODO} this also applies to distributed shared memory.

\filbreak
This could be quite a mess to model and use.
We need to
\begin{itemize}
  \item Figure out the mapping from tensor coordinates to physical collective units
  \begin{itemize}
    \item \flagged{TODO} ``collective assignment'' idea I've been talking about
    \item May be more complicated than the 1:1 example I just gave (\lighttt{[n]} for thread $n$)
  \end{itemize}
  \item Enforce that collective lanes mostly access only tensor coordinates assigned to them
  \begin{itemize}
    \item Particularly tricky across multiple parallel for loops
    \item What if the iteration space (shape) changes? Error?
  \end{itemize}
  \item Modify the compiler to generate correct indexing expressions
\end{itemize}

\filbreak
NOTE: I say ``mostly'' earlier because we may want to model warp shuffles, where threads can access registers for another thread in the same warp.
We can externalize this as some sort of accelerator function, or bake this into the compiler.
Both have pros and cons; the former sounds attractive, but has hidden complications.
For example, if we externalize the shuffle, the shuffle has to be able to comprehend translating multi-dimensional parallelism to a literal thread index, e.g. in the following:

{\color{lightttColor}
\begin{verbatim}
for blockIdx in cuda_blocks(lo, hi):
    home_value : f32[8,2,4,2,2] @ CudaRmem
    xor_x_value : f32[8,2,4,2,2] @ CudaRmem
    xor_y_value : f32[8,2,4,2,2] @ CudaRmem
    xor_xy_value : f32[8,2,4,2,2] @ CudaRmem
    for w in cuda_warps(0, 8):
        # Map 2 x 4 x 2 x 2 iteration space to the 32 threads of a warp
        for yo in cuda_threads(0, 2):
            for xo in cuda_threads(0, 4):
                for yi in cuda_threads(0, 2):
                    for xi in cuda_threads(0, 2):
                        home_value[w,yo,xo,yi,xi] = ...
                        # Exchange horizontally, vertically, diagonally in 2x2 tiles
                        # (side note, need to fission here for S/M equivalence ...
                        # also xor isn't affine but this is just to illustrate)
                        xor_x_value[w,yo,xo,yi,xi] = home_value[w,yo,xo,yi,xi ^ 1]
                        xor_y_value[w,yo,xo,yi,xi] = home_value[w,yo,xo,yi ^ 1,xi]
                        xor_xy_value[w,yo,xo,yi,xi] = home_value[w,yo,xo,yi ^ 1,xi ^ 1]
\end{verbatim}
}
(This isn't a made-up situation ... this really corresponds conceptually to some high performance code I wrote in the \webText{vk\_compute\_mipmaps sample}{https://github.com/nvpro-samples/vk\_compute\_mipmaps}).

\filbreak
\flagged{TODO} this shuffling is actually critical to think about, because it's also needed to model TMA multicast (broadcast GMEM tile to distributed SMEM in multiple blocks inside the cluster), and that's critical to match the performance of high-performance CUDA gemm.

\newpage
\myTitle{Proposed Model \& Vocabulary}

The goal is to prove equivalence between single-threaded semantics and multi-threaded semantics.
I'll refer to them as ``S-semantics'' and ``M-semantics''.
Furthermore the prefix S- and M- will mean ``under single-threaded semantics'' or ``under multi-threaded semantics'' respectively, e.g., ``write X is S-prior to read Y means ``write X is prior to read Y under single-threaded semantics''.

Originally my hope was to implement safety with static formal program analysis.
Talking to William and Jonathan, I've come to see that it's best to forget this for now and focus on (a) getting the CUDA backend for Exo written at all and (b) doing dynamic correctness verification first.

\filbreak
\mySub{Program Concepts}

\filbreak
\myKey{Collective Unit:} Saliently different groupings of cooperative threads, including the base case of 1 thread
\begin{itemize}
  \item CPU thread
  \item CUDA thread
  \item CUDA warp: 32 threads
  \item CUDA warpgroup: 128 threads
  \item CUDA block (CTA)
  \item CUDA cluster
  \item CUDA grid
\end{itemize}
Named by analogy with e.g. ``distance unit'' (inch, foot, mile...) or ``mass unit'' (gram, kilogram, metric ton...)

\filbreak
\myKey{Collectivity:} Required collective unit used to execute some instruction, e.g.
\begin{itemize}
  \item ``typical'' memory and arithmetic: CPU thread or CUDA thread
  \item wgmma MMA: CUDA warpgroup
  \item \lighttt{\_\_syncthreads}: CUDA block (CTA)
\end{itemize}

\filbreak
\myKey{Collective scope:} Each collective scope (formerly parscope) is defined by its \textit{resources}, \textit{collective unit type}, \textit{shape}, and its parent collective lane (except the top-level collective scope, representing the main CPU thread, has no parent). Two collective scopes are the same when all these values match. A collective scope is created when executing parallel-for loops of the form

{\color{lightttColor}
\begin{verbatim}
# The collective lane that executes this loop is the collective lane
with SpecializeCollective(cuda_{unit type}, lo, hi):  # Optional resource specialization
    # Partial collective scope defined here (shape not yet defined)
    for _ in cuda_{unit type}s(lo, hi):
        # Invalid collective scope (between dimensions)
        for _ in cuda_{unit type}s(lo, hi):  # Optional multidimensional for loop
            # The N-dimensional iteration space is the shape
            # Here we say that we are in {unit type}-scope (e.g. block-scope)
\end{verbatim}
}

\filbreak
Only control flow and synchronization statements may appear where the collective scope is partially defined (synchronization operates over the subset of resources specified by the resource specialization).
No code at all can appear where the collective scope is invalid (between dimensions).
The collective scope contains one collective lane of the given collective unit type for each point (index) in the shape.

The resources of a collective scope are a subset of its parent collective lane (e.g. subset of warps within parent block), except for the case of CPU code launching a cuda grid (\lighttt{CudaDeviceFunction}).

the resources are defined by the shape and the optional collective specialization.

There must not be more collective lanes than that supported by the available resources, but there may be fewer.
\flagged{TODO} maybe this is not a good restriction; we may want to allow a larger iteration space than the number of units available, and schedule temporally based on a certain ``collective assignment''.
This is particularly needed at the top-level (CTAs or clusters) where we may want to map in some non-trivial way all the ``work units'' to a fixed-size number of clusters/CTAs that depends on the hardware resources available.
However, this will pose difficulties for distributed memory.

\filbreak
\myKey{Collective Lane:} A single unit of a collective scope, which is assigned to execute one iteration of the parallel-for loop (which defined the collective scope).
Only collective lanes with collective unit type cpu-thread or cuda-thread can execute ``ordinary'' arithmetic, read, or write instructions.
The expectation is that after scheduling, all code at cluster, block, warpgroup, or warp scope is either synchronization, control flow, or custom procedures.
Two collective lanes are equal when they come from the same collective scope and correspond to the same index.

\filbreak
\myKey{Temporal/Spatial Loop Index:} Loop index variables are ``spatial'' if they correspond to a parallel-for loop and ``temporal'' otherwise.

\filbreak
\myKey{Actor Signature:}
The actor signature is an attribute of each memory read or write performed, which may vary based on which accelerator performed it.

\filbreak
\myKey{Actor Kind:}
Set of actor signatures, plus a $V_1$-transitive flag (see SyncEnv).

\filbreak
\mySub{Synchronization Environment Semantics}

As mentioned, for now the plan is to set up some dynamic checking.
The mental model I'm accepting is that we'll compile the Exo program to an S-program (single-threaded CPU program) that simulates how the program would behave had it been compiled to an M-program.
We will track in the ``synchronization environment'' additional information for each read and write performed.
We'll need to prove that the M-program would produce the same results, i.e., that
\begin{enumerate}
  \item For each read, there exists sufficient synchronization between it and the S-prior write to the same variable (generalized RAW hazard avoidance)
  \item For each write, there exists sufficient synchronization between it and the S-next write to the same variable (WAW hazard avoidance)
  \item For each read, there exists sufficient synchronization between it and the S-next write to the same variable (WAR hazard avoidance)
\end{enumerate}

\filbreak
Note this is a strictly stronger condition than race freedom: the M-program must be race-free to be equivalent to the (trivially deterministic) S-program, but moreover, the reordering caused by parallelization must not cause any read to pick up the ``wrong'' previous write, i.e., one that does not match the immediately previous write performed under S-semantics.

\filbreak
\myKey{Synchronization Environment (SyncEnv):} Each variable in the environment has an associated ``assignment record'' in the SyncEnv.
Each time a variable is assigned to, it is associated with a completely new assignment record in the SyncEnv.

\filbreak
\myKey{Sigthread:} Pair of (thread index, actor signature); this is a conceptual ``sub-thread'' of the given thread that executes only actions with the given actor signature.

\filbreak
\myKey{Accessor Set:}
The set of sigthreads $\{(t_0, A_S), (t_1, A_S), ... (t_{n-1}, A_S)\}$ performing a given read/write, where $t_0, t_1, ... ,t_{n-1}$ is the range of threads performing the access and $A_S$ is the actor signature.
Note there could be multiple threads due to convergent code at warp/warpgroup scope.

\filbreak
\myKey{Visibility Record:} A visibility record consists of
\begin{enumerate}
  \item $V_A$: async visibility set (set of sigthreads)
  \item $V_S$: sync visibility set (set of sigthreads)
  \item pending barrier awaits: set of (barrier variable, counter) pairs.
  \item $A_O$: original actor signature
\end{enumerate}

\textbf{Invariant:} $V_S \subseteq V_A$.

\filbreak
\myKey{Assignment Record:} The full assignment record for a variable consists of
\begin{enumerate}
  \item a single write visibility record;
  \item a list of any number of read visibility records
\end{enumerate}

\filbreak
Conceptually, the write visibility record's $V_S$ stores the set of sigthreads that can safely observe the written value of the variable at the current program point (safely means to deterministically read or overwrite the prior written value).

\filbreak
Supposing that there have been $N$ reads since the last write (in S-order), then there will be $N$-many read visibility records, and the $i^{th}$ read visibility record's $V_S$ stores the set of sigthreads that ``observed'' the $i^{th}$ such read, in the sense that performing a write will not cause a WAR hazard with said read.

\filbreak
The async visibility set ($V_A$) of the visibility record is needed to model asynchronous instructions that have \textit{some} effect on the S-program's environment, but cannot actually be deterministically observed in the equivalent M-program by any sigthread (including the sigthread that issued the asynchronous instruction) until future synchronization.

\filbreak
The original actor signature ($A_O$) effectively encodes the ``category'' of instruction that performed the read or write, and will be needed to model non-transitivity for specialized barrier types.

\filbreak
\myKey{Write State Tracking:} When a variable is written to, we discard the assignment record (safety checking to be described later) and associate a new assignment record.
The new assignment record has no read access records, no pending barrier awaits, and has a write record with

\filbreak
\begin{enumerate}
  \item $V_A$ set to the write's accessor set (both for synchronous and asynchronous instructions);
  \item $V_S$ set to the write's accessor set if the instruction is synchronous, empty otherwise\footnote{Special treatment needed for wgmma registers; accesses with actor signature \lighttt{wgmma\_rmem\_d} (accumulator) go in $V_S$ despite wgmma instructions otherwise being asynchronous.};
  \item an empty pending barrier awaits;
  \item $A_O$ set to the actor signature of the accessor set.
\end{enumerate}

\filbreak
\myKey{Read State Tracking:} When a variable is read, we add a new read visibility record to the variable's assignment record, initialized from the accessor set in the same manner as a write visibility record is.
This may be tricky to implement efficiently, but it really is needed to check for WAR hazards.

\filbreak
\myKey{Synchronization Statement Tracking:} Each non-split barrier, and each matched pair of arrive and await statements, defines a first visibility set and a second visibility set ($V_1$ and $V_2$); informally, this describes the sigthreads ``before'' and the sigthreads ``after'' the synchronization, respectively.
The visibility sets are defined by the set of sigthreads $V_i = T_i \times A_i$ where $T_i$ is a set of threads and $A_i$ is an actor kind (set of actor signatures).
Specifically,

\filbreak
\begin{enumerate}
  \item The actor kind and set of executing threads for an arrive statement defines $A_1$ and $T_1$.
  \item The actor kind and set of executing threads for an await statement defines $A_2$ and $T_2$.
  \item A non-split barrier statically defines both $A_1$ and $A_2$, with the set of executing threads defining both $T_1$ and $T_2$.
\end{enumerate}

\filbreak
A barrier may be transitive or non-transitive.
It is transitive whenever the $V_1$-transitive flag of $A_1$ is true; if so, a visibility record synchronizes-with an arrive or a non-split synchronization statement when ${V_A \cap V_1}$ is non-empty.
(The consideration of $V_A$ represents that synchronization statements can synchronize with pending asynchronous instructions, when the appropriate actor kind is used).
If non-transitive, we substitute $V_A$ with ${\{ (t, a) \in V_A \mid a = A_O \}}$ where $A_O$ is the original actor kind of the visibility record; this effectively models that the barrier only has effect on reads and writes performed by the instruction kinds targetted by the barrier.

\filbreak
A synchronization statement augments a visibility record by setting ${V_A \leftarrow V_A \cup V_2}$ and ${V_S \leftarrow V_S \cup V_2}$.
This represents that a read or write operation is visible to more sigthreads thanks to the synchronization.
Notes:
\begin{enumerate}
  \item In this way a sigthread may ``graduate'' from $V_A$ to $V_S$.
  \item The augmenting of $V_A$ is needed to uphold the invariant $V_S \subseteq V_A$; this is mainly for implementation purposes (see memoization).
\end{enumerate}

\filbreak
When a non-split barrier is executed, it augments all visibility records of all variables that synchronize-with the synchronization statement.

\filbreak
When an arrive statement is executed, it adds itself to the pending barrier awaits of all visibility records that synchronize-with the arrive statement.

\filbreak
When an await statement is executed, it augments all visibility records that have the correct (barrier, counter) pair in its pending barrier awaits.
Note, because of how write state tracking clears the pending variable awaits for a variable upon assignment, this prevents a matched pair of arrive/await from synchronizing a write that occured subsequent to the arrive.

\filbreak
\myKey{Access Safety Checking:} We can finally define safety for a read or write access.
Let $A$ be the accessor set for the access.
We say that a visibility record is visible-to $A$ when ${V_S \cap A}$ is non-empty; note we don't consider $V_A$ (the async visibility set).

A write access to the variable is safe when the variable's assignment record
\begin{enumerate}
  \item has its write visibility record visible-to $A$ (WAW check);
  \item has all its read visibility records visible-to $A$ (WAR check).
\end{enumerate}
A read access to the variable is safe when the variable's write visibility record is visible-to $A$ (RAW check).

\flagged{TODO} extend this model for atomics

\filbreak
\mySub{Synchronization Environment Implementation Notes}

\myKey{Memoization:} As described, this model would be very expensive to implement literally, as we model synchronization statements as transformations applied over the visibility records of \textit{all} variables in the synchronization environment, and furthermore, each variable can be associated with an unbounded number of visibility records (due to the list of read visibility records).

\filbreak
This is a bit of an abuse of terminology, but we can use ``memoization'' to combat this.
Observe that the effect of a synchronization statement (non-split, arrive, await) is a pure function of the modified visibility record.
Therefore, if the same visibility record value appears multiple times in the synchronization environment, we can have all the users reference one physical ``visibility record'' object that's modified only once upon applying the effects of a synchronization statement.

\filbreak
In my current prototype implementation, I reference count \lighttt{VisibilityRecord} objects, which are stored (by weak reference) in a global\footnote{thread-local} memoization table.
Each live, unique visibility record is stored exactly once in the memoization table and given a unique ID (based on address).
Each synchronization statement is conceptually applied to all visibility records in the memoization table, although I optimize this by storing visibility records in a hierarchical thread-index-based data structure, so that we can skip visibility records corresponding to thread ranges that are guaranteed not to intersect the synchronization statement's first visibility set ($V_1$).

\filbreak
Often, duplicates only occur due to the effects of synchronization (e.g. if threads in a thread block each write to \lighttt{arr[threadIdx.x]}, then each \lighttt{arr[i]} has a distinct write visibility set ${\{(i, A_O)\}}$; after a \lighttt{\_\_syncthreads()} though, all the write visibility sets will be equal to ${\{(0, A_O), (1, A_0), ... ,(\lighttt{blockDim.x}-1, A_O)\}}$).
This is handled by ``forwarding'': if a duplicate is detected due to the effects of a synchronization statement, we can set the visibility record to a forwarding state and forward to (strongly reference) the duplicated visibility record.

\filbreak
Chains of forwarding could be formed due to the effects of multiple synchronization statements.
This breaks the assumption that visibility records can be compared for equality based on unique ID, but we can fix this by ``resolving'' the chain of forwarded visibility records and recovering the ID of the base visibility record referenced.

\filbreak
\myKey{Read Visibility Set Optimization:} As mentioned, the unbounded list of read visibility records could be a serious time sink.
For now I'm handling this by removing duplicate visibility records in the list, which we can quickly-ish recognize based on ID comparison.
I'm not at all happy with this solution (this isn't so much making the algorithm run fast as it is making it not completely intractible) but so far as I can tell, it's not trivial to do better.

\filbreak
Ignoring the complexity of wgmma and co. for now, the core asymmetry is that the ``set of [sig] threads allowed to read'' can easily be tracked as a single set, since there's only one prior write to worry about, wheras the ``set of threads allowed to write'' more-or-less has to be modelled as a conjunction of disjunctions, to handle possibly separate synchronization paths protecting each prior read (WAR hazard checking).

\filbreak
Observe, for example, that if each thread of a block reads a variable, then the set of threads that are allowed to write that variable is \textit{empty} (assuming at least 2 threads per block).
This is because thread 0's read restricts the valid writer thread set to $\{0\}$, thread 1's read restricts the valid writer thread set to $\{1\}$, and so on; and the conjunction of all those sets is empty.\footnote{Specifically, this conjunction corresponds to the step in ``access safety checking'' where all read visibility sets must be visible-to the write's accessor set.}

\filbreak
In the current model I'm proposing, an empty visibility set by itself can never expand, since it'll never intersect any synchronization statement's $V_1$.
This is why I have to track each read's visibility set separately; in this case, after a \lighttt{\_\_syncthreads()}, all of the ``valid writer thread'' sets will now be ${\{0, 1, ..., \lighttt{blockIdx.x} - 1}\}$, and the conjunction of all those identical sets is now non-empty.

\filbreak
If we \textit{stop} ignoring the complexity of wgmma and co., the situation becomes even more difficult to easily optimize, as we can't rely on simple zero/one/many-threads-accessed\footnote{\web{https://xkcd.com/764/}} state flags or such, in the manner of \webText{HiRace}{https://arxiv.org/pdf/2401.04701}.

\filbreak
A conjunction of disjunctions is in some sense maximally simplified and impossible to improve on (assuming that my proposed union-based ``augment visibility sets'' model is really the way to go to model real-world synchronization patterns, which could be wrong).
This makes me not too hopeful for finding a dramatically better implementation.
However, ideally, there would be smarter ways to ``compress'' the read visibility state, at least for common access patterns.


\filbreak
\mySub{Unanswered Questions}

\begin{enumerate}
  \item Extending memory types, particularly clearly specifying out distributed memory
  \filbreak
  \item Compiling arrive and await statements into real CUDA synchronization constructs.
    In particular, there's the difficulty of proving 1:1 correspondence between arrive and await operations.
    Need to analyze the loop iteration space.
  \filbreak
  \item How to best express producer-consumer warp specialization.
    We have a start with collective specialization statements, \lighttt{Arrive} (producer), and \lighttt{Await} (consumer); however, we will still need to model pipeline depth and ring buffering; also, we have to model the synchronization in the other direction, ensuring the producer doesn't overwrite a ring buffer entry before the consumer is done with it.
  \filbreak
  \item Learn about formal program verification and figure out how to compile this simulation-based safety model into something that is statically verifiable.
  \filbreak
  \item Extensions to the parview concept needed to express common safe patterns such as deep pipelining and ring-buffer optimization (statically partitioning an array into disjoint subsets that are uniquely assigned to loop iterations, rather than uniquely assigned to collective lanes).
  \filbreak
  \item Modeling sparse tensor operations. This would require data-dependent indexing so probably this won't happen ... or will it? (vsauce music plays)
  \filbreak
  \item How to handle swizzling shared memory for optimizing away bank conflicts.
    Separate memory types for each swizzle pattern, with the swizzling done by customizing the read/write/window syntax generated?
  \filbreak
  \item Controlling the order that thread blocks are executed.
  This can have an impact on L2 cache rate.
  In a sense, it's good that I decouple the concept of physical execution units (collective units) from the abstract parallel iteration space, to give us this flexibility. e.g. if we compile the following
  \filbreak
{
\color{lightttColor}
\begin{verbatim}
  for y in cuda_blocks(0, y_hi):
      for x in cuda_blocks(0, x_hi):
          # Code for blocks
          for w in cuda_warps(...): ...
\end{verbatim}
}
  \filbreak
  and implement it literally, we will process blocks in row-major order, which won't give optimal L2 cache usage compared to something tiled.
  (I guess in principle we can address this by splitting into inner/outer loops).
\end{enumerate}

\newpage
\myTitle{Sketch of Implementation Changes}

I'm sensitive to the fact that LoopIR is difficult to change, but I feel there is too much value in re-using LoopIR to justify creating a new IR specialized for the GPU backend.
I'm hoping it's possible to find a really minimal amount of changes to LoopIR that provides enough for expressing CUDA concepts.

\filbreak
\mySub{LoopIR Additions}

\myKey{For statements:} The binary \lighttt{seq} or \lighttt{par} values for \lighttt{loop\_mode} are generalized to the \lighttt{LoopMode} class.
Similar to \lighttt{seq}/\lighttt{par}, the choice of loop mode should be ignored for all scheduling operations -- only the backend checks, if any, care.

\filbreak
\myKey{With statements:} We should have a new LoopIR node type for the new syntactic constructs based on the \lighttt{with} statement.
These will presumably hold just a \lighttt{body} and the ``with context'' object.
Adding this is REALLY HARD because this will interact with everything in the rewrite and eff code.

\filbreak
For now I have a really hacky solution; \lighttt{CudaDeviceFunction}, \lighttt{CudaAsync}, and \lighttt{SpecializeCollective} all inherit from \lighttt{BaseWithContext}; I then compile the \lighttt{with} statements to \lighttt{LoopIR.If} having a \lighttt{cond} of type \lighttt{LoopIR.Const}, which holds a ``constant'' of type \lighttt{BaseWithContext}.
Most code just treats this as \lighttt{if True} (\lighttt{BaseWithContext} is truthy), which is what I want under S-semantics anyway.
I just have to recognize this hacky backdoor in the LoopIR pretty printer, the spork compiler, and also \lighttt{simplify}.

\filbreak
\myKey{SyncStmt:} Would need to consist of a barrier type enum (arrive, await, syncthreads, stream sync, wgmma fence); for arrive and awaits, we need the sym of a barrier variable and an actor kind.
For scheduling and safety checks, they should be treated the same as a \lighttt{pass} statement (except don't remove them in \lighttt{simplify}, \lighttt{remove\_pass}, etc.).

\filbreak
\myKey{Types:} Need to add the \lighttt{Barrier} type.
The barrier may be used for allocating variables.

\filbreak
\myKey{Read/Write/Reduce:} These will not have to change in LoopIR, but the compilation of these will have to be substantially modified in order to handle actor kinds and distributed memory (GPU registers).

\filbreak
\mySub{Spork Compiler Env}

I'll put most of the implementation for CUDA codegen into \lighttt{SporkEnv}, which the main LoopIR compiler will call into at a few critical points.
The \lighttt{SporkEnv} will track
\begin{enumerate}
  \item the collective scope and collective lane (including the current collective unit)
  \item actor kind
  \item access patterns for distributed memory (arrays of CUDA registers)
  \item that externs and proc calls have compatible collectivity and actor kinds (logical extension of memory type checking)
  \item that memory reads/reduces/writes are valid for the current collective unit, actor kind, and distributed memory-ness (need to think this through)
  \item needed CUDA kernel parameters (in order to generate syntax for kernel launches)
  \item compiling SyncStmt to physical CUDA code
\end{enumerate}

\filbreak
For the most part, we need to call into \lighttt{SporkEnv} when compiling for statements, with statements, sync statements, and memory reads/writes (the last one is probably the hardest).

\filbreak
Also, it's a good thing there's an \lighttt{add\_line} helper, because when compiling CUDA code, we'll have to reset the indentation and put all code into a separate stream.
This is because we can't mix host and device code in one function (so a single exo proc will need to be compiled to one CPU function that launches any number of CUDA kernels, each compiled as separate device functions).

\filbreak
\mySub{Backend Target}

As mentioned, we're probably going to put static analysis on the backburner.
For now I'm envisioning that we can target either compiling to an M-program (CUDA code), or compiling to some ``simulator representation'' for the SyncEnv simulator.
Running the simulation with no errors would then validate the generated CUDA's synchronization is correct -- this may be slow, and would only prove correctness for a single problem size.
This simulation depends critically on the fact that data-dependent indexing doesn't exist, so this won't interact well with Chexo.

\filbreak
\mySub{Other Complications}

Things I'm not sure how to handle best yet:

\begin{enumerate}
  \item The specifics of how parameters are passed from host code to device functions.
  We need to be able to cuda-memcpy stuff (model as an \lighttt{instr} most likely), pass kernel parameters (not sure how to best do this), and also reason about the CUDA grid constant memory type that I mention from time to time.
  \filbreak
  \item How to best modify memory types to handle the new GPU environment (can\_read must consider the current actor kind, and we need to model distributed memory like GPU registers).
  \filbreak
  \item Warp shuffles and multicast, as discussed previously (collective assignment?)
  \item Sparse tensors (interacts with Kenneth Moon's project ... so I may put this off)
\end{enumerate}

\end{document}
