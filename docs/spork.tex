\input{common.tex}

\newcommand{\mbarrier}{\webText{mbarrier}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier}}
\newcommand{\cpAsync}{\webText{cp.async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-cp-async}}
\newcommand{\cpAsyncBulk}{\webText{cp.async.bulk}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-cp-async-bulk}}
\newcommand{\fenceProxyAsync}{\webText{fence.proxy.async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-async-proxy}}
\newcommand{\wgmma}{\webText{wgmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions}}
\newcommand{\hopperBlog}{\webText{NVIDIA Hopper Architecture In-Depth}{https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/}}
\newcommand{\expectTxOperation}{\webText{expect-tx operation}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-expect-tx-operation}}
\newcommand{\completeTxOperation}{\webText{complete-tx operation}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation}}

\begin{document}
\myTitle{Project Spork: EXO GPU}

Prototyping for how to extend Exo to safely support GPU accelerators (specifically what CUDA offers), including features like \lighttt{memcpy\_async} and wgmma, which I've argued are impossible to model using a fork/join approach while preserving maximum throughput. We will still be taking the approach of proving equivalence between ``sequential (single-threaded) semantics'' and ``parallel (multi-threaded) semantics'', where parallel-for loops and barriers are modelled as sequential-for loops and no-ops respectively under sequential semantics.

\mySub{Goals}

\myKey{Supported CUDA Features:} Hierarchical parallel-for over clusters, CTAs (blocks), warpgroups, warps, and threads; support simple barriers (e.g. \lighttt{\_\_syncthreads}); split barriers (\mbarrier); \lighttt{memcpy\_async} (a.k.a. \webText{cp.async and cp.async.bulk}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-asynchronous-copy}); \webText{wmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions} (warp matrix instructions); \wgmma\ (async warpgroup matrix instructions).

\filbreak
\myKey{Minimize synchronization overhead:} As pointed out by Hazy Research (and others), it's absolutely critical that the tensor cores are fed each cycle; you cannot later compensate for the performance lost for each missed cycle. Therefore for maximum performance, the new split barriers \textit{must} be used. You can't afford the bubble caused by \lighttt{\_\_syncthreads}. This eliminates fork-join as a viable model for maximally performance accelerators.

\filbreak
\myKey{Safety:} We need to prove race freedom, but moreover prove that each read gets the \textit{expected} previously-written value (or the initial input value). This is beyond what \textit{Descend} handles, which is only race freedom, i.e. in some code like

{\color{lightttColor}
\begin{verbatim}
Thread 0: x = 3;             x = 5;
Thread 1:         y = 10*x;
\end{verbatim}
}

the \textit{Descend} borrow-checking model only requires proving that thread 1's read of \lighttt{x} does not overlap with either of thread 0's assignments to \lighttt{x}, and so \lighttt{y = 10*}$x_0$, \lighttt{y = 30}, and \lighttt{y = 50} are all possible outcomes depending on how the user synchronized the two threads. Wheras for Exo we will check that the user's synchronization will guarantee the single outcome that matches that given by the equivalent code under single-threaded semantics.

\filbreak
\myKey{Ring Buffer:} We need to support ring buffer optimization under multi-threaded semantics as well.

\filbreak
\myKey{Array Race Analysis:} We may need a mechanism similar to the ``view'' concept in \textit{Descend} to make more complicated array access patterns tractible to analyze. i.e. we need to allow the user to prove that two threads' parallel writes to the same array won't cause a hazard by showing the access pattern leads to disjoint arary indices used for each thread.

\filbreak
\myKey{Other Features:} I am thinking less about these features as they are not as relevant to the AI-centric use case for Hopper, but nevertheless in the background we should think about \webText{grid group synchronization}{https://docs.nvidia.com/cuda/cuda-c-programming-guide/\#grid-group} (pre-Hopper method for synchronizing across all threads in an entire grid), associative atomic reductions (add, min, max), and maybe modelling the extremely powerful \webText{cub prefix sum}{https://nvidia.github.io/cccl/cub/api/structcub_1_1DeviceScan.html} functions.

\filbreak
\mySub{Project Scope}

For the most part it seems like this work will be an add-on just prior to code generation, where we check sequential and parallel equivalence. I hope there's relatively few \hook{``hooks''} needed in the rest of Exo for this work. I say this because for now to make this project feasible, imo it's best to bake-in assumptions about CUDA's synchronization model, and not try to generalize that, the way we might for modeling shared memory (as a memory type) or specific accelerator functions. So ideally it would not be too much of a maintenence burden to remove the initial Exo GPU code if it turns out not to be the best approach long term.

\filbreak
For the more complicated synchronization, a more ``declarative'' model (where the user specifies what blocks of code they intend to synchronize, rather than working at the level of individual fence/barrier instructions) may be better for making proving correctness feasible.

\filbreak
\myTitle{Background on GPU Features}

TODO: verify \flagged{flagged} claims.

The primary point of this is to take a census of the synchronization patterns we would have to model and how they interact with async copies \& wgmma. Other details like swizzling etc. should be modellable with enough work using Exo's existing features.

\myKey{WARNING:} The links I have should take you to the correct subsection of the giant PTX documentation, but there is a bug where sometimes the Nvidia website warps you back to the top of the page. Select the address bar and press enter to fix this.

\filbreak
\mySub{Synchronization Summary}

CUDA has two different notions of ``async'': asynchronous instructions (simple enough), and the more complicated \webText{async proxy}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#proxies}. An instruction being async implies control continues to the next instruction without waiting for completion. If an async instruction further documents that it operates on the async proxy, the user (in addition to synchronizing execution order) must further include \fenceProxyAsync\ (and also maybe \webText{wgmma.fence}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence}) to ensure visibility between the async proxy and the ``generic proxy'', which is what most CUDA instructions operate on. Note in the case of mbarrier that this includes observing the barrier itself: ``To make the initialized barrier visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used. This instruction ensures that subsequent bulk-asynchronous copy operations operate on the initialized barrier'' (\webText{source}{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#tensor-memory-access}).

\filbreak
Of the new instructions we want to model,

\begin{enumerate}
  \item non-bulk async copy (\cpAsync) are asynchronous instructions operating on the generic proxy;
  \item bulk async copy (\cpAsyncBulk; Hopper TMA) are asynchronous instructions operating on the async proxy;
  \item pre-Hopper tensor cores (\webText{wmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions-wmma-mma}) are not async at all;
  \item Hopper tensor cores (\wgmma) are asynchronous instructions operating on the async proxy.
\end{enumerate}

\filbreak
For the most part there seem to be four categories of synchronization primitives:

\filbreak
\myKey{All-to-all:} Like \lighttt{\_\_syncthreads} and cooperative group syncs; useful in the common case but these do nothing for asynchronous instructions.

\filbreak
\myKey{mbarrier:} Split barrier where consumer threads wait for a certain number of producer threads to arrive. This may be used to synchronize ordinary non-async instructions, synchronize non-bulk \cpAsync\ (Ampere), and to synchronize a supported subset of Hopper \cpAsyncBulk\ (TMA) instructions. The \mbarrier\ must be constructed in shared memory.

\filbreak
\myKey{Async-group:} Supported with separate instructions for \webText{non-bulk async copy}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms}, some bulk async copies, and for \webText{wgmma}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group}. These don't require constructing state like mbarrier. Each wgmma or async copy instruction is initially uncommitted, and is commited to a new async-group with a commit\_group instruction. You may then wait for the Nth previous async-group of the same thread to finish with a wait\_group instruction. This may implement deep pipelining but \flagged{cannot be used} to implement separate producer/consumer threads.

\filbreak
\myKey{Fences:} The previous primitives only synchronize execution order, and as mentioned, for the async proxy, a fence is needed in addition to this to ensure visibility. wgmma and bulk copy instructions include an implicit fence afterwards, so only execution order synchronization is needed to see the outputs; the reverse is not true (i.e. generic proxy code generating inputs for async proxy instructions requires a fence).

\flagged{Question:} Unclear if \webText{cp.async.bulk.\textbf{tensor}}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#data-movement-and-conversion-instructions-cp-async-bulk-tensor} methods include the implicit fence; the \webText{async proxy documentation}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#async-proxy} states ``The completion of a \lighttt{cp\{.reduce\}.async.bulk} operation is followed by an implicit generic-async proxy fence.'' which I'm unsure if it intends to exclude the tensor variants.

%% \filbreak
%% \hook{Async Code Label:} We may need a mechanism to allow users to label certain Exo code and accelerator functions as async, and only allow async accelerator functions to be substituted for async code. Somehow distinguish labelling code as ``merely'' async, or as async and executing in the async proxy.

\filbreak
\mySub{Asynchronous Memory Copy}

According to William, it's best to use inline PTX instead of the CUDA C++ \lighttt{cuda::memcpy\_async} function, as the C++ function is too prone to silently decaying to a regular memory copy. Async copies can be non-bulk (requires sm\_80 a.k.a. Ampere or higher) or bulk (sm\_90; Hopper).

\filbreak
\myKey{Ampere / Non-bulk:} We use the \cpAsync\ instruction to issue a single asynchronous copy of 4, 8, or 16 aligned bytes from global memory to shared memory. This is the only source+destination memory type supported. Presumably the CUDA C++ async copy functions for Ampere are implemented by \flagged{distributing the copy over the participating threads}.

All cp.async instructions issued are not implicitly ordered with each other, not even in the same thread, so write-after-write hazards are possible if two cp.async instructions share the same destination address. Since these don't operate on the async proxy, we do need synchronization after the \cpAsync\ instruction, but synchronization isn't needed for \cpAsync\ to see global memory writes issued earlier in program order, as they run in the generic proxy and ``asynchronous operations are ordered after prior instructions in the same thread'' (\webText{Asynchronous Operations}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#program-order-async-operations}).

\filbreak
\myKey{Hopper TMA / Bulk:} \cpAsyncBulk\ instructions allow you to instead specify a range of 16-byte-aligned memory to copy, and supports these src/dst memory types: global to cluster-shared, cluster-shared to CTA-shared, and CTA-shared to global. The memory types determine whether commit\_group or mbarrier must be used (completion mechanism). Unlike non-bulk async copy, the expected usage is to nominate just one thread to issue the instruction: ``...the TMA programming model is single-threaded, where a single thread in a warp is elected to issue an asynchronous TMA operation'' (\hopperBlog). As a reminder, this operates on the async proxy.

\filbreak
TODO Look into tensor copy versions of these instructions, which seem really complicated.

% https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tensor-memory-access

\filbreak
\myKey{commit\_group:} The commit\_group mechanism has a per-thread scope for bulk and non-bulk async copies. So it's rather unclear to me how this mechanism is useful in the common case where we cache stuff in shared memory for all the threads to read from. (Contrast with wgmma async-groups, which have per-warpgroup scope). I'll discuss the more complicated mbarrier in the next section.

TODO Investigate how this is implemented: ``a single thread in a warp is elected to issue an asynchronous TMA operation ... multiple threads can wait on a cuda::barrier for completion of the data transfer'' (\hopperBlog).

\filbreak
\mySub{mbarrier}

\myKey{Split Barrier:} The \mbarrier\ may be used to implement split barriers. This opaque object is initialized in shared memory with an ``expected arrival count'' $n$, then, each \webText{``phase''}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-phase}=0,1,2,... of the mbarrier consists of

\begin{enumerate}
  \item $n$-many threads calling \webText{mbarrier.arrive}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-mbarrier-arrive} on the mbarrier.
  \item \textit{one} thread successfully calling \webText{mbarrier.test\_await}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait} or similar instruction on the barrier. This makes the mbarrier ready for the next phase.
\end{enumerate}

\filbreak
Instructions performed in the \textit{generic proxy} following the await operation observe changes made in the generic proxy by \textit{non-async} instructions in the arriving threads, prior to their arrival.

The ``one thread'' condition for advancing the phase complicates implementing multiple threads waiting on an mbarrier. The mechanism for \webText{this sort of  wait}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait} is either to pass the \lighttt{state} operand (from mbarrier.arrive) or specify the parity of the phase to wait for with the \lighttt{phaseParity} operand; either way, this allows the wait instruction to know whether to immediately return due to the previous phase being complete, or block waiting for the current phase.

It's required that no ``arrive'' operation for phase $n+1$ occurs until an ``await'' operation for phase $n$ is successful. This requirement is easily met for self-synchronizing $n$-many threads (with useful work possible between the alternating arrives and waits), but is harder if implementing a producer/consumer threads model. It's also not allowed to wait for phase $n-2$ or older (in constrast to async-group).

\filbreak
\myKey{Non-bulk Async Copy mbarrier:} The \webText{cp.async.mbarrier.arrive}{https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html\#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive} instruction takes the place of mbarrier.arrive in the above description, causing the awaiting threads to observe changes performed by non-bulk \cpAsync\ instructions issued by the arriving threads, \textit{instead} of changes by non-async instructions issued.

Question: what use cases \lighttt{.noinc} has here.

\filbreak
\myKey{Bulk Async Copy mbarrier:} Since the expected model for bulk async copies is to nominate just one thread to issue the copy, the expected usage is to set expected arrival count $n = 1$, and use the additional tx-count feature: execute an \expectTxOperation\ (\webText{mbarrier.expect\_tx}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#parallel-synchronization-and-communication-instructions-mbarrier-expect-tx}) with the number of bytes expected to be copied, then use the \completeTxOperation\ built into \cpAsyncBulk\ to signal completion. (\webText{example code}{https://research.colfax-intl.com/tutorial-hopper-tma/})

\filbreak
\mySub{wmma}

Documentation: \webText{Warp-level matrix-multiply-accumulate}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#warp-level-matrix-instructions-wmma-mma}

These don't seem so bad at all. We are just distributing the storage for a ``matrix fragment'' (matrix tile) across the registers of 32 threads in a warp, with a wide variety of supported row-major and column-major formats, and computing multiply-add entirely in registers. There seem to be no synchronization requirements. TODO investigate sparse operations.

\filbreak
\mySub{wgmma}

Documentation: \webText{Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions}

These \webText{wgmma.mma\_async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-operation-wgmma-mma-async} instructions support $D = AB + D$ or $D = AB$ operations on matrix fragments. Unlike wmma, the work is distributed across warpgroups of 128 aligned threads; there are very complicated synchronization requirements; and fragment storage may be in shared memory or distributed across 128 threads' registers, with $A$ in either, $B$ in shared memory only, and the accumulator $D$ in registers only.

\filbreak
As mentioned, the execution order is handled by the fairly straightforward pipelined async-group mechanism: \webText{wgmma.commit\_group}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-multiply-and-accumulate-instruction-wgmma-commit-group} and \webText{wgmma.wait\_group}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-multiply-and-accumulate-instruction-wgmma-wait-group}. Furthermore, the new $D$ value (in registers) is immediately visible to the generic proxy after the wait due to the \webText{implicit proxy fence}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-async-proxy} for wgmma.mma\_async. In constrast to bulk async copy, register matrix values themselves may be modified asynchronously; hence, it's forbidden to modify registers holding $A$, or use any registers holding $D$, prior to the completion of the instruction (registers holding pointers, control codes, etc., don't exhibit this asynchronous behavior).

\filbreak
It's another story though for fencing the inputs, with separate mechanisms for shared memory and for matrix fragment registers.

\myKey{Shared Memory Fence:} The \webText{fence.proxy.async}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-async-proxy} instruction must be used to make prior writes in the generic proxy to shared memory visible to subsequent wgmma.mma\_async instructions issued by the same warpgroup. Supposedly this is not needed for inputs loaded by bulk async copy (TMA) according to \webText{this source}{https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/}, as TMA also operates in the async proxy: ``Since we use TMA load, we don’t need fence.proxy.async in our example, and indeed it doesn’t appear in the WGMMA tutorial code or in the mainloop of CUTLASS Hopper GEMM kernels''. However, it appears in this case that \flagged{we still need to synchronize the execution order between bulk async copies and wgmma operations} ... only the memory visibility given by fence.proxy.async is not needed in this case.

\filbreak
\myKey{Register Fence:} The confusingly-similar-sounding \webText{wgmma.fence}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence} instruction must be issued before the first wgmma.mma\_async instruction, and issued whenever we must make prior register writes to $A$ and $D$ visible to a subsequent wgmma.mma\_async instruction. This includes between two wgmma.mma\_async instructions, with the \textbf{notable, common exception} of two wgmma.mma\_async instructions using the same registers for the accumulator $D$ where $D$ is using the same matrix fragment format. After the wgmma.mma\_async instruction is issued and prior to its completion, the registers holding $A$ and/or $D$ may be read or modified asynchronously.

\filbreak
In both cases, I don't see it explicitly stated, but it appears based on Nvidia's expected usage pattern that the wgmma.mma\_async instruction following either kind of fence \flagged{observes changes made by all four warps of the warpgroup} issuing the wgmma.mma\_async instruction, so there is some sort of weak execution order guarantee as well.

TODO investigate sparse operations.

\filbreak
\mySub{Common Theme}

The common theme of all of this is that we will need to model waits that \textbf{only carry certain kinds of dependencies}: register only, shared memory only, registers with a common matrix format (for the two wgmma.mma\_async case), or special dependecies that only carry specific memory, e.g. the non-transitive \completeTxOperation: ``The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same asynchronous instruction, and in particular it does not transitively establish ordering with respect to prior instructions from the issuing thread.'' (\webText{Documentation}{https://docs.nvidia.com/cuda/parallel-thread-execution/\#program-order-async-operations})

\filbreak
\myTitle{Proposed Model \& Vocabulary}

I'll start with an outline of the new language features and then move on to a proposal.
The point of this for now is to try to frame our thinking and set up some vocabulary to further discuss the problem -- this is ``non-constructive'' in that I will have to learn more about formal program analysis and Exo's solver to figure out how to implement this.

\filbreak
The goal is to prove equivalence between single-threaded semantics and multi-threaded semantics.
I'll refer to them as ``S-semantics'' and ``M-semantics''.
Furthermore the prefix S- and M- will mean ``under single-threaded semantics'' or ``under multi-threaded semantics'' respectively, e.g., ``write X is S-prior to read Y means ``write X is prior to read Y under single-threaded semantics''.

\filbreak
\mySub{Sketch of New Language Features}

\myKey{Parallel-for:} Allow replacing ``\lighttt{for \_ in seq(lo, hi):}'' with a hierarchy of parallel for loops for each GPU unit (cluster, block, warpgroup, warp, thread).

\filbreak
{\color{lightttColor}
\begin{verbatim}
for _ in cuda_clusters(lo, hi, blocks = _):  # Defines number of blocks per cluster
for _ in cuda_blocks(lo, hi, warps = _):     # Defines number of warps per block
for _ in cuda_warpgroups(lo, hi):
for _ in cuda_warps(lo, hi):
for _ in cuda_threads(lo, hi):
\end{verbatim}
}

\filbreak
Different levels of the hiererchy need to be strictly nested, except that as a convenience, immediately-nested parallel for loops of the same level may be used to define a multidimensional shape for the parallel iteration space.
Skipping the cluster, warpgroup, or warp level is acceptable.
Each level of parallel-for defines a new ``parscope'' (parallelization scope), with each iteration executed by its own ``parlane'' (parallelization lane); both will be detailed in the concepts section.
Each parlane is indexed by the parallel-for loop index and comprises all the GPU threads of the loop level's GPU unit (e.g. each parlane of a \lighttt{cuda\_warps} loop consists of 32 threads).

\filbreak
We also need a mechanism for specializing warps, i.e. specifying that a \lighttt{for \_ in cuda\_warps} statement is parallelized over only a subset of warps.
Something like \lighttt{with cuda\_warps(lo, hi):} may work.

\filbreak
\myKey{Async-for:} The \lighttt{for \_ in seq(lo, hi, xmodel=...)} construct will have a new optional \lighttt{xmodel} (execution model) parameter.
The default xmodel is sequential; all others are asynchronous xmodels.
If an async xmodel is given, then child statements are consider to execute with an asynchronous xmodel, marking them as eligible for replacement with asynchronous CUDA constructs.
The for loop and all child for loops are considered async-for loops in this case.
Child for loops must not be parallel-for loops, and must not specify a different xmodel.

\filbreak
\myKey{Sequential-for:} for loops other than as described above are sequential-for loops.

\hook{Hook:} Need to teach Exo that parallel-for and async-for loops are to be treated as ordinary for loops under S-semantics.

\filbreak
\myKey{Parallel View (parview):} Borrowing a concept from \textit{Descend}.
A parview consists of an array variable and a mapping function $f$ mapping array indices to parlane indices.
If an array is accessed through a parview at index $i$, it is a promise that the access is being performed by the parlane with index $f(i)$.
Simple patterns may be statically checked as in \textit{Descend}, but note it would not be too difficult to defer checking to runtime by compiling the given mapping function to an \lighttt{assert} statement.
We probably need to extend this to handle asserting that reads and writes won't conflict ``temporally'' i.e. proving that different loop iterations only use disjoint subsets of some array.
This would be needed for ring buffer optimization to work for deep pipelining.

\hook{Hook:} Need to teach Exo to statically verify the promise, or pass through the mapping function to codegen in order to generate the assert.

\filbreak
\myKey{Synchronization Statement:} We need to support \lighttt{barrier} (all-to-all), \lighttt{arrive}, \lighttt{await}, and \lighttt{wgmma\_fence}.
These define a synchronization relation between all GPU threads of the executing parlane.
For example,

\filbreak
{\color{lightttColor}
\begin{verbatim}
for blockIdx in cuda_blocks(lo, hi, warps = 8):
    for threadIdx in cuda_threads(0, 256):
        ...
    barrier()  # is a __syncthreads() as the executing parlane here is a full block
    for warpIdx in cuda_warps(0, 8):
        for threadIdx in cuda_threads(32*warpIdx, 32*(warpIdx + 1)):
            ...
        barrier()  # __syncwarp() as the executing parlane here is a warp
        for threadIdx in cuda_threads(32*warpIdx, 32*(warpIdx + 1)):
            ...
\end{verbatim}
}

By requiring synchronization statements to be lifted to block or warp level, we trivially enforce convergence requirements.

\filbreak
The \lighttt{arrive} and \lighttt{await} statements take parameters \lighttt{(split\_barrier, index, xmodel = sequential)} where \lighttt{split\_barrier} is a variable of split-barrier type constructed at a scope other than thread-scope, the \lighttt{index} is a loop index, and the optional \lighttt{xmodel} parameter determines the specific CUDA construct this compiles to.
This defines a dependency from the threads of the parlane executing the \lighttt{arrive} to the threads of the parlane executing the \lighttt{await} with the same barrier and index parameter.
I'm not sure this is really the right approach to take, but my gut feeling is it's best to expose a higher-level synchronization interface and compile to the appropriate CUDA synchronization primitive depending on the \lighttt{xmodel} (e.g. a ring buffer of \mbarrier).

\filbreak
\hook{Hook:} For the analysis and codegen to be feasible, I'm expecting language restrictions that make it possible to statically verify that for each split barrier constructed, we call at most one matching pair of \lighttt{arrive} and \lighttt{await} for each loop index, with the same parlane used for all arrives and the same parlane used for all awaits.

\filbreak
\myKey{GPU Register Variables:} The barrier lifting requirement will cause the bodies of for-cuda-blocks loops to be split into many for-cuda-threads (or for-cuda-warps) loops.
Thus, we need a way to define GPU register variables whose lifetime can span multiple for-cuda-threads loops.
At block scope, we should allow defining \lighttt{@CUDA\_REGISTER} variables as arrays indexed by thread index and optionally warp index.
At thread scope, a thread must only access entries at its own index.
At warp scope, only the warp index must match -- certain patterns of reads and writes to random access thread indices may be implemented with \lighttt{shfl\_sync}.
GPU register accesses are always considered to be using an implied parview with an identity mapping function.

\filbreak
\mySub{Concepts}

For illustration, imagine that the Exo program is being simulated sequentially by an ``S-machine'', which builds a graph of execution nodes (xnodes) and execution edges (xedges).
Statements executed are grouped into xnodes, tagged with the parlane and xmodel used to execute them.
The (directed) xedges connecting xnodes represent synchronization.
We then analyze this graph to ensure the xedges generated are sufficient to ensure the same results had the program been simulated on an M-machine.

The question is how to implement this with static formal program analysis i.e. without actually doing this simulation.

\filbreak
\myKey{Parscope:} Each parscope is defined by its \textit{resources}, \textit{unit type}, \textit{shape}, and its parent parlane (except the top-level parscope, representing the main CPU thread, has no parent). Two parscopes are the same when all these values match. A parscope is created when executing parallel-for loops of the form

{\color{lightttColor}
\begin{verbatim}
# The parlane that executes this loop is the parent parlane
with cuda_{unit type}s(lo, hi):  # Optional resource specialization
    # Partial parscope defined here (shape not yet defined)
    for _ in cuda_{unit type}s(lo, hi):
        # Invalid parscope (between dimensions)
        for _ in cuda_{unit type}s(lo, hi):  # Optional multidimensional for loop
            # The N-dimensional iteration space is the shape
            # Here we say that we are in {unit type}-scope (e.g. block-scope)
\end{verbatim}
}

Only control flow and synchronization statements may appear where the parscope is partially defined (synchronization operates over the subset of resources specified by the resource specialization)
No code at all can appear where the parscope is invalid.
The parscope contains one parlane of the given unit type for each point (index) in the shape.

The resources of a parlane are a subset of its parent parlane (except for the case of a CPU thread launching a CUDA grid, where the resources are defined by the grid size); the resources are defined by the shape and the optional resource specialization.
There must not be more parlanes than that supported by the available resources, but there may be fewer.

\filbreak
\myKey{Parlane:} A single unit of a parscope, which is assigned to execute one iteration of the parscope's defining parallel-for loop.
Only parlanes with unit type cpu-thread or cuda-thread can execute ``ordinary'' arithmetic, read, or write instructions. 
The expectation is that after scheduling, all code at cluster, block, warpgroup, or warp scope is either synchronization, control flow, or custom procedures.
Two parlanes are equal when they come from the same parscope and correspond to the same index.

\filbreak
\myKey{Execution Model (xmodel):} %% sequential, non_bulk_cp_async, tma_to_shared, tma_to_global, wgmma
The xmodel is an attribute of each memory read or write performed, as defined by the async-loop construct.
We need the following xmodels; all models other than \lighttt{sequential} are async xmodels:
\begin{enumerate}
  \item \lighttt{sequential} (default)
  \item \lighttt{non\_bulk\_cp\_async} (Ampere non-bulk async copies)
  \item \lighttt{tma\_to\_shared} (Hopper bulk async copies to shared memory; mbarrier synchronization)
  \item \lighttt{tma\_to\_global} (Hopper bulk async copies to global memory; async-group synchronization)
  \item \lighttt{wgmma}
\end{enumerate}

\filbreak
\myKey{Race Eligibility:} In the context of analyzing a write operation, a read operation or another write operation is considered to be \textit{race-ineligible} if any of the following:

\begin{enumerate}
  \item The operations are not using the same variable.
  \item The operations are done using the same parview, and are performed by parlanes from the same parscope; if either xmodel is not sequential, the two parlanes must be distinct.
  \item (Future work) The two operations are both atomic operations of the same type (e.g. both \lighttt{atomicAdd}), and the return value of the atomic is not used.
\end{enumerate}

If none of these apply, the operation in question is \textit{race-eligible}.

\filbreak
\myKey{Execution Node (xnode):}
Vertices of the graph being built for analysis.
The S-machine records reads and writes performed as xnodes, with each xnode tracking the access set (reads and writes), the parlane used, and the xmodel used.

When simulating code under the sequential xmodel, the S-machine may group the effects of a block of sequential statements into one xnode, as long as the executing parlane has not changed and no synchronization statement occurs in the block. When simulating code under an async xmodel, each statement must become its own xnode.

Note that there is a total ordering of the xnodes under S-semantics.

\filbreak
\myKey{Access Sets:} Each xnode has a write set and a read set, tracking which variables were written to / read from, and optionally the parview used.
Note array indices are not tracked.

\hook{Hook:} Building these access sets will require some non-trivial introspection.
The lack of index information should greatly simplify this.

\filbreak
\myKey{Execution Edge (xedge):} Directed edge connecting two xnodes.
The S-machine generates explicit xedges between xnodes related by a synchronization operation.
The S-machine generates implicit xedges between xnodes that are executed sequentially by the same parlane.
To model all the async proxy complexity, we will have to store a \textit{lot} of info for each xedge, but at a minimum, each explicit xedge records the xmodel used for the arrive operation and the await operation (implicitly \lighttt{sequential} for \lighttt{barrier}, and implicitly \lighttt{sequential}-arrive, \lighttt{wgmma}-await for \lighttt{wgmma\_fence}).

\filbreak
\myKey{Execution Edge Admissibility:} Not all synchronization points can be used to handle all dependencies.
A pair of writes, or a read and a write, are \textit{admissible} to an xedge if the underlying synchronization primitive is documented to protect the kind of memory operation done. For example, only non-async instructions' reads and writes are admissible to the xedges defined by a \lighttt{\_\_syncthreads}.

\filbreak
The most important restriction for an explicit xedge is that the xmodel used for the prior memory operation and the xmodel used for the subsequent memory operation must match, respectively, the xmodel used for the arrive and await (exceptions: Ampere non-bulk async copies can be protected by an \textit{await} operation with sequential xmodel; \lighttt{wgmma.fence} handles both \lighttt{sequential} and \lighttt{wgmma} xmodels for the implied arrive).

\filbreak
Implicit xedges (defined between subsequent xnodes executed with the same parlane) admit only reads and writes with the \lighttt{sequential} xmodel, except for the special ``same matrix format'' exception for \lighttt{wgmma} accumulators.

\filbreak
An xedge may further restrict the types of memory types for admissible memory operations, e.g., the xedges defined by the \lighttt{wgmma.fence} further restrict admissibility to register reads and writes.

\filbreak
\mySub{Safety Checking}

The hypothetical model I'm imagining is that we simulate an S-machine that executes the program under S-semantics, constructing the graph of xnodes and xedges. Then, for each xnode $n_1$, and for each write $w_1$ recorded (variable-parview tuple) in the \textbf{write set} of the xnode
\begin{enumerate}
  \item Search backwards in S-order for the previous xnode $n_0$ that has a race-eligible write $w_0$.
  \item Check that there exists a directed path of xedges from $n_0$ to $n_1$ that admit the pair of writes $(w_0, w_1)$ (WAW hazard check).
  \item For each xnode $n_2$ in between $n_0$ and $n_1$ in S-order, if the read set of $n_2$ contains a race-eligible read,
  \begin{enumerate}
    \item Check that there exists a directed path of xedges from $n_0$ to $n_2$ that admit the write/read pair $(w_0, r)$ (RAW hazard check).
    \item Check that there exists a directed path of xedges from $n_2$ to $n_1$ that admit the read/write pair $(r, w_1)$ (WAR hazard check).
  \end{enumerate}
\end{enumerate}
Finally, we check that there exist no xedge cycles (deadlock freedom).

\filbreak
\mySub{Unanswered Questions}

Primarily, I need to learn about formal program verification and figure out how to compile this simulation-based safety model into something that is statically verifiable.

Further unanswered questions:
\begin{enumerate}
  \item Compiling \lighttt{arrive} and \lighttt{await} statements into real CUDA synchronization constructs.
    In particular, there's the difficulty of proving 1:1 correspondence between arrive and await operations.
    Need to analyze the loop iteration space.
  \item Extensions to the parview concept needed to express common safe patterns such as deep pipelining and ring-buffer optimization (statically partitioning an array into disjoint subsets that are uniquely assigned to loop iterations, rather than uniquely assigned to parlanes).
\end{enumerate}

\end{document}
