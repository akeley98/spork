%exocc guide_correct.py && python3 code_to_tex.py guide_correct.py guide && python3 code_to_tex.py guide_wrong.py guide && xelatex spork_guide.tex < /dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\begin{document}
\myTitle{Exo GPU -- Spork User Guide}

We're extending Exo with constructs for generating CUDA device code, with the goals of supporting
\begin{itemize}
  \item explicit instruction selection, including support for asynchronous copies and tensor cores (wgmma)
  \filbreak
  \item explicit selection of memory types for variables
  \filbreak
  \item explicit assignment of work to threads (no implied parallelism; Exo's imperative C-style programming is preserved) and explicit synchronization (choice of barriers, mbarriers, commit group)
  \filbreak
  \item codegen to CUDA C++ headers and CPU C code, which will allow us to support mixed CPU/GPU code, and mixed Exo-generated and handwritten CUDA
  \filbreak
  \item ``sequential logic'' for scheduling; that is, Exo's existing analysis continues to use sequential semantics;
    all parallelism features are expressed using existing concepts with ``annotations'' (e.g. parallel-for loops) that can be ignored to analyze the program as a sequential program
  \filbreak
\item S/M equivalence (single-threaded / multi-threaded equivalence); we check that the scheduled program, interpreted sequentially, will give the same results as the scheduled program interpreted as a parallel CUDA program.
    This two phase checking (Exo scheduling with sequential semantics, S/M equivalence checking) proves that the unscheduled sequential program gives the same results as the final parallelized program.
\end{itemize}
\filbreak
In the initial product, we will support checking only for concrete problem sizes, using a simulator that tracks each memory access.
\filbreak
\begin{tikzpicture}[node distance=8mm]
\sffamily
\node(proc0) [normalnode, text width=2cm, minimum width=2cm] {Original proc};
\node(procNS) [normalnode, right=of proc0, text width=4cm, minimum width=4cm] {Scheduled proc, interpreted \myKeyA{sequentially}};
\node(procNM) [normalnode, right=of procNS, text width=4cm, minimum width=4cm] {Scheduled proc, \myKeyB{parallel} interpretation};
\node(cuda) [normalnode, right=of procNM] {CUDA C++ header};

\draw [arrow] (proc0) to node(exo)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=4cm, minimum width=4cm, below=of exo, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Existing Exo Rewrites};
\node(sync) [normalnode, text width=5cm, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {S/M equivalence check: ``above the waterline''};
\node(spork) [normalnode, text width=6cm, xshift=1cm, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Backend Compiler: ``below the waterline'' (not proven correct)};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg, xshift=15mm] {Physically same proc, \textbf{different interpretation}};
\node(caption) [left=of note] {``\textbf{chain of equivalence}''};
\node(c) [smallnode, right=of note] {C code};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\draw [arrow] (procNM.east) to (c.west);
\end{tikzpicture}

\filbreak
\myTitle{Fundamental Language Concepts}

In Exo-GPU, each statement is executed with a certain \myKeyA{thread collective}, or grouping of threads guaranteed to have uniform control flow, and with a certain \myKeyA{instruction timeline} (``actor kind'' today, but this will probably change).
We statically analyze, for each statement of the program, its \myKeyA{instruction timeline} and \myKeyA{collective unit}; these are defined by the user via async blocks and cuda\_threads loops (``parallel for''), respectively.

\filbreak
The instruction timeline (\myKeyA{instr-tl}) categorizes instructions based on their synchronization requirements (e.g. CPU, in-order ``classic'' CUDA instructions, and various non-ordered asynchronous CUDA instructions, e.g. cp.async, wgmma, TMA).

\filbreak
The \myKeyA{collective unit} describes the thread count and ``shape'' of the concrete \myKeyA{thread collectives} assigned to execute the statement.
For example, the collective unit ``warp'' describes the thread intervals [0,31], [32,63], [64,95],...

\filbreak
\myTitle{Object Code Features}

\mainSub{Async Blocks (Instruction Timeline Control)}

\begin{figure*}[!b]
\input{guide/async_block.0.tex}
\caption{Example async blocks, with sm\_80 cp.async instructions used}
\label{fig:AsyncBlocks}
\end{figure*}

All non-\lighttt{@instr} procs consist of CPU code at the top level.
Exo-GPU defines two kinds of async blocks, which override the instr-tl within the block \textbf{(figure \ref{fig:AsyncBlocks})}.

\filbreak
\lighttt{with CudaDeviceFunction(...)}: from CPU code, open a block of in-order CUDA code.
This lowers to a CUDA kernel launch, parameterized with keyword arguments taken from the \lighttt{CudaDeviceFunction} object (e.g. \lighttt{clusterDim}, \lighttt{blockDim}, \lighttt{blocks\_per\_sm}).

\filbreak
\lighttt{with CudaAsync(<instr-tl>)}: within a CUDA device function, open a block of async CUDA code, using instructions with the specified instr-tl (``actor kind'').

\filbreak
The selection of instructions within async blocks is not automated, except simple scalar code.
Each Exo instr declares its instr-tl, and each memory type declares the instr-tl needed for allocations, reads, and writes (e.g. CUDA global memory -- \lighttt{CudaGmemLinear} -- must be allocated from the CPU but can only be accessed from CUDA device code).
The compiler checks each usage matches that of the instr-tl of the current scope.
For example, in \textbf{figure \ref{fig:AsyncBlocks}}, if the \lighttt{Sm80\_cp\_async\_f32} gets moved outside of the \lighttt{CudaAsync} block, the compiler will issue a ``wrong instr-tl'' error.

\minorKey{S/M Equivalence:} We need this feature to be able to reason about whether the correct barriers are used when checking S/M equivalence. For example, a barrier that lowers to \lighttt{\_\_syncthreads();} will be enough to ensure visibility across the CTA for prior in-order CUDA instructions, but not prior \lighttt{cp.async} instructions.

\filbreak
\mainSub{Loop Mode}

\begin{figure*}[!b]
\input{guide/intro_tasks_threads.0.tex}
\caption{Simple vector addition example. We break \lighttt{X} and \lighttt{Y} into chunks of 128 values, each handled by one ``task''.}
\label{fig:intro_tasks_threads}
\end{figure*}

Each for loop has an associated loop mode, conveyed using the syntax

\input{guide/loop_mode_syntax.0.tex}

The loop mode is a compile time object (i.e. a literal Python object created while scheduling) created using the supplied keyword arguments.
In contrast, the \lighttt{iter} variable, and \lighttt{lo, hi} bounds, are runtime values.
We call the \lighttt{iter} variable a \lighttt{loop-mode-name}-iterator (e.g. \lighttt{seq}-iterator, \lighttt{cuda\_threads}-iterator).

\filbreak
Exo already defines \myKeyA{\texttt{seq}} (sequential) and \myKeyA{\texttt{par}} (OpenMP) loop modes.
The new loops modes, \myKeyA{\texttt{cuda\_tasks}} and \myKeyA{\texttt{cuda\_threads}}, form a strict two-level hierarchy within a \lighttt{CudaDeviceFunction} block.

\filbreak
The only child statement of a \lighttt{CudaDeviceFunction} block is a loop nest of 1 or more \lighttt{cuda\_tasks} loops \textbf{(figure \ref{fig:intro_tasks_threads})}.
The loop nest defines a space of ``tasks'' which are assigned round-robin to \myKeyA{top-level collectives} to execute (i.e. we implement persistent kernels).
If \lighttt{clusterDim=1} (default), then the top-level collective is a CTA; otherwise, it's a cluster.

\filbreak
Within the inner-most \lighttt{cuda\_tasks} loops, both \lighttt{seq} and \lighttt{cuda\_threads} loops may appear.
The \lighttt{cuda\_threads} loop modifies the collective unit, to be described in the next section.

\minorKey{S/M Equivalence:} This is the core part of the S/M equivalence model, i.e., that the parallelism effected by the parallel loops doesn't change the program's output compared to sequential execution.

\filbreak
\mainSub{cuda\_threads Loops (Collective Unit Control)}

\input{guide/cuda_threads_syntax.0.tex}

The \lighttt{cuda\_threads} loop takes a \myKeyA{collective unit} argument, and assigns one thread collective to execute each ``iteration'' of the loop.
In the common case, the collective unit of the loop body is the same as the \lighttt{unit} argument passed; the standard library provides \lighttt{cuda\_thread}, \lighttt{cuda\_warp}, \lighttt{cuda\_warpgroup}, and \lighttt{cuda\_cta\_in\_cluster} as collective units.

\filbreak
Unlike some other systems, we do not have a fixed hierarchy of collective units, and the user can define their own.
To expand the collective unit by a factor of \lighttt{N}, use the ``\lighttt{N *}'' prefix , e.g. \lighttt{32 * cuda\_thread} is the same as \lighttt{cuda\_warp}.
More advanced cases (e.g. non-contiguous groups of threads) may be handled by creating a \lighttt{CollUnit} object directly.

\filbreak
Unlike CPU parallel-for loops, the \lighttt{cuda\_threads} loop
\begin{itemize}
  \item Supports assigning multiple threads to execute one iteration (when \lighttt{unit} is not \lighttt{cuda\_thread}).
  \item Cannot spawn new threads, and instead subdivides threads supplied by the thread collective that executes the parallel-for.
    For example, the loop \lighttt{for i in cuda\_threads(0, \redBox{10}, unit=\blueBox{4 * cuda\_thread})} will not compile when placed in a scope with collective unit warp, since \redBox{10} $\times$ \blueBox{4} = 40 exceeds the 32 threads available in a warp.
  \item Requires \lighttt{lo} to be 0 and \lighttt{hi} to be a compile-time constant.
\end{itemize}

See \lighttt{coll\_algebra.pdf} for more detailed discussion (including the non-common case where the actual collective unit is different from the \lighttt{unit} parameter specified).

\filbreak
Use nested \lighttt{cuda\_threads} loops to define a multidimensional iteration space.
The \lighttt{cuda\_threads} loops lower to C++ code that subdivide thread collectives by assigning different iterator values based on thread index:

\input{guide/cuda_threads_cxx.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (cta) [yellownode, text width=20mm, minimum width=20mm, minimum height=40mm] {CTA};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m = 0}; threads [0, 15]};
\node (m7) [Mnode, below=of m2, yshift=-9mm] {\texttt{m = 7}; threads [112, 127]};
\draw [arrow] (cta) -- node[above] {\texttt{for \greenBox{m}}} (m2);
\draw [dotted] (m2) -- (m7);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m7n0) [Nnode, right=of m7, xshift=16mm] {112};
\node (m7n1) [Nnode, right=of m7n0] {113};
\node (m7n2) [Nnode, right=of m7n1] {114};
\node (m7n15) [Nnode, right=of m7n2, xshift=8mm] {127};
\draw [arrow] (m0) -- node[above] {\texttt{for \violetBox{n}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \violetBox{n}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \violetBox{n}}} (m2n0);
\draw [arrow] (m7) -- node[below] {\texttt{for \violetBox{n}}} (m7n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n=0}} (m7n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n=1}} (m7n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n=2}} (m7n2);
\draw [dotted] (m2n15) --node[] {\texttt{n=15}} (m7n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m7n2) -- (m7n15);
\end{tikzpicture}

Finally, similar to the instr-tl checks, Exo-GPU statically checks that scalar reduce/writes are performed using single threads only, and that all instructions are executed with the correct collective unit (e.g. wgmma instructions must be executed with a collective unit equivalent to \lighttt{cuda\_warpgroup}).

\filbreak
\mainSub{Warp Specialization}

We primarily support warp specialization with the ``named warps'' feature.
As an alternative to the \lighttt{blockDim} argument, the \lighttt{CudaDeviceFunction} block supports \lighttt{warp\_config}.
Use this to populate the CTA with warps that have names, and optional register counts (using the PTX instruction \lighttt{setmaxnreg}).
Then use \lighttt{with CudaWarps} to map a subtree of code to execute with the warps named (this changes the collective unit inside the body of the \lighttt{CudaWarps} block).

\input{guide/my_warp_config.0.tex}

\filbreak
We generate completely separate C++ code paths for each warp name.
This way, warps don't pay static costs for code only executed by differently-named warps (particularly crucial for \lighttt{setmaxnreg}).
For example, the above code generates C++ helper functions \lighttt{exo\_deviceTask\_consumer}, \lighttt{exo\_deviceTask\_producer}, and \lighttt{exo\_deviceTask\_unused}.

\filbreak
Additionally, \lighttt{with CudaWarps(lo, hi)} just selects a subset \lighttt{[lo, hi)} of the current thread collective's warps for executing child statements\footnote{This is compatible with named warps, but numbered \lighttt{CudaWarps} blocks must appear as children of named \lighttt{CudaWarps} blocks.} (indices are interpreted relative to the start of the thread collective).
For example, the following selects threads [96, 127] and [224, 255]:

\input{guide/warpgroup_CudaWarps.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (wg1) [widenode] {\texttt{wg = 1}; threads [128, 255]};
\node (wg0) [widenode, above=of wg1] {\texttt{wg = 0}; threads [0, 127]};
\draw [draw=white] (wg1) to node(wg_midpoint)[]{} (wg0);
\node (cta) [normalnode, text width=18mm, minimum width=18mm, minimum height=18mm, left=of wg_midpoint, xshift=-44mm] {CTA};
\draw [arrow] (cta) -- node[above] {\texttt{for wg}} ($(wg_midpoint)-(3.2,0)$);
\node (CudaWarps0) [normalnode, minimum height=8mm, right=of wg0, xshift=30mm] { [96, 127] };
\node (CudaWarps1) [normalnode, minimum height=8mm, right=of wg1, xshift=30mm] { [224, 255] };
\draw [arrow] (wg0) -- node[above] {\texttt{CudaWarps(3, 4)}} (CudaWarps0);
\draw [arrow] (wg1) -- node[above] {\texttt{CudaWarps(3, 4)}} (CudaWarps1);
\end{tikzpicture}

\filbreak
\mainSub{Distributed Memory}

A common pattern in CUDA tensor processing programs is to have a tile of values owned by a large thread collective (e.g., a full CTA), but stored in a memory type that is physically allocated at a finer granularity (e.g. registers, which are allocated per-thread).
For example, each thread may allocate an $8 \times 4$ tile, and the threads in the CTA may be arranged in a $16 \times 32$ grid, causing the full CTA matrix to be $128 \times 128$.
We call this pattern \myKeyA{distributed memory}, and the physically allocated pieces \myKeyA{distributed shards}.

\filbreak
Similar patterns may occur with larger granularity, for example, allocating per-warpgroup shards for wgmma, or per-CTA shards for (cluster) distributed SMEM.
In Exo, this all falls under the umbrella of ``distributed memory''.

\filbreak
In handwritten CUDA C++, the declared size of the allocation (distributed shard) won't match the logical size of the tile\footnote{Template libraries like CuTe may smooth this over.}. In Exo-GPU, we want to
\begin{itemize}
  \item Have tensor allocations declare the logical size of the tile. This prioritizes expressing \textit{dataflow} in the object code, rather than lowering details (crucial for S/M equivalence).
  \filbreak
  \item Support predictable \& user-controllable subdivision of the tile into distributed shards (in contrast to, for example, Triton's automated approach).
\end{itemize}

\filbreak
Exo-GPU meets these goals with \myKeyA{distributed memory deduction}, dividing dimensions into \redBox{distributed dimensions}, to the left, and \yellowBox{non-distributed dimensions}, to the right.
Identically, windowing and indexing expressions divide into distributed indices and non-distributed indices, with distributed indices required to be plain reads of \lighttt{cuda\_threads}-iterators.
The non-distributed dimensions give the size of the distributed shards.
In Exo syntax, the previous example looks like:

\filbreak
\input{guide/simple_dist.0.tex}

\filbreak
Our goal is to have this ``just work'' for common cases, by inferring distributed memory from the memory's usage pattern (indexing expressions), rather than requiring explicit annotations on distributed memory allocations.
The user still has full control over this via their loop structure and scheduling operations like \lighttt{reorder\_dim}.
This guide contains only a summary of the deduction rules; \lighttt{coll\_algebra.py} contains precise documentation.

\filbreak
\mainKey{Rule 1/4, Native Units:} Each memory type has its own \myKeyA{native unit}, which is the collective unit expected for physically allocating the memory type; e.g., \lighttt{CudaRmem} has native unit \lighttt{cuda\_thread}, and \lighttt{CudaSmemLinear} has native unit CTA.
The deduction assigns each distributed shard to be allocated by its own distinct thread collective that matches the native unit.
We enforce this ``one-to-one'' distinctness requirement by deducing a tiling chain.

\begin{figure*}[!b]
\input{guide/simple_dist.1.tex}
\caption{Distributed memory example, with tiling chain annotated}
\label{fig:tiling_chain}
\end{figure*}

\begin{figure*}[!b]
\input{guide/simple_dist_cxx.0.tex}
\caption{Distributed memory example, compiled to CUDA C++}
\label{fig:simple_dist_cxx}
\end{figure*}

\filbreak
\mainKey{Rule 2/4, Tiling Chains:} 
The \myKeyA{tile thread count} of a given scope is the number of threads in the scope's collective unit, \textbf{ignoring} the effects of warp specialization (see \lighttt{coll\_algebra.py} for more precise treatment).
The \myKeyA{tiling operator} for a \lighttt{cuda\_threads}-iterator maps the tile thread count $t_0$ of the loop's parent scope to the tile thread count $t_1$ of the loop body; we denote this as $\mathit{iter}: t_0 \mapsto t_1$ \textbf{(figure \ref{fig:tiling_chain})}.
A valid \myKeyA{tiling chain} is a permutation of \lighttt{cuda\_threads}-iterators such that
\begin{itemize}
  \item let $t_a$ be the tile thread count in the scope of the variable's allocation;
  \item let $t_n$ be the number of threads in the native unit;
  \item the permuted iterators' tiling operators form a chain $t_a \mapsto ... \mapsto t_n$.
\end{itemize}
\filbreak
The permutation is unambiguous, aside from no-op tiling operators $\mathit{iter}: t_0 \mapsto t_1$ with $t_0 = t_1$.

\filbreak
Each time the distributed memory is indexed, we consume indices starting from the left, until the consumed indices form a valid tiling chain (successful deduction), or we conclude that deducing a valid tiling chain is impossible, or until we cannot consume another \lighttt{cuda\_threads}-iterator (the deduction fails; recall that distributed indices must be \lighttt{cuda\_threads}-iterators).
The consumed indices are distributed, and the remaining indices are non-distributed.

\filbreak
\mainKey{Rule 3/4, Consistent Deductions:} Each time a given variable is indexed, we run the distributed memory deduction again, and the result must match that deduced from all other usages of the variable, i.e., the same thread collective is always used for a given distributed shard.

\filbreak
\mainKey{Rule 4/4, Lowering:} The distributed dimensions and indices get stripped away as part of the code lowering process.
The generated C++ contains only non-distributed indices \textbf{(figure \ref{fig:simple_dist_cxx})}.
More importantly for the Exo user, the compiler removes distributed dimensions and indices before calling code generation callbacks for memory types and instructions.
So the authors of externalized stuff can remain completely ignorant of this system.

\begin{figure*}[!b]
\input{guide/why_dist.0.tex}
\caption{Counterexample for S/M equivalence for a hypothetical design where variables are implicitly duplicated across threads. Note: in general we cannot fuse the \lighttt{tid} loops or sink the allocation of \lighttt{x} into the loop scope, since there may be required synchronization or other convergent code between them.}
\label{fig:why_dist}
\end{figure*}

\filbreak
\minorKey{S/M Equivalence:} The distributed dimensions of an allocation must be explicit in Exo object code for S/M equivalence to work.
If we were to adopt the approach that each thread collective has its own implicit copy of a variable, then there would be unintended aliasing when intepreting the program sequentially \textbf{(figure \ref{fig:why_dist})}.

\filbreak
Another reason: although for now we describe each distributed shard as being accessible only by its owner thread collective, we plan to support instructions like warp shuffles and TMA multicast that allow limited exceptions to this rule (respectively, accessing registers of other threads in the same warp, and multicasting to SMEM in different CTAs of the cluster).
This means it's really important that the distributed indices be explicit in Exo object code, so the user can vary them and control the indices used for shuffling.

\filbreak
\mainSub{Synchronization Statements}

\begin{figure*}[b!]
\input{guide/sync_warp_cta.0.tex}
\caption{Synchronizing within a warp, and across warps}
\label{fig:sync_warp_cta}
\end{figure*}

\begin{figure*}[b!]
\input{guide/Sm80_cp_async_simple.0.tex}
\caption{Synchronizing mixed synchronous and asynchronous instructions}
\label{fig:Sm80_cp_async_simple}
\end{figure*}

Unlike everything else in Exo, synchronization statements support a somewhat abstracted style, where the user just specifies which sets of threads and what category of instruction effects to synchronize, rather than directly calling wrapped CUDA synchronization instructions.
We still support an imperative programming style (as opposed to automated synchronization, or a dependency graph approach or such), where synchronization statements appear in-line with and get executed in sequence with other imperative Exo code.

\filbreak
\mainKey{Timeline:} Each memory action (read or write) is performed with a certain instr-tl (instruction timeline), a certain usage-tl (usage timeline), and thread(s)\footnote{plural, because some instructions are executed by multiple threads convergently, e.g. wgmma. How to precisely model this is a hole in my current model.}.
The \myKeyA{instr-tl} and \myKeyA{threads} for a memory action are statically deduced from the instr-tl and collective unit in-scope; the \myKeyA{usage-tl} is programmed per-parameter for wgmma instructions (usage-tl is not relevant for non-wgmma instructions).
See \lighttt{summer\_of\_wgmma.pdf}, ``Timelines and Synchronization'' for more details on usage-tl.

\filbreak
Parameterize synchronization statements with compile-time \myKeyA{sync-tl} values (synchronization timelines), which contain sets of $(\text{instr-tl} \times \text{usage-tl})$ pairs (additional state described in \lighttt{summer\_of\_wgmma.pdf}, ``Timelines and Synchronization'').

\filbreak
\mainKey{Fence:} The \lighttt{Fence(first\_tl, second\_tl)} statement expresses a non-split barrier.
Informally, this causes all threads in the executing \myKeyA{thread collective} to wait for all other such threads.
This means that syntactically-identical \lighttt{Fence} statements may lower to different code, depending on the collective unit in scope \textbf{(figure \ref{fig:sync_warp_cta})}.

\filbreak 
The sync-tl parameters filter the effects of the fence \textbf{(figure \ref{fig:Sm80_cp_async_simple})}.
That is, the fence orders memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{second\_tl}
  \item executed with thread(s) of the fence's executing thread collective
  \item executed after the fence
\end{enumerate}
\filbreak
after memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{first\_tl}
  \item executed with thread(s) of the fence's executing thread collective
  \item executed before the fence
\end{enumerate}

\filbreak
TODO: implement the timeline system (replace actor kinds, actor signatures) and document them all.
Anything you've seen anywhere else is probably really out of date now.

\filbreak
\mainKey{Split Barrier (Arrive, Await):}

A combination of \lighttt{Arrive(first\_tl, barrier\_name[...], N)} (syntax to change\footnote{\lighttt{Arrive(first\_tl, N) >> barrier\_name[...]}}) and \lighttt{Await(barrier\_name[...], second\_tl, N)} statements express split barriers.
\lighttt{first\_tl} and \lighttt{second\_tl} are sync-tl values, as before; \lighttt{barrier\_name} is a \lighttt{barrier}-typed variable, and \lighttt{N} is an integer (to be explained in the ``Queue Barrier'' section).

\filbreak
Declare a barrier variable as \lighttt{<varname>: barrier @ <Memory>}, where \lighttt{<Memory>} is one of \lighttt{CudaCommitGroup}, \lighttt{CudaMbarrier}, or \lighttt{CudaEvent}.
The memory type controls how the barrier is implemented in CUDA.

\filbreak
Indices may be supplied in the \lighttt{barrier\_name}; each separate position is a logically separate barrier.
Currently the \lighttt{barrier} variable declaration doesn't express if it's an ``array'' or not ... maybe change this.
Interpret all indices as distributed indices (unlike non-barrier allocations, there is no native unit that tells distributed memory deduction when to stop).

\filbreak
TODO: I'm summarizing the behavior of split barriers in the abstract machine, but for real users it will likely make much more sense to lead with ``this is the barrier you need to generate CUDA construct X''.

\filbreak
When an arrive statement executes, it orders memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{second\_tl}
  \item executed with thread(s) of the arrive statement's executing thread collective
  \item executed after the arrive statement
\end{enumerate}
\filbreak
after memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{first\_tl}
  \item executed with thread(s) of the paired await statement's executing thread collective (pairing to be described in the ``Queue Barrier'' section)
  \item executed before the paired await statement\footnote{For TMA, we will have to directly associate instructions with awaits, hence the move to \lighttt{>>} syntax. It will look something like \lighttt{some\_tma\_instr(...) >> barrier\_name[...]}.}.
\end{enumerate}

\filbreak
For \lighttt{CudaMbarrier} only, we support \lighttt{ReverseArrive} and \lighttt{ReverseAwait} statements, which behave and pair with each other just an \lighttt{Arrive} and \lighttt{Await} statements do, except that reversed statements use a hidden ``reverse barrier'' array parallel to the primary ``forward barrier'' array.
TODO: this seems bizarre at first glance; explain why this exists.

\filbreak
See \lighttt{summer\_of\_wgmma.pdf}, ``Arrive/Await Pairing and Multicast'', for detailed usage requirements of barriers.

\filbreak
\mainKey{Queue Barrier:} In our abstraction, an arrive and await statement use the same \myKeyA{queue barrier} state iff they use the same barrier variable and index values (same for reverse arrive and reverse await statements, which use separate queue barriers from their non-reverse counterparts).
We control pairing with the \lighttt{N} parameter.

\filbreak
For simple usage, $N = 1$ for arrive statements.

\filbreak
For await statements with $N \ge 0$, the await pairs with the $N^{th}$ prior arrive statement using the same queue barrier
(0-indexed, so $N = 0$ causes await statements to pair with the latest arrive statement).
This models the meaning of \lighttt{wait\_group N} in CUDA, i.e., wait for all but the last N-many \lighttt{commit\_group} instructions.

\filbreak
For await statements with $N < 0$, let \lighttt{lag = \textasciitilde N}, and let $Q^2$ be the number of awaits executed using the queue barrier (including this one).
Then the await statement pairs with the $(Q^2 - \lighttt{lag})^{th}$ arrive statement using the same queue barrier.

\filbreak
\minorKey{S/M Equivalence:} We need this thin abstraction since checking that the user satisfied the requirements of the abstract machine is much more tractible than trying to model the semantics of, and check correct usage of, raw CUDA synchronization instructions. See the next section for more justification.

\filbreak
\myTitle{Abstract Machine (Synchronization Model)}

We will only sketch the basic concepts behind ``synchronization checking'' (which is a somewhat misleading term).
In this process, we will store, in each memory location, a history of each read and write performed on that location.
These ``read visibility records'' and ``write visibility records'' store the set of threads (and on what timelines) that can observe the read or write.
These records start only with the thread/timeline that issued the memory action, but they may grow as a result of synchronization statements.

\filbreak
This process is really defining the semantics and correct usage requirements of the abstract machine that Exo-GPU code executes on, rather than ``synchronization checking'', as synchronization checking is any possible algorithm for verifying a program meets those correct usage requirements (which need not be our initial, simulate-each-memory-access algorithm).
Given that the Exo-GPU program passes these requirements, it's guaranteed to lower to a correctly-synchronized CUDA C++ program.

\filbreak
A \myKeyA{timeline signature} is a 3-tuple $(\text{instr-tl}, \text{usage-tl}, \text{thread ID})$; each memory action is performed with a certain timeline signature.

\filbreak  
A \myKeyA{visibility record} contains an \myKeyA{async visibility set} $V_A$ and a \myKeyA{sync visibility set} $V_S$, both consisting of timeline signatures.

\filbreak
The abstract machine interprets the program sequentially; for each read or write, we add a new read or write visibility record for that location.
$V_A$ is initialized with the timeline signature of the memory action, and $V_S$ is initialized either the same as $V_A$, or as the empty set, for synchronous and asynchronous instructions, respectively.

\filbreak
When interpreting a read, a RAW hazard (read-after-write) gets detected if \textit{any} $V_S$ of a write visibility record for the read location does not contain the timeline signature of the interpreted read.

\filbreak
When interpreting a write, a WAW or RAW hazard gets detected if \textit{any} $V_S$ of a write or read visibility record for the written location does not contain the timeline signature of the interpreted write.

\filbreak
If no hazards occur, the program has correct usage.

\filbreak
Each fence and await statement interpreted defines a ``second visibility set'' $V_2 = L_2 \times C$, with $L_2$ being a set of $(\text{instr-tl} \times \text{usage-tl})$ from \lighttt{second\_tl: sync-tl}, and $C$ being the threads of the executing thread collective.
When we interpret the statement, we check for visibility records that synchronize-with the fence or the paired arrive statement.
Roughly, a visibility record synchronizes-with a fence or an arrive if it contains a timeline signature in $V_A$ that is targetted by the fence or arrive statement\footnote{The precise requirements are in the linked document, and involve a similar first visibility set $V_1$}.
All such visibility records get augmented, setting $V_A \leftarrow V_A \cup V_2$ and $V_S \leftarrow V_S \cup V_2$.
In this way, $V_S$ grows (including cases where timeline signatures ``graduate'' from $V_A$ to $V_S$), which resolves potential future hazards.

\filbreak
Common point of confusion: the read and write visibility sets are \textit{not} ``allowed to read'' and ``allowed to write'' sets.
Hazard checking is essentially a ``conjunction of disjunctions''; ``disjunction'', because visibility sets grow as a union of $V_2$ sets, and ``conjunction'', because of how all visibility sets $V_S$ must contain the checked timeline signature.
A single visibility set by itself doesn't give enough permission information.

\filbreak
See \lighttt{summer\_of\_wgmma.pdf}, ``Timelines and Synchronization'' for more details.

\filbreak
\myTitle{Conclusion}

\mainSub{User's View}

To the language user, Exo-GPU consists of a sequential language with added parallelization directives that cause the Exo program to predictably transform into parallel CUDA C++ code.
These constructs, with the exception of some synchronization code, transform 1:1 to CUDA C++, i.e., the generated CUDA C++ code will be syntactically ``isomorphic'' to the scheduled imperative Exo-GPU code.

\filbreak
The user generally starts scheduling their proc as sequential code, postponing substituting GPU instructions and inserting parallelization directives (parallelize for loop, etc.) as long as possible.
After the ``skeleton'' of sequential code is ready, the user may issue scheduling directives to move subtrees of code from the CPU to the CUDA device, to parallelize loops, to explicitly assign variables to different CUDA memory types, to substitute asynchronous instructions, and to insert needed synchronization.

\filbreak
After issuing these directives, the user must ensure that the program, when dynamically executed on the abstract machine, meets the correct usage requirements of the abstract machine, at least for the concrete problem size they care about.
The user may view the abstract machine as a sequential computer that tracks additional per-memory-location state that's relevant for ensuring the final compiled CUDA C++ program is correctly synchronized.

\filbreak
This thin abstraction layer over raw CUDA synchronization enables S/M equivalence (allowing use of Exo's existing scheduling) and enables synchronization checking tools that can comprehend the user's intent more clearly than if they were at the level of analyzing raw CUDA code.

\filbreak
\mainSub{Compiler's View}

To the author of the compiler backend, Exo-GPU exposes a number of features that can, for the most part, be translated mechanically into CUDA code.
Only synchronization statements require more in-depth handling (for example, initializing mbarriers at startup).

\filbreak
Because of the static instr-tl and collective unit type systems, the compiler is empowered to (and must) diagnose straightforward incorrect usage of Exo-GPU: invalid parallel-for loops, incorrect collective units for instructions (convergence requirements not met), or incorrect instr-tl for memory/instr usage.
Fully checking correct parallelism is outside the scope of the Exo-to-CUDA compiler; we will check correct usage dynamically, or with hypothetical future language extensions.

\filbreak
\mainSub{Our View\footnote{The ``Our View'' section is a rhetorical device borrowed from the OpenGL spec.}}

We view Exo-GPU programs as targetting a fundamentally sequential abstract machine, but mechanically translatable to efficient and predictable code that is parallelized for execution on a physical device.
This divides the labor of writing correct programs between the user and the compiler author: ``above the waterline'', language users and checking tools must guarantee the correct usage requirements of the abstract machine are followed; ``below the waterline'', the compiler author must guarantee that correct programs written against the abstract machine translate into correct CUDA programs.

\filbreak
We anticipate this model will meet the needs of both programming language researchers and performance engineers.
For PL researchers, this model reduces correctness checking to predicates on sequential programs, and abstracts away the minute details of GPU instructions, while still preserving the essential problem of enforcing correct usage of asynchronous instructions.
For performance engineers, this model retains fine-grained control over instruction selection, mapping of work to threads, and synchronization (at least in the tile-and-tensor-oriented domain Exo is traditionally used for).

\filbreak
In the initial implementation, correct usage will be checked by literal simulation of the abstract machine.
Ideally, this model will enable synchronization correctness checks (or programming language safety features) that are more abstract and efficient, which could be built upon existing formalism that, until now, supported only programs targetting physically sequential machines.

\newpage
\myTitle{Appendix: sm\_80 cp.async GEMM Example}

The appendix builds up an output stationary matmul example, using sm\_80 \lighttt{cp.async} and \lighttt{mma} (warp-cooperative matrix multiply-accumulate).

The kernel
\begin{itemize}
  \item Divides the output matrix into \lighttt{M1 $\times$ N1} tiles; each is a task assigned to one CTA.
  \item Subdivides the CTA tile into sub-tiles, each assigned to one warp of the CTA.
  \item Tiles the input block-row of $A$ and block-column of $B$ by \lighttt{K1=16} on the $k$ dimension.
  \item Caches $A$ and $B$ tiles in a double buffer; one buffer is the destination of in-flight \lighttt{cp.async} instructions and the other is being read from by \lighttt{mma} instructions.
  \item Loops over the $k$-tiles, simultaneously accumulating this iteration's $k$ tiles and issuing the async copies for the next iteration's $k$-tiles.
\end{itemize}

\newpage
\mainSub{Skeleton Code}

We'll fill in the \codecomment{\#...} code later.

The \texttt{\yellowBox{Fence}} statement ensures that the consumer (\lighttt{mma}) code waits for the producer (\lighttt{cp.async}) code to finish, and that the producer code doesn't overwrite a tile that the consumer is still reading from.

\input{guide/xgemm_Sm80_fence.0.tex}

\newpage
\mainSub{Per-Warp Tiles: Distributed Memory}

Here we filled out the code for declaring the accumulator tile \lighttt{D\_rmem}, initializing it to 0 (using \lighttt{Sm80\_mma\_zero\_d\_tf32}), and writing the final accumulator back to global memory (using \lighttt{Sm80\_mma\_store\_d\_tf32}). The native unit for \lighttt{D\_rmem @ Sm80\_RmemMatrixD} is \lighttt{cuda\_warp}, but the variable is declared at CTA-scope, so it's subject to distributed memory deduction.

\input{guide/xgemm_Sm80_fence.1.tex}

\newpage
\mainSub{Async Copies}

The \lighttt{cp.async} instruction gets called by individual threads (unlike warp-cooperative \lighttt{mma}) and, in our usage, copies 16 bytes at a time.
We split the CTA into individual threads with \lighttt{cuda\_threads} loops.
Note that there's too many values to copy all at once, so we have \lighttt{seq} loops; each thread issues multiple \lighttt{cp.async} instructions for loading the $A$ and $B$ tiles.

\input{guide/xgemm_Sm80_fence.2.tex}

\newpage
\mainSub{MMA Instructions}

In the consumer code, we split the CTA into warps.
Each warp owns a distributed shard of \lighttt{D\_rmem} with dimensions \lighttt{[Mw/16, Nw/8, 16, 8]}, but the actual \lighttt{mma} instruction (\lighttt{Sm80\_mma\_tf32} in Exo) operates only on packed $16 \times 8$ tiles, so we need a lot of \lighttt{seq} loops to load the packed tiles from SMEM and accumulate them.
This is another place that \lighttt{D\_rmem} is subject to distributed memory deduction; the compiler checks consistency with the other deductions for \lighttt{D\_rmem}.

\input{guide/xgemm_Sm80_fence.3.tex}

\end{document}
