%exocc guide_correct.py && python3 code_to_tex.py guide_correct.py guide && python3 code_to_tex.py guide_wrong.py guide && xelatex spork_guide.tex < /dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\begin{document}
\myTitle{Exo GPU -- Spork User Guide}

We're extending Exo with constructs for generating CUDA device code, with the goals of supporting
\begin{itemize}
  \item explicit instruction selection, including support for asynchronous copies and tensor cores (wgmma)
  \filbreak
  \item explicit selection of memory types for variables
  \filbreak
  \item explicit assignment of work to threads (no implied parallelism; Exo's imperative C-style programming is preserved) and explicit synchronization (choice of barriers, mbarriers, commit group)
  \filbreak
  \item codegen to CUDA C++ headers and CPU C code, which will allow us to support mixed CPU/GPU code, and mixed Exo-generated and handwritten CUDA
  \filbreak
  \item ``sequential logic'' for scheduling; that is, Exo's existing analysis continues to use sequential semantics;
    all parallelism features are expressed using existing concepts with ``annotations'' (e.g. parallel-for loops) that can be ignored to analyze the program as a sequential program
  \filbreak
\item S/M equivalence (single-threaded / multi-threaded equivalence); we check that the scheduled program, interpreted sequentially, will give the same results as the scheduled program interpreted as a parallel CUDA program.
    This two phase checking (Exo scheduling with sequential semantics, S/M equivalence checking) proves that the unscheduled sequential program gives the same results as the final parallelized program.
\end{itemize}
\filbreak
In the initial product, we will support checking only for concrete problem sizes, using a simulator that tracks each memory access.
\filbreak
\begin{tikzpicture}[node distance=8mm]
\sffamily
\node(proc0) [normalnode, text width=2cm, minimum width=2cm] {Original proc};
\node(procNS) [normalnode, right=of proc0, text width=4cm, minimum width=4cm] {Scheduled proc, interpreted \myKeyA{sequentially}};
\node(procNM) [normalnode, right=of procNS, text width=4cm, minimum width=4cm] {Scheduled proc, \myKeyB{parallel} interpretation};
\node(cuda) [normalnode, right=of procNM] {CUDA C++ header};

\draw [arrow] (proc0) to node(exo)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=4cm, minimum width=4cm, below=of exo, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Existing Exo Rewrites};
\node(sync) [normalnode, text width=5cm, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {S/M equivalence check: ``above the waterline''};
\node(spork) [normalnode, text width=6cm, xshift=1cm, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Backend Compiler: ``below the waterline'' (not proven correct)};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg, xshift=15mm] {Physically same proc, \textbf{different interpretation}};
\node(caption) [left=of note] {``\textbf{chain of equivalence}''};
\node(c) [smallnode, right=of note] {C code};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\draw [arrow] (procNM.east) to (c.west);
\end{tikzpicture}

\filbreak
\myTitle{Fundamental Language Concepts}

In Exo-GPU, each statement is executed with a certain \myKeyA{thread collective}, or grouping of threads guaranteed to have uniform control flow, and with a certain \myKeyA{instruction timeline} (``actor kind'' today, but this will probably change).
We statically analyze, for each statement of the program, its \myKeyA{instruction timeline} and \myKeyA{collective unit}; these are defined by the user via async blocks and cuda\_threads loops (``parallel for''), respectively.

\filbreak
The instruction timeline (\myKeyA{instr-tl}) categorizes instructions based on their synchronization requirements (e.g. CPU, in-order ``classic'' CUDA instructions, and various non-ordered asynchronous CUDA instructions, e.g. cp.async, wgmma, TMA).

\filbreak
The \myKeyA{collective unit} describes the thread count and ``shape'' of the concrete \myKeyA{thread collectives} assigned to execute the statement.
For example, the collective unit ``warp'' describes the thread intervals [0,31], [32,63], [64,95],...

\filbreak
\myTitle{Object Code Features}

\mainSub{Async Blocks (Instruction Timeline Control)}

\begin{figure*}[!b]
\input{guide/async_block.0.tex}
\caption{Example async blocks, with sm\_80 cp.async instructions used}
\label{fig:AsyncBlocks}
\end{figure*}

All non-\lighttt{@instr} procs consist of CPU code at the top level.
Exo-GPU defines two kinds of async blocks, which overrides the instr-tl within the block \textbf{(figure \ref{fig:AsyncBlocks})}.

\filbreak
\lighttt{with CudaDeviceFunction(...)}: from CPU code, open a block of in-order CUDA code.
This lowers to a CUDA kernel launch, parameterized with keyword arguments taken from the \lighttt{CudaDeviceFunction} object (e.g. \lighttt{clusterDim}, \lighttt{blockDim}, \lighttt{blocks\_per\_sm}).

\filbreak
\lighttt{with CudaAsync(<instr-tl>)}: within a CUDA device function, open a block of async CUDA code, using instructions with the specified instr-tl (``actor kind'').

\filbreak
The selection of instructions within async blocks is not automated, except simple scalar code.
Each Exo instr declares its instr-tl, and each memory type declares the instr-tl needed for allocations, reads, and writes (e.g. CUDA global memory -- \lighttt{CudaGmemLinear} -- must be allocated from the CPU but can only be accessed from CUDA device code).
The compiler checks each usage matches that of the instr-tl of the current scope.
For example, in \textbf{figure \ref{fig:AsyncBlocks}}, if the \lighttt{Sm80\_cp\_async\_f32} gets moved outside of the \lighttt{CudaAsync} block, the compiler will issue a ``wrong instr-tl'' error.

\minorKey{S/M Equivalence:} We need this feature to be able to reason about whether the correct barriers are used when checking S/M equivalence. For example, a barrier that lowers to \lighttt{\_\_syncthreads();} will be enough to ensure visibility across the CTA for prior in-order CUDA instructions, but not prior \lighttt{cp.async} instructions.

\filbreak
\mainSub{Loop Mode}

\begin{figure*}[!b]
\input{guide/intro_tasks_threads.0.tex}
\caption{Simple vector addition example. We break \lighttt{X} and \lighttt{Y} into chunks of 128 values, each handled by one ``task''.}
\label{fig:intro_tasks_threads}
\end{figure*}

Each for loop has an associated loop mode, conveyed using the syntax

\input{guide/loop_mode_syntax.0.tex}

The loop mode is a compile time object (i.e. a literal Python object created while scheduling) created using the supplied keyword arguments.
In contrast, the \lighttt{iter} variable, and \lighttt{lo, hi} bounds, are runtime values.
We call the \lighttt{iter} variable a \lighttt{loop-mode-name}-iterator (e.g. \lighttt{seq}-iterator, \lighttt{cuda\_threads}-iterator).

\filbreak
Exo already defines \myKeyA{\texttt{seq}} (sequential) and \myKeyA{\texttt{par}} (OpenMP) loop modes.
The new loops modes, \myKeyA{\texttt{cuda\_tasks}} and \myKeyA{\texttt{cuda\_threads}}, form a strict two-level hierarchy within a \lighttt{CudaDeviceFunction} block.

\filbreak
The only child statement of a \lighttt{CudaDeviceFunction} block is a loop nest of 1 or more \lighttt{cuda\_tasks} loops \textbf{(figure \ref{fig:intro_tasks_threads})}.
The loop nest defines a space of ``tasks'' which are assigned round-robin to \myKeyA{top-level collectives} to execute (i.e. we implement persistent kernels).
If \lighttt{clusterDim=1} (default), then the top-level collective is a CTA; otherwise, it's a cluster.

\filbreak
Within the inner-most \lighttt{cuda\_tasks} loops, both \lighttt{seq} and \lighttt{cuda\_threads} loops may appear.
The \lighttt{cuda\_threads} loop modifies the collective unit, to be described in the next section.

\minorKey{S/M Equivalence:} This is the core part of the S/M equivalence model, i.e., that the parallelism effected by the parallel loops doesn't change the program's output compared to sequential execution.

\filbreak
\mainSub{cuda\_threads Loops (Collective Unit Control)}

\input{guide/cuda_threads_syntax.0.tex}

The \lighttt{cuda\_threads} loop takes a \myKeyA{collective unit} argument, and assigns one thread collective to execute each ``iteration'' of the loop.
In the common case, the collective unit of the loop body is the same as the \lighttt{unit} argument passed; the standard library provides \lighttt{cuda\_thread}, \lighttt{cuda\_warp}, \lighttt{cuda\_warpgroup}, and \lighttt{cuda\_cta\_in\_cluster} as collective units.

\filbreak
Unlike some other systems, we do not have a fixed hierarchy of collective units, and the user can define their own.
To expand the collective unit by a factor of \lighttt{N}, use the ``\lighttt{N *}'' prefix , e.g. \lighttt{32 * cuda\_thread} is the same as \lighttt{cuda\_warp}.
More advanced cases (e.g. non-contiguous groups of threads) may be handled by creating a \lighttt{CollUnit} object directly.

\filbreak
Unlike CPU parallel-for loops, the \lighttt{cuda\_threads} loop
\begin{itemize}
  \item Supports assigning multiple threads to execute one iteration (when \lighttt{unit} is not \lighttt{cuda\_thread}).
  \item Cannot spawn new threads, and instead subdivides threads supplied by the thread collective that executes the parallel-for.
    For example, the loop \lighttt{for i in cuda\_threads(0, \redBox{10}, unit=\blueBox{4 * cuda\_thread})} will not compile when placed in a scope with collective unit warp, since \redBox{10} $\times$ \blueBox{4} = 40 exceeds the 32 threads available in a warp.
  \item Requires \lighttt{lo} to be 0 and \lighttt{hi} to be a compile-time constant.
\end{itemize}

See \lighttt{coll\_algebra.pdf} for more detailed discussion (including the non-common case where the actual collective unit is different from the \lighttt{unit} parameter specified).

\filbreak
Use nested \lighttt{cuda\_threads} loops to define a multidimensional iteration space.
The \lighttt{cuda\_threads} loops lower to C++ code that subdivide thread collectives by assigning different iterator values based on thread index:

\input{guide/cuda_threads_cxx.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (cta) [yellownode, text width=20mm, minimum width=20mm, minimum height=40mm] {CTA};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m = 0}; threads [0, 15]};
\node (m7) [Mnode, below=of m2, yshift=-9mm] {\texttt{m = 7}; threads [112, 127]};
\draw [arrow] (cta) -- node[above] {\texttt{for \greenBox{m}}} (m2);
\draw [dotted] (m2) -- (m7);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m7n0) [Nnode, right=of m7, xshift=16mm] {112};
\node (m7n1) [Nnode, right=of m7n0] {113};
\node (m7n2) [Nnode, right=of m7n1] {114};
\node (m7n15) [Nnode, right=of m7n2, xshift=8mm] {127};
\draw [arrow] (m0) -- node[above] {\texttt{for \violetBox{n}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \violetBox{n}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \violetBox{n}}} (m2n0);
\draw [arrow] (m7) -- node[below] {\texttt{for \violetBox{n}}} (m7n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n=0}} (m7n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n=1}} (m7n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n=2}} (m7n2);
\draw [dotted] (m2n15) --node[] {\texttt{n=15}} (m7n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m7n2) -- (m7n15);
\end{tikzpicture}

Finally, similar to the instr-tl checks, Exo-GPU statically checks that scalar reduce/writes are performed using single threads only, and that all instructions are executed with the correct collective unit (e.g. wgmma instructions must be executed with a collective unit equivalent to \lighttt{cuda\_warpgroup}).

\filbreak
\mainSub{Warp Specialization}

We primarily support warp specialization with the ``named warps'' feature.
As an alternative to the \lighttt{blockDim} argument, the \lighttt{CudaDeviceFunction} block supports \lighttt{warp\_config}.
Use this to populate the CTA with warps that have names, and optional register counts (using the PTX instruction \lighttt{setmaxnreg}).
Then use \lighttt{with CudaWarps} to map a subtree of code to execute with the warps named (this changes the collective unit inside the body of the \lighttt{CudaWarps} block).

\input{guide/my_warp_config.0.tex}

\filbreak
We generate completely separate C++ code paths for each warp name.
This way, warps don't pay static costs for code only executed by differently-named warps (particularly crucial for \lighttt{setmaxnreg}).
For example, the above code generates C++ helper functions \lighttt{exo\_deviceTask\_consumer}, \lighttt{exo\_deviceTask\_producer}, and \lighttt{exo\_deviceTask\_unused}.

\filbreak
Additionally, \lighttt{with CudaWarps(lo, hi)} just selects a subset \lighttt{[lo, hi)} of the current thread collective's warps for executing child statements\footnote{This is compatible with named warps, but numbered \lighttt{CudaWarps} blocks must appear as children of named \lighttt{CudaWarps} blocks.} (indices are interpreted relative to the start of the thread collective).
For example, the following selects threads [64, 127] and [192, 255]:

\input{guide/warpgroup_CudaWarps.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (wg1) [widenode] {\texttt{wg = 1}; threads [128, 255]};
\node (wg0) [widenode, above=of wg1] {\texttt{wg = 0}; threads [0, 127]};
\draw [draw=white] (wg1) to node(wg_midpoint)[]{} (wg0);
\node (cta) [normalnode, text width=18mm, minimum width=18mm, minimum height=18mm, left=of wg_midpoint, xshift=-44mm] {CTA};
\draw [arrow] (cta) -- node[above] {\texttt{for wg}} ($(wg_midpoint)-(3.2,0)$);
\node (CudaWarps0) [normalnode, minimum height=8mm, right=of wg0, xshift=30mm] { [64, 127] };
\node (CudaWarps1) [normalnode, minimum height=8mm, right=of wg1, xshift=30mm] { [192, 255] };
\draw [arrow] (wg0) -- node[above] {\texttt{CudaWarps(2, 4)}} (CudaWarps0);
\draw [arrow] (wg1) -- node[above] {\texttt{CudaWarps(2, 4)}} (CudaWarps1);
\end{tikzpicture}

\filbreak
\mainSub{Distributed Memory}

A common pattern in CUDA tensor processing programs is to have a tile of values owned by a large thread collective (e.g., a full CTA), but stored in a memory type that is physically allocated at a finer granularity (e.g. registers, which are allocated per-thread).
For example, each thread may allocate an $8 \times 4$ tile, and the threads in the CTA may be arranged in a $16 \times 32$ grid, causing the full CTA matrix to be $128 \times 128$.
We call this pattern \myKeyA{distributed memory}, and the physically allocated pieces \myKeyA{distributed shards}.

\filbreak
Similar patterns may occur with larger granularity, for example, allocating per-warpgroup shards for wgmma, or per-CTA shards for (cluster) distributed SMEM.
In Exo, this all falls under the umbrella of ``distributed memory''.

\filbreak
In handwritten CUDA C++, the declared size of the allocation (distributed shard) won't match the logical size of the tile\footnote{Template libraries like CuTe may smooth this over.}. In Exo-GPU, we want to
\begin{itemize}
  \item Have tensor allocations declare the logical size of the tile. This prioritizes expressing \textit{dataflow} in the object code, rather than lowering details (crucial for S/M equivalence).
  \filbreak
  \item Support predictable \& user-controllable subdivision of the tile into distributed shards (in contrast to, for example, Triton's automated approach).
\end{itemize}

\filbreak
Exo-GPU meets these goals with \myKeyA{distributed memory deduction}, dividing dimensions into \redBox{distributed dimensions}, to the left, and \yellowBox{non-distributed dimensions}, to the right.
Identically, windowing and indexing expressions divide into distributed indices and non-distributed indices, with distributed indices required to be plain reads of \lighttt{cuda\_threads}-iterators.
The non-distributed dimensions give the size of the distributed shards.
In Exo syntax, the previous example looks like:

\filbreak
\input{guide/simple_dist.0.tex}

\filbreak
Our goal is to have this ``just work'' for common cases, by inferring distributed memory from the memory's usage pattern (indexing expressions), rather than requiring explicit annotations on distributed memory allocations.
The user still has full control over this via their loop structure and scheduling operations like \lighttt{reorder\_dim}.
This guide contains only a summary of the deduction rules; \lighttt{coll\_algebra.py} contains precise documentation.

\filbreak
\mainKey{Rule 1/4, Native Units:} Each memory type has its own \myKeyA{native unit}, which is the collective unit expected for physically allocating the memory type; e.g., \lighttt{CudaRmem} has native unit \lighttt{cuda\_thread}, and \lighttt{CudaSmemLinear} has native unit CTA.
The deduction assigns each distributed shard to be allocated by its own distinct thread collective that matches the native unit.
We enforce this ``one-to-one'' distinctness requirement by deducing a tiling chain.

\begin{figure*}[!b]
\input{guide/simple_dist.1.tex}
\caption{Distributed memory example, with tiling chain annotated}
\label{fig:tiling_chain}
\end{figure*}

\begin{figure*}[!b]
\input{guide/simple_dist_cxx.0.tex}
\caption{Distributed memory example, compiled to CUDA C++}
\label{fig:simple_dist_cxx}
\end{figure*}

\filbreak
\mainKey{Rule 2/4, Tiling Chains:} 
The \myKeyA{tile thread count} of a given scope is the number of threads in the scope's collective unit, \textbf{ignoring} the effects of warp specialization (see \lighttt{coll\_algebra.py} for more precise treatment).
The \myKeyA{tiling operator} for a \lighttt{cuda\_threads}-iterator maps the tile thread count $t_0$ of the loop's parent scope to the tile thread count $t_1$ of the loop body; we denote this as $\mathit{iter}: t_0 \mapsto t_1$ \textbf{(figure \ref{fig:tiling_chain})}.
A valid \myKeyA{tiling chain} is a permutation of \lighttt{cuda\_threads}-iterators such that
\begin{itemize}
  \item let $t_a$ be the tile thread count in the scope of the variable's allocation;
  \item let $t_n$ be the number of threads in the native unit;
  \item the permuted iterators' tiling operators form a chain $t_a \mapsto ... \mapsto t_n$.
\end{itemize}
\filbreak
The permutation is unambiguous, aside from no-op tiling operators $\mathit{iter}: t_0 \mapsto t_1$ with $t_0 = t_1$.

\filbreak
Each time the distributed memory is indexed, we consume indices starting from the left, until the consumed indices form a valid tiling chain (successful deduction), or we conclude that deducing a valid tiling chain is impossible, or until we cannot consume another \lighttt{cuda\_threads}-iterator (the deduction fails; recall that distributed indices must be \lighttt{cuda\_threads}-iterators).
The consumed indices are distributed, and the remaining indices are non-distributed.

\filbreak
\mainKey{Rule 3/4, Consistent Deduction:} Each time a given variable is indexed, we run the distributed memory deduction again, and the result must match that deduced from all other usages of the variable, i.e., the same thread collective is always used for a given distributed shard.

\filbreak
\mainKey{Rule 4/4, Lowering:} The distributed dimensions and indices get stripped away as part of the code lowering process.
The generated C++ contains only non-distributed indices \textbf{(figure \ref{fig:simple_dist_cxx})}.
More importantly for the Exo user, the compiler removes distributed dimensions and indices before calling code generation callbacks for memory types and instructions.
So the authors of externalized stuff can remain completely ignorant of this system.

\begin{figure*}[!b]
\input{guide/why_dist.0.tex}
\caption{Counterexample for S/M equivalence for a hypothetical design where variables are implicitly duplicated across threads. Note: in general we cannot fuse the \lighttt{tid} loops or sink the allocation of \lighttt{x} into the loop scope, since there may be required synchronization or other convergent code between them.}
\label{fig:why_dist}
\end{figure*}

\filbreak
\minorKey{S/M Equivalence:} The distributed dimensions of an allocation must be explicit in Exo object code for S/M equivalence to work.
If we were to adopt the approach that each thread collective has its own implicit copy of a variable, then there would be unintended aliasing when intepreting the program sequentially \textbf{(figure \ref{fig:why_dist})}.

\filbreak
Another reason: although for now we describe each distributed shard as being accessible only by its owner thread collective, we plan to support instructions like warp shuffles and TMA multicast that allow limited exceptions to this rule (respectively, accessing registers of other threads in the same warp, and multicasting to SMEM in different CTAs of the cluster).
This means it's really important that the distributed indices be explicit in Exo object code, so the user can vary them and control the indices used for shuffling.

\filbreak
\mainSub{Synchronization Statements}

\begin{figure*}[b!]
\input{guide/sync_warp_cta.0.tex}
\caption{Synchronizing within a warp, and across warps}
\label{fig:sync_warp_cta}
\end{figure*}

\begin{figure*}[b!]
\input{guide/Sm80_cp_async_simple.0.tex}
\caption{Synchronizing mixed synchronous and asynchronous instructions}
\label{fig:Sm80_cp_async_simple}
\end{figure*}

Unlike everything else in Exo, synchronization statements support a somewhat declarative style, where the user just specifies which sets of threads and what category of instruction effects to synchronize, rather than directly calling wrapped CUDA synchronization instructions.
We still support an imperative programming style (as opposed to automated synchronization, or a dependency graph approach or such), where synchronization statements appear in-line with and get executed in sequence with other imperative Exo code.

\filbreak
\mainKey{Timeline:} Each memory action (read or write) is performed with a certain instr-tl (instruction timeline), a certain usage-tl (usage timeline), and thread(s).
The \myKeyA{instr-tl} and \myKeyA{threads} for a memory action are statically deduced from the instr-tl and collective unit in-scope; the \myKeyA{usage-tl} is programmed per-parameter for wgmma instructions (usage-tl is not relevant for non-wgmma instructions).
See \lighttt{summer\_of\_wgmma.pdf}, ``Timelines and Synchronization'' for more details on usage-tl.

\filbreak
Parameterize synchronization statements with \myKeyA{sync-tl} objects (synchronization timelines), which are sets of $(\text{instr-tl} \times \text{usage-tl})$ pairs.

\filbreak
\mainKey{Fence:} The \lighttt{Fence(first\_tl, second\_tl)} statement expresses a non-split barrier.
Informally, this causes all threads in the executing \myKeyA{thread collective} to wait for all other such threads.
This means that syntactically-identical \lighttt{Fence} statements may lower to different code, depending on the collective unit in scope \textbf{(figure \ref{fig:sync_warp_cta})}.

\filbreak 
The sync-tl parameters filter the effects of the fence \textbf{(figure \ref{fig:Sm80_cp_async_simple})}.
That is, the fence orders memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{first\_tl}.
  \item executed with thread(s) of the fence's executing thread collective
  \item executed before the fence
\end{enumerate}
\filbreak
before memory actions that
\begin{enumerate}
  \item use an $(\text{instr-tl}, \text{usage-tl})$ pair in \lighttt{second\_tl}.
  \item executed with thread(s) of the fence's executing thread collective
  \item executed after the fence
\end{enumerate}

\filbreak
TODO: implement the timeline system (replace actor kinds, actor signatures) and document them all.
Anything you've seen anywhere else is probably really out of date now.

\filbreak
\mainKey{Split Barrier (Arrive, Await):}

\filbreak
\mainKey{Queue Barrier:}

\filbreak
\minorKey{S/M Equivalence:} We need this thin abstraction since checking that the user satisfied the requirements of the abstract machine is much more tractible than trying to model the semantics of, and check correct usage of, raw CUDA synchroniztaion instructions. See the next section for more justification.

\filbreak
\myTitle{Abstract Machine (Synchronization Model)}

\filbreak
\mainSub{User's View}

To the language user, Exo-GPU consists of a
This thin abstraction layer over raw CUDA synchronization enables 

\filbreak
\mainSub{Compiler's View}

To the author of the compiler backend, Exo-GPU exposes a number of features that can, for the most part, be translated 1:1 to CUDA code.
Only synchronization statements require more in-depth handling (for example, initializing mbarriers at startup).

\filbreak
Because of the static instr-tl and collective unit type systems, the compiler is empowered to (and must) diagnose straightforward incorrect usage of Exo-GPU: invalid parallel-for loops, incorrect collective units for instructions (convergence requirements not met), or incorrect instr-tl for memory/instr usage.
Beyond that, checking correct parallelism is outside the scope of the Exo-to-CUDA compiler; we will check correct usage dynamically, or with hypothetical future language extensions.

\filbreak
\mainSub{Our View\footnote{The ``Our View'' section is a rhetorical device borrowed from the OpenGL spec.}}

We view Exo-GPU programs as targetting a \textit{fundamentally sequential} abstract machine, but mechanically translatable to efficient and predictable code that is parallelized for execution on a physical device.
This divides the labor of writing correct programs between the user and the compiler author: ``above the waterline'', language users must guarantee the valid usage requirements of the abstract machine are followed; ``below the waterline'', the compiler author must guarantee that correct programs written against the abstract machine translate into correct CUDA programs.

\filbreak
We anticipate this model will meet the needs of both programming language researchers and performance engineers.
For PL researchers, this model reduces correctness checking to predicates on sequential programs, and abstracts away the minute details of GPU instructions, while still preserving the essential problem of enforcing correct usage of asynchronous instructions.
For performance engineers, this model retains fine-grained control over instruction selection, mapping of work to threads, and synchronization (at least in the tile-and-tensor-oriented domain Exo is traditionally used for).

\filbreak
In the initial implementation, valid usage will be checked by literal simulation of the abstract machine.
Ideally, this model will enable synchronization correctness checks (or programming language safety features) that are more abstract and efficient, which could be built upon existing formalism that, until now, supported only programs targetting physically sequential machines.

\end{document}
