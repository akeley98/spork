%python3 code_to_tex.py guide_correct.py guide && python3 code_to_tex.py guide_wrong.py guide && xelatex spork_guide.tex < /dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Exo GPU -- Spork User Guide}

We're extending Exo with constructs for generating CUDA device code, with the goals of supporting
\begin{itemize}
  \item explicit instruction selection, including support for asynchronous copies and tensor cores (wgmma)
  \filbreak
  \item explicit selection of memory types for variables
  \filbreak
  \item explicit assignment of work to threads (no implied parallelism; Exo's imperative C-style programming is preserved) and explicit synchronization (choice of barriers, mbarriers, commit group)
  \filbreak
  \item codegen to CUDA C++ headers and CPU C code, which will allow us to support mixed CPU/GPU code, and mixed Exo-generated and handwritten CUDA
  \filbreak
  \item ``sequential logic'' for scheduling; that is, Exo's existing analysis continues to use sequential semantics;
    all parallelism features are expressed using existing concepts with ``annotations'' (e.g. parallel-for loops) that can be ignored to analyze the program as a sequential program
  \filbreak
\item S/M equivalence (single-threaded / multi-threaded equivalence); we check that the scheduled program, interpreted sequentially, will give the same results as the scheduled program interpreted as a parallel CUDA program.
    This two phase checking (Exo scheduling with sequential semantics, S/M equivalence checking) proves that the unscheduled sequential program gives the same results as the final parallelized program.
\end{itemize}
\filbreak
In the initial product, we will support checking only for concrete problem sizes, using a per-memory-access simulator.
\filbreak
\begin{tikzpicture}[node distance=8mm]
\sffamily
\node(proc0) [normalnode, text width=2cm, minimum width=2cm] {Original proc};
\node(procNS) [normalnode, right=of proc0, text width=4cm, minimum width=4cm] {Scheduled proc, interpreted \myKeyA{sequentially}};
\node(procNM) [normalnode, right=of procNS, text width=4cm, minimum width=4cm] {Scheduled proc, \myKeyB{parallel} interpretation};
\node(cuda) [normalnode, right=of procNM] {CUDA C++ Header};

\draw [arrow] (proc0) to node(exo)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=4cm, minimum width=4cm, below=of exo, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Existing Exo Rewrites};
\node(sync) [normalnode, text width=5cm, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {S/M equivalence check: ``above the waterline''};
\node(spork) [normalnode, text width=6cm, xshift=1cm, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Backend Compiler: ``below the waterline'' (not proven correct)};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg, xshift=15mm] {Physically same proc, \textbf{different interpretation}};
\node(caption) [left=of note] {``\textbf{chain of equivalence}''};
\node(c) [smallnode, right=of note] {C code};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\draw [arrow] (procNM.east) to (c.west);
\end{tikzpicture}

\filbreak
\myTitle{Fundamental Language Concepts}

In Exo-GPU, each statement is executed with a certain \myKeyA{thread collective}, or grouping of threads guaranteed to have uniform control flow, and with a certain \myKeyA{instruction timeline} (``actor kind'' today, but this will probably change).
We statically analyze, for each statement of the program, its \myKeyA{instruction timeline} and \myKeyA{collective unit}; these are defined by the user via async blocks and cuda\_threads loops (``parallel for''), respectively.

\filbreak
The instruction timeline (\myKeyA{instr-tl}) categorizes instructions based on their synchronization requirements (e.g. CPU, in-order ``classic'' CUDA instructions, and various non-ordered asynchronous CUDA instructions, e.g. cp.async, wgmma, TMA).

\filbreak
The \myKeyA{collective unit} describes the thread count and ``shape'' of the concrete \myKeyA{thread collectives} assigned to execute the statement.
For example, the collective unit ``warp'' describes the thread intervals [0,31], [32,63], [64,95],...

\filbreak
\myTitle{Object Code Features}

\mainSub{Async Blocks (Instruction Timeline Control)}

\begin{figure*}[!b]
\input{guide/async_block.0.tex}
\caption{Example async blocks, with sm\_80 cp.async instructions used}
\label{fig:AsyncBlocks}
\end{figure*}

All procs consist of CPU code at the top level.
Exo-GPU defines two kinds of async blocks, which overrides the instr-tl within the block \textbf{(figure \ref{fig:AsyncBlocks})}.

\filbreak
\lighttt{with CudaDeviceFunction(...)}: from CPU code, open a block of in-order CUDA code.
This lowers to a CUDA kernel launch, parameterized with keyword arguments taken from the \lighttt{CudaDeviceFunction} object (e.g. \lighttt{clusterDim}, \lighttt{blockDim}, \lighttt{blocks\_per\_sm}).

\filbreak
\lighttt{with CudaAsync(<instr-tl>)}: within a CUDA device function, open a block of async CUDA code, using instructions with the specified instr-tl (``actor kind'').

\filbreak
The selection of instructions within async blocks is not automated, except simple scalar code.
Each Exo instr declares its instr-tl, and each memory type declares the instr-tl needed for allocations, reads, and writes (e.g. CUDA global memory -- \lighttt{CudaGmemLinear} -- must be allocated from the CPU but can only be accessed from CUDA device code).
The compiler checks each usage matches that of the instr-tl of the current scope.
For example, in \textbf{figure \ref{fig:AsyncBlocks}}, if the \lighttt{Sm80\_cp\_async\_f32} gets moved outside of the \lighttt{CudaAsync} block, the compiler will issue a ``wrong instr-tl'' error.

\minorKey{S/M Equivalence:} We need this feature to be able to reason about whether the correct barriers are used when checking S/M equivalence. For example, a barrier that lowers to \lighttt{\_\_syncthreads();} will be enough to ensure visibility across the CTA for prior in-order CUDA instructions, but not prior \lighttt{cp.async} instructions.

\filbreak
\mainSub{Loop Mode}

\begin{figure*}[!b]
\input{guide/intro_tasks_threads.0.tex}
\caption{Simple vector addition example. We break \lighttt{X} and \lighttt{Y} into chunks of 128 values, each handled by one ``task''.}
\label{fig:intro_tasks_threads}
\end{figure*}

Each for loop has an associated loop mode, conveyed using the syntax

\input{guide/loop_mode_syntax.0.tex}

The loop mode is a compile time object (i.e. a literal Python object created while scheduling) created using the supplied keyword arguments.
In contrast, the \lighttt{iter} variable, and \lighttt{lo, hi} bounds, are runtime values.

\filbreak
Exo already defines \myKeyA{\texttt{seq}} (sequential) and \myKeyA{\texttt{par}} (OpenMP) loop modes.
The new loops modes, \myKeyA{\texttt{cuda\_tasks}} and \myKeyA{\texttt{cuda\_threads}}, form a strict two-level hierarchy within a \lighttt{CudaDeviceFunction} block.

\filbreak
The only child statement of a \lighttt{CudaDeviceFunction} block is a loop nest of 1 or more \lighttt{cuda\_tasks} loops \textbf{(figure \ref{fig:intro_tasks_threads})}.
The loop nest defines a space of ``tasks'' which are assigned round-robin to \myKeyA{top-level collectives} to execute (i.e. we implement persistent kernels).
If \lighttt{clusterDim=1} (default), then the top-level collective is a CTA; otherwise, it's a cluster.

\filbreak
Within the inner-most \lighttt{cuda\_tasks} loops, both \lighttt{seq} and \lighttt{cuda\_threads} loops may appear.
The \lighttt{cuda\_threads} modifies the collective unit, to be described in the next section.

\minorKey{S/M Equivalence:} This is the core part of the S/M equivalence model, i.e., that the parallelism effected by the parallel loops doesn't change the program's output compared to sequential execution.

\filbreak
\mainSub{cuda\_threads Loops (Collective Unit Control)}

% subdivides, not spawns
% multiple threads per iteration; note about non-exactness of unit
% lowers to i = index expr

\filbreak
\mainSub{Warp Specialization}

\filbreak
\mainSub{Distributed Memory}

\filbreak
\mainSub{Synchronization Statements}

\filbreak
\myTitle{Abstract Machine (Synchronization Model)}

\end{document}
