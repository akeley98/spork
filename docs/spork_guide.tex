%exocc guide_correct.py && python3 code_to_tex.py guide_correct.py guide && python3 code_to_tex.py guide_wrong.py guide && xelatex spork_guide.tex < /dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\begin{document}
\myTitle{Exo GPU -- Spork User Guide}

We're extending Exo with constructs for generating CUDA device code, with the goals of supporting
\begin{itemize}
  \item explicit instruction selection, including support for asynchronous copies and tensor cores (wgmma)
  \filbreak
  \item explicit selection of memory types for variables
  \filbreak
  \item explicit assignment of work to threads (no implied parallelism; Exo's imperative C-style programming is preserved) and explicit synchronization (choice of barriers, mbarriers, commit group)
  \filbreak
  \item codegen to CUDA C++ headers and CPU C code, which will allow us to support mixed CPU/GPU code, and mixed Exo-generated and handwritten CUDA
  \filbreak
  \item ``sequential logic'' for scheduling; that is, Exo's existing analysis continues to use sequential semantics;
    all parallelism features are expressed using existing concepts with ``annotations'' (e.g. parallel-for loops) that can be ignored to analyze the program as a sequential program
  \filbreak
\item S/M equivalence (single-threaded / multi-threaded equivalence); we check that the scheduled program, interpreted sequentially, will give the same results as the scheduled program interpreted as a parallel CUDA program.
    This two phase checking (Exo scheduling with sequential semantics, S/M equivalence checking) proves that the unscheduled sequential program gives the same results as the final parallelized program.
\end{itemize}
\filbreak
In the initial product, we will support checking only for concrete problem sizes, using a simulator that tracks each memory access.
\filbreak
\begin{tikzpicture}[node distance=8mm]
\sffamily
\node(proc0) [normalnode, text width=2cm, minimum width=2cm] {Original proc};
\node(procNS) [normalnode, right=of proc0, text width=4cm, minimum width=4cm] {Scheduled proc, interpreted \myKeyA{sequentially}};
\node(procNM) [normalnode, right=of procNS, text width=4cm, minimum width=4cm] {Scheduled proc, \myKeyB{parallel} interpretation};
\node(cuda) [normalnode, right=of procNM] {CUDA C++ Header};

\draw [arrow] (proc0) to node(exo)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, text width=4cm, minimum width=4cm, below=of exo, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Existing Exo Rewrites};
\node(sync) [normalnode, text width=5cm, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {S/M equivalence check: ``above the waterline''};
\node(spork) [normalnode, text width=6cm, xshift=1cm, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Spork Backend Compiler: ``below the waterline'' (not proven correct)};
\node(note) [above=of SM, draw=redBoxFg, fill=redBoxBg, text=redBoxFg, xshift=15mm] {Physically same proc, \textbf{different interpretation}};
\node(caption) [left=of note] {``\textbf{chain of equivalence}''};
\node(c) [smallnode, right=of note] {C code};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\draw [arrow] (procNM.east) to (c.west);
\end{tikzpicture}

\filbreak
\myTitle{Fundamental Language Concepts}

In Exo-GPU, each statement is executed with a certain \myKeyA{thread collective}, or grouping of threads guaranteed to have uniform control flow, and with a certain \myKeyA{instruction timeline} (``actor kind'' today, but this will probably change).
We statically analyze, for each statement of the program, its \myKeyA{instruction timeline} and \myKeyA{collective unit}; these are defined by the user via async blocks and cuda\_threads loops (``parallel for''), respectively.

\filbreak
The instruction timeline (\myKeyA{instr-tl}) categorizes instructions based on their synchronization requirements (e.g. CPU, in-order ``classic'' CUDA instructions, and various non-ordered asynchronous CUDA instructions, e.g. cp.async, wgmma, TMA).

\filbreak
The \myKeyA{collective unit} describes the thread count and ``shape'' of the concrete \myKeyA{thread collectives} assigned to execute the statement.
For example, the collective unit ``warp'' describes the thread intervals [0,31], [32,63], [64,95],...

\filbreak
\myTitle{Object Code Features}

\mainSub{Async Blocks (Instruction Timeline Control)}

\begin{figure*}[!b]
\input{guide/async_block.0.tex}
\caption{Example async blocks, with sm\_80 cp.async instructions used}
\label{fig:AsyncBlocks}
\end{figure*}

All non-\lighttt{@instr} procs consist of CPU code at the top level.
Exo-GPU defines two kinds of async blocks, which overrides the instr-tl within the block \textbf{(figure \ref{fig:AsyncBlocks})}.

\filbreak
\lighttt{with CudaDeviceFunction(...)}: from CPU code, open a block of in-order CUDA code.
This lowers to a CUDA kernel launch, parameterized with keyword arguments taken from the \lighttt{CudaDeviceFunction} object (e.g. \lighttt{clusterDim}, \lighttt{blockDim}, \lighttt{blocks\_per\_sm}).

\filbreak
\lighttt{with CudaAsync(<instr-tl>)}: within a CUDA device function, open a block of async CUDA code, using instructions with the specified instr-tl (``actor kind'').

\filbreak
The selection of instructions within async blocks is not automated, except simple scalar code.
Each Exo instr declares its instr-tl, and each memory type declares the instr-tl needed for allocations, reads, and writes (e.g. CUDA global memory -- \lighttt{CudaGmemLinear} -- must be allocated from the CPU but can only be accessed from CUDA device code).
The compiler checks each usage matches that of the instr-tl of the current scope.
For example, in \textbf{figure \ref{fig:AsyncBlocks}}, if the \lighttt{Sm80\_cp\_async\_f32} gets moved outside of the \lighttt{CudaAsync} block, the compiler will issue a ``wrong instr-tl'' error.

\minorKey{S/M Equivalence:} We need this feature to be able to reason about whether the correct barriers are used when checking S/M equivalence. For example, a barrier that lowers to \lighttt{\_\_syncthreads();} will be enough to ensure visibility across the CTA for prior in-order CUDA instructions, but not prior \lighttt{cp.async} instructions.

\filbreak
\mainSub{Loop Mode}

\begin{figure*}[!b]
\input{guide/intro_tasks_threads.0.tex}
\caption{Simple vector addition example. We break \lighttt{X} and \lighttt{Y} into chunks of 128 values, each handled by one ``task''.}
\label{fig:intro_tasks_threads}
\end{figure*}

Each for loop has an associated loop mode, conveyed using the syntax

\input{guide/loop_mode_syntax.0.tex}

The loop mode is a compile time object (i.e. a literal Python object created while scheduling) created using the supplied keyword arguments.
In contrast, the \lighttt{iter} variable, and \lighttt{lo, hi} bounds, are runtime values.
We call the \lighttt{iter} variable a <loop-mode-name>-iterator (e.g. seq-iterator, cuda\_threads-iterator).

\filbreak
Exo already defines \myKeyA{\texttt{seq}} (sequential) and \myKeyA{\texttt{par}} (OpenMP) loop modes.
The new loops modes, \myKeyA{\texttt{cuda\_tasks}} and \myKeyA{\texttt{cuda\_threads}}, form a strict two-level hierarchy within a \lighttt{CudaDeviceFunction} block.

\filbreak
The only child statement of a \lighttt{CudaDeviceFunction} block is a loop nest of 1 or more \lighttt{cuda\_tasks} loops \textbf{(figure \ref{fig:intro_tasks_threads})}.
The loop nest defines a space of ``tasks'' which are assigned round-robin to \myKeyA{top-level collectives} to execute (i.e. we implement persistent kernels).
If \lighttt{clusterDim=1} (default), then the top-level collective is a CTA; otherwise, it's a cluster.

\filbreak
Within the inner-most \lighttt{cuda\_tasks} loops, both \lighttt{seq} and \lighttt{cuda\_threads} loops may appear.
The \lighttt{cuda\_threads} modifies the collective unit, to be described in the next section.

\minorKey{S/M Equivalence:} This is the core part of the S/M equivalence model, i.e., that the parallelism effected by the parallel loops doesn't change the program's output compared to sequential execution.

\filbreak
\mainSub{cuda\_threads Loops (Collective Unit Control)}

\input{guide/cuda_threads_syntax.0.tex}

The \lighttt{cuda\_threads} loop takes a \myKeyA{collective unit} argument, and assigns one thread collective to execute each ``iteration'' of the loop.
In the common case, the collective unit of the loop body is the same as the \lighttt{unit} argument passed; the standard library provides \lighttt{cuda\_thread}, \lighttt{cuda\_warp}, \lighttt{cuda\_warpgroup}, or \lighttt{cuda\_cta\_in\_cluster} as collective units.

\filbreak
Unlike some other systems, we do not have a fixed hierarchy of collective units, and the user can define their own.
To expand the collective unit by a factor of \lighttt{N}, use the ``\lighttt{N *}'' prefix , e.g. \lighttt{32 * cuda\_thread} is the same as \lighttt{cuda\_warp}.
More advanced cases (e.g. non-contiguous groups of threads) may be handled by creating a \lighttt{CollUnit} object directly.

\filbreak
Unlike CPU parallel-for loops, the \lighttt{cuda\_threads} loop
\begin{itemize}
  \item Supports assigning multiple threads to execute one iteration (when \lighttt{unit} is not \lighttt{cuda\_thread}).
  \item Cannot spawn new threads, and instead subdivides threads supplied by the thread collective that executes the parallel-for.
    For example, the loop \lighttt{for i in cuda\_threads(0, \redBox{10}, unit=\blueBox{4 * cuda\_thread})} will not compile when placed in a scope with collective unit warp, since \redBox{10} $\times$ \blueBox{4} = 40 exceeds the 32 threads available in a warp.
  \item Requires \lighttt{lo} to be 0 and \lighttt{hi} to be a compile-time constant.
\end{itemize}

See \lighttt{coll\_algebra.pdf} for more detailed discussion (including the non-common case where the actual collective unit is different from the \lighttt{unit} parameter specified).

\filbreak
Use nested \lighttt{cuda\_threads} loops to define a multidimensional iteration space.
The \lighttt{cuda\_threads} loops lower to C++ code that subdivide thread collectives by assigning different iterator values based on thread index:

\input{guide/cuda_threads_cxx.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (cta) [yellownode, text width=20mm, minimum width=20mm, minimum height=40mm] {CTA};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m = 0}; threads [0, 15]};
\node (m7) [Mnode, below=of m2, yshift=-9mm] {\texttt{m = 7}; threads [112, 127]};
\draw [arrow] (cta) -- node[above] {\texttt{for \greenBox{m}}} (m2);
\draw [dotted] (m2) -- (m7);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m7n0) [Nnode, right=of m7, xshift=16mm] {112};
\node (m7n1) [Nnode, right=of m7n0] {113};
\node (m7n2) [Nnode, right=of m7n1] {114};
\node (m7n15) [Nnode, right=of m7n2, xshift=8mm] {127};
\draw [arrow] (m0) -- node[above] {\texttt{for \violetBox{n}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \violetBox{n}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \violetBox{n}}} (m2n0);
\draw [arrow] (m7) -- node[below] {\texttt{for \violetBox{n}}} (m7n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n=0}} (m7n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n=1}} (m7n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n=2}} (m7n2);
\draw [dotted] (m2n15) --node[] {\texttt{n=15}} (m7n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m7n2) -- (m7n15);
\end{tikzpicture}

\filbreak
\mainSub{Warp Specialization}

We primarily support warp specialization with the ``named warps'' feature.
As an alternative to the \lighttt{blockDim} argument, the \lighttt{CudaDeviceFunction} block supports \lighttt{warp\_config}.
Use this to populate the CTA with warps that have names, and optional register counts (using the PTX instruction \lighttt{setmaxnreg}).
Then use \lighttt{with CudaWarps} to map a subtree of code to execute with the warps named.
This changes the collective unit inside the body of the \lighttt{CudaWarps} block.

\input{guide/my_warp_config.0.tex}

\filbreak
We generate completely separate C++ code paths for each warp name.
This way, warps don't pay static costs for code only executed by differently-named warps (particularly crucial for \lighttt{setmaxnreg}).
For example, the above code generates C++ helper functions \lighttt{exo\_deviceTask\_consumer}, \lighttt{exo\_deviceTask\_producer}, and \lighttt{exo\_deviceTask\_unused}.

\filbreak
Additionally, \lighttt{with CudaWarps(lo, hi)} just selects a subset \lighttt{[lo, hi)} of the current thread collective's warps for executing child statements\footnote{This is compatible with named warps, but numbered \lighttt{CudaWarps} blocks must appear as children of named \lighttt{CudaWarps} blocks.} (indices are interpreted relative to the start of the thread collective).
For example, the following selects threads [64, 127] and [192, 255]:

\input{guide/warpgroup_CudaWarps.0.tex}

\begin{tikzpicture}[node distance=2mm]
\sffamily
\node (wg1) [widenode] {\texttt{wg = 1}; threads [128, 255]};
\node (wg0) [widenode, above=of wg1] {\texttt{wg = 0}; threads [0, 127]};
\draw [draw=white] (wg1) to node(wg_midpoint)[]{} (wg0);
\node (cta) [normalnode, text width=18mm, minimum width=18mm, minimum height=18mm, left=of wg_midpoint, xshift=-44mm] {CTA};
\draw [arrow] (cta) -- node[above] {\texttt{for wg}} ($(wg_midpoint)-(3.2,0)$);
\node (CudaWarps0) [normalnode, minimum height=8mm, right=of wg0, xshift=30mm] { [64, 127] };
\node (CudaWarps1) [normalnode, minimum height=8mm, right=of wg1, xshift=30mm] { [192, 255] };
\draw [arrow] (wg0) -- node[above] {\texttt{CudaWarps(2, 4)}} (CudaWarps0);
\draw [arrow] (wg1) -- node[above] {\texttt{CudaWarps(2, 4)}} (CudaWarps1);
\end{tikzpicture}

\filbreak
\mainSub{Distributed Memory}

\filbreak
\mainSub{Synchronization Statements}

\filbreak
\myTitle{Abstract Machine (Synchronization Model)}

\end{document}
