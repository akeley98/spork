\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColor, fill=white]
\tikzstyle{smallsmemnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

{\hfill \LARGE \textbf{\textsf{Project Spork: EXO GPU}}}

\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage

\myTitle{Project Goals}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
\myKey{Exo Today:} Python-embedded USL with ``imperative'' rewrite-based scheduling

\texttt{@proc} decorator captures annotated Python AST; procs translated (almost) 1:1 to CPU C code

Rewrite with proc$\to$proc scheduling functions
\begin{itemize}
  \item Example: loop division (into inner/outer loops)
  \item \myKey{Goal:} rewrite rules for CUDA parallelism
\end{itemize}
Safety Checking
\begin{itemize}
  \item Exo checks that \textit{each} rewrite preserves program behavior.
  \item \myKey{Goal:} check parallelized program matches original intended sequential program's behavior (synchronization check)
\end{itemize}
Custom accelerator instructions and memory types defined by the user (or stdlib)
\begin{itemize}
  \item Behavior declared as Python AST (\texttt{@instr})
  \item \texttt{replace} rewrite rule substitutes matching code blocks with custom instr.
  \item \myKey{Goal:} expand feature to express special CUDA instructions (async copies, wgmma)
\end{itemize}
\end{minipage} %
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Unscheduled Example Program: Na√Øve Code}
\vspace{3mm}
{
\tiny
\begin{verbatim}
@proc
def original_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                  B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for m in seq(0, M):
        for n in seq(0, N):
            accum: f32 @ DRAM
            accum = 0
            for k in seq(0, K):
                accum += A[m, k] * B[k, n]
            C[m, n] = accum
\end{verbatim}
}
\vspace{1cm}
\mySub{Compiled C Code}
\vspace{3mm}
{
\tiny
\begin{verbatim}
void original_gemm(void *ctxt, int_fast32_t M, int_fast32_t N, int_fast32_t K,
                   const float* A, const float* B, float* C ) {
EXO_ASSUME(M % 16 == 0);
EXO_ASSUME(N % 16 == 0);
EXO_ASSUME(K % 8 == 0);
for (int_fast32_t m = 0; m < M; m++) {
  for (int_fast32_t n = 0; n < N; n++) {
    float accum;
    accum = 0.0f;
    for (int_fast32_t k = 0; k < K; k++) {
      accum += A[m * K + k] * B[k * N + n];
    }
    C[m * N + n] = accum;
  }
}
\end{verbatim}
}
\end{minipage} %
\vfill
% Intro to Exo
\newpage
\myTitle{Exo CPU GEMM Example: Original Code}

\begin{minipage}[t]{0.5\textwidth}\codeminipage
Imagine we want to reschedule this matrix multiply function to target a hypothetical CPU matrix tile accelerator that operates on $16 \times 16 \times 8$ tiles ($M \times N \times K$):
\vspace{6mm}
\tiny
\begin{verbatim}
@proc
def original_gemm(M: size, N: size, K: size, A: f32[M,K], B: f32[K,N], C: f32[M,N]):
    # Avoid requiring predication
    assert M % m_tile == 0
    assert N % n_tile == 0
    assert K % k_tile == 0

    for m in seq(0, M):
        for n in seq(0, N):
            accum : f32
            accum = 0
            for k in seq(0, K):
                accum += A[m,k] * B[k,n]
            C[m,n] = accum
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\vspace{3cm}
We specify the custom instruction's behavior and start writing a \texttt{proc}$\to$\texttt{proc} scheduling function \texttt{schedule\_gemm}.
\vspace{6mm}
\tiny
\begin{verbatim}
m_tile = 16
n_tile = 16
k_tile = 8

@instr("example_mma_tile({C_tile_data}, {A_tile_data}, {B_tile_data});")
def example_mma_tile(C_tile: [f32][m_tile,n_tile] @ ExampleAcceleratorTile,
                     A_tile: [f32][m_tile,k_tile] @ ExampleAcceleratorTile,
                     B_tile: [f32][k_tile,n_tile] @ ExampleAcceleratorTile):
    for m in seq(0, m_tile):
        for n in seq(0, n_tile):
            for k in seq(0, k_tile):
                C_tile[m,n] += A_tile[m,k] * B_tile[k,n]


def schedule_gemm(p, new_name):
    p = rename(p, new_name)
    return p  # TODO

exo_cpu_gemm = schedule_gemm(original_gemm, "exo_gpu_gemm")
\end{verbatim}
\end{minipage}
\newpage
\myTitle{Exo CPU GEMM Example: Divide and Reorder Loops}

\begin{minipage}[t]{0.5\textwidth}\codeminipage
\tiny
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    for mo in seq(0, M / 16):
        for mi in seq(0, 16):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
            for no in seq(0, N / 16):
                for ni in seq(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    accum: f32 @ DRAM
                    accum = 0
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
                    for ko in seq(0, K / 8):
                        for ki in seq(0, 8):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                            accum += A[16 * mo + mi, 8 * ko +
                                       ki] * B[8 * ko + ki, 16 * no + ni]
                    C[16 * mo + mi, 16 * no + ni] = accum
\end{verbatim}
\vspace{5mm}
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
        for no in seq(0, N / 16):
            for mi in seq(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                for ni in seq(0, 16):
                    accum: f32 @ DRAM
                    accum = 0
                    for ko in seq(0, K / 8):
                        for ki in seq(0, 8):
                            accum += A[16 * mo + mi, 8 * ko +
                                       ki] * B[8 * ko + ki, 16 * no + ni]
                    C[16 * mo + mi, 16 * no + ni] = accum
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\codeminipage
The rescheduling divides the $M$, $N$, and $K$ loops into outer and inner loops, with the inner loops matching the custom instruction's tile dimensions.
We then move the \texttt{no} (n-outer) loop outwards.
Eventually the \texttt{ko} (k-outer) loop should move too, but we can't do that yet.
\vspace{1cm}
\tiny
\begin{verbatim}
def schedule_gemm(p, new_name):
    # ...
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = divide_loop(p, "m", m_tile, ("mo", "mi"), perfect = True)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    p = divide_loop(p, "n", n_tile, ("no", "ni"), perfect = True)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
    p = divide_loop(p, "k", k_tile, ("ko", "ki"), perfect = True)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
    p = reorder_loops(p, "mi no")
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    # ...
\end{verbatim}
\end{minipage}
\newpage
\myTitle{Exo CPU GEMM Example: Expand and Lift Accumulator}

\begin{minipage}[t]{0.6\textwidth}\codeminipage
\tiny
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
        for no in seq(0, N / 16):
            for mi in seq(0, 16):
                for ni in seq(0, 16):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                    accum: f32[16, 16] @ DRAM   # <<< c_accum_alloc
                    accum[mi, ni] = 0
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    for ko in seq(0, K / 8):
                        for ki in seq(0, 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                            accum[mi, ni] += \
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                                A[16 * mo + mi, 8 * ko + ki] * B[8 * ko + ki, 16 * no + ni]
                    C[16 * mo + mi, 16 * no + ni] = \
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                        accum[mi, ni]
\end{verbatim}
\end{mdframed}
\vspace{5mm}
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
        for no in seq(0, N / 16):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
            accum: f32[16, 16] @ DRAM  # <<< c_accum_alloc
            for mi in seq(0, 16):
                for ni in seq(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    accum[mi, ni] = 0
\end{verbatim}

\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
                    for ko in seq(0, K / 8):  # Still in the wrong place
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                        for ki in seq(0, 8):
                            accum[mi, ni] += \
                                A[16 * mo + mi, 8 * ko + ki] * B[8 * ko + ki, 16 * no + ni]
                    C[16 * mo + mi, 16 * no + ni] = \
                        accum[mi, ni]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}\codeminipage
We now need to expand \texttt{accum} into a $16 \times 16$ tile, indexed by \texttt{mi,ni}.
This prepares the accumulator to be used as the destination operand of the hardware accelerator.
\vspace{6mm}
{
\tiny
\begin{verbatim}
def schedule_gemm(p, new_name):
    # ...
    c_accum_alloc = p.find("accum : f32")  # Cursor
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = expand_dim(p, c_accum_alloc, n_tile, 'ni')
    p = expand_dim(p, c_accum_alloc, m_tile, 'mi')
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
    p = lift_alloc(p, c_accum_alloc, n_lifts = 2)
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    # ...
\end{verbatim}
}
\vspace{6mm}
We further hoist \texttt{accum}'s allocation out of the inner loops.
This will be needed later when we fix the \violetBox{\texttt{ko}} loop being in the wrong place.
\end{minipage}

\newpage
\myTitle{Exo CPU GEMM Example: Loop Fission}

\begin{minipage}[t]{0.5\textwidth}\codeminipage
\tiny
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
        for no in seq(0, N / 16):
            accum: f32[16, 16] @ DRAM
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    accum[mi, ni] = 0
                    # (gap cursor) c_accum_zero.after()
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            for ko in seq(0, K / 8):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                for mi in seq(0, 16):
                    for ni in seq(0, 16):
                        for ki in seq(0, 8):
                            accum[mi,
                                  ni] += A[16 * mo + mi, 8 * ko +
                                           ki] * B[8 * ko + ki, 16 * no + ni]
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    # (gap cursor) c_accum_export.before()
                    C[16 * mo + mi, 16 * no + ni] = accum[mi, ni]
\end{verbatim}
\end{mdframed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\codeminipage
We now fission the inner loops (\texttt{mi,ni}) before and after the \texttt{+=} operation.
This factors out the \blueBox{loop nest (blue)} for later targetting with the custom MMA instruction.

\vspace{6mm}
{\tiny
\begin{verbatim}
def schedule_gemm(p, new_name):
    # ...
    c_accum_zero = p.find("accum = 0")
    c_accum_export = p.find("_ = accum")
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = fission(p, c_accum_zero.after(), n_lifts = 2)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    p = fission(p, c_accum_export.before(), n_lifts = 2)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
    p = reorder_loops(p, "ni ko")
    p = reorder_loops(p, "mi ko")
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    # ...
\end{verbatim}
}
\vspace{6mm}
We now also move out the \violetBox{\texttt{ko}} loop to its proper place outside the \texttt{mi},\texttt{ni} loops.
\end{minipage}
\newpage
\myTitle{Exo CPU GEMM Example: Stage Memory}

\begin{minipage}[t]{0.4\textwidth}\codeminipage
\tiny
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
        for no in seq(0, N / 16):
            accum: f32[16, 16] @ DRAM
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    accum[mi, ni] = 0
            for ko in seq(0, K / 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                A_tile: f32[16, 8] @ DRAM
                for i0 in seq(0, 16):
                    for i1 in seq(0, 8):
                        A_tile[i0, i1] = A[i0 + 16 * mo, i1 + 8 * ko]
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                B_tile: f32[8, 16] @ DRAM
                for i0 in seq(0, 8):
                    for i1 in seq(0, 16):
                        B_tile[i0, i1] = B[i0 + 8 * ko, i1 + 16 * no]
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                for mi in seq(0, 16):         # <<< c_tile_reduce
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    for ni in seq(0, 16):     # <<< parent().parent()
                        for ki in seq(0, 8):  # <<< parent()
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                            accum[mi, ni] += A_tile[mi, ki] \
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                                             * B_tile[ki, ni]
\end{verbatim}
\end{mdframed}
\begin{verbatim}
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    C[mi + 16 * mo, ni + 16 * no] = accum[mi, ni]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.6\textwidth}\codeminipage
We now construct a \blueBox{``cursor''} to the root of the future location for the matrix tile instruction,
  and ask Exo to stage $16 \times 8$ and $8 \times 16$ tiles of the input matrices (\texttt{A\_tile}, \texttt{B\_tile}).
This
\begin{itemize}
  \item Causes \texttt{A\_tile} and \texttt{B\_tile} to be prepared \myKey{outside} the scope pointed-to by the cursor.
  \item Causes \texttt{A\_tile} and \texttt{B\_tile} to be substituted (with updated indexing) for uses of \texttt{A} and \texttt{B} \myKey{inside} the scope of the cursor.
\end{itemize}
\vspace{6mm}
{\tiny
\begin{verbatim}
def schedule_gemm(p, new_name):
    # ...
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
    c_tile_reduce = p.find("accum += _").parent().parent().parent()
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = stage_mem(p, c_tile_reduce,
                  f"A[mo*{m_tile}:(mo+1)*{m_tile}, ko*{k_tile}:(ko+1)*{k_tile}]", "A_tile")
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    p = stage_mem(p, c_tile_reduce,
                  f"B[ko*{k_tile}:(ko+1)*{k_tile}, no*{n_tile}:(no+1)*{n_tile}]", "B_tile")
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    p = simplify(p)
    # ...
\end{verbatim}
}
\end{minipage}
\newpage
\myTitle{Exo CPU GEMM Example: Custom Accelerator}

\begin{minipage}[t]{0.5\textwidth}\codeminipage
\tiny
\begin{verbatim}
def exo_cpu_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                 B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in seq(0, M / 16):
        for no in seq(0, N / 16):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
            accum: f32[16, 16] @ ExampleAcceleratorTile
\end{verbatim}
\end{mdframed}
\begin{verbatim}
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    accum[mi, ni] = 0
            for ko in seq(0, K / 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                A_tile: f32[16, 8] @ ExampleAcceleratorTile
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                for i0 in seq(0, 16):
                    for i1 in seq(0, 8):
                        A_tile[i0, i1] = A[i0 + 16 * mo, i1 + 8 * ko]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                B_tile: f32[8, 16] @ ExampleAcceleratorTile
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                for i0 in seq(0, 8):
                    for i1 in seq(0, 16):
                        B_tile[i0, i1] = B[i0 + 8 * ko, i1 + 16 * no]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                example_mma_tile(accum[0:16, 0:16], A_tile[0:16, 0:8],
                                 B_tile[0:8, 0:16])
\end{verbatim}
\end{mdframed}
\begin{verbatim}
            for mi in seq(0, 16):
                for ni in seq(0, 16):
                    C[mi + 16 * mo, ni + 16 * no] = accum[mi, ni]
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\codeminipage
Finally, we update the memory types for the tiles.
This makes it possible to substitute the custom instruction for the \texttt{mi}, \texttt{ni}, \texttt{ki} loop nest.
\vspace{6mm}
{\tiny
\begin{verbatim}
@instr("example_mma_tile({C_tile_data}, {A_tile_data}, {B_tile_data});")
def example_mma_tile(
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
                     C_tile: [f32][m_tile,n_tile] @ ExampleAcceleratorTile,
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                     A_tile: [f32][m_tile,k_tile] @ ExampleAcceleratorTile,
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                     B_tile: [f32][k_tile,n_tile] @ ExampleAcceleratorTile):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    for m in seq(0, m_tile):
        for n in seq(0, n_tile):
            for k in seq(0, k_tile):
                C_tile[m,n] += A_tile[m,k] * B_tile[k,n]

def schedule_gemm(p, new_name):
    # ...
    c_accum_alloc = p.find("accum : f32")  # Cursor
    # ...
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
    p = set_memory(p, c_accum_alloc, ExampleAcceleratorTile)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = set_memory(p, "A_tile", ExampleAcceleratorTile)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    p = set_memory(p, "B_tile", ExampleAcceleratorTile)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
    p = replace(p, c_tile_reduce, example_mma_tile)
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    return p
\end{verbatim}
}
\vspace{6mm}
A real accelerator would likely also need custom load and store instructions for \texttt{ExampleAcceleratorTile}, but we skip this for the example.
\end{minipage}
\newpage
\myTitle{Basic CUDA Features}

\begin{minipage}[t]{0.5\textwidth}\fixminipage

Core ``unit of parallelism'': cooperative thread array
\begin{itemize}
  \item a.k.a. CTA or ``block''
  \item \texttt{blockDim}-many threads (dynamic value)
  \item Think of a block as a ``lane'' consisting of sub-lanes that can split and coalesce at will
  \item ``lane units'' (my vocabulary)
  \begin{itemize}
      \item block: \texttt{blockDim} threads
      \item warpgroup: 128 threads (new in H100)
      \item warp: 32 threads
      \item thread: 1 thread
  \end{itemize}
  \item Different operations require different lane units (i.e. must synchronize threads of 1 lane unit)
  \item \myKeyB{Frequent} communication \myKeyB{within} blocks
\end{itemize}

CPU launches ``grid'' of \texttt{gridDim}-many blocks
\begin{itemize}
  \item \texttt{gridDim} also dynamically specified
  \item \myKey{Minimal} communication \myKey{between} blocks
  \item Limited cooperation with atomics possible
  \item Also clusters, cooperative grid (not discussed)
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\myKey{GMEM} (Global Memory): \myKey{``slow''}
\begin{itemize}
  \item any thread in grid may access
\end{itemize}
\myKeyB{SMEM} (Shared Memory)
\begin{itemize}
  \item per-CTA memory (L1 cache carveout)
  \item \myKeyB{``fast''}, but still $\approx$10$\times$ slower than math
\end{itemize}
\textbf{RMEM} (Register ``Memory'') -- 255 per thread

\begin{tikzpicture}[node distance=5mm]
\node (grid) [normalnode] {grid};
\node (gmem) [gmemnode, right=of grid, xshift=4mm] {\myKey{GMEM}};
\draw[line] (grid) -- (gmem);

\node (block0) [smallnode, fill=violetBoxBg, below=of grid, xshift=-1cm, yshift=-16mm] {block};
\node (block1) [smallnode, right=of block0, xshift=0.6cm] {block};
\node (block2) [smallnode, right=of block1, xshift=2cm] {block};
\draw[arrow] (grid.south) to[out=270,in=0] (block0.east);
\draw[arrow] (grid.south) to[out=300,in=180] (block1.west);
\draw[arrow] (grid.south) to[out=330,in=180] ($(block2.west)+(0,0.2)$);
\draw[dotted] ($(block0.east) - (0, 0.3)$) to ($(block1.west) - (0, 0.3)$);
\draw[dotted] ($(block1.east) - (0, 0.3)$) to node[]{gridDim} ($(block2.west) - (0, 0.3)$);

\node (thread0) [smallnode, fill=violetBoxBg, below=of block0, yshift=-4mm] {thread};
\node (thread1) [smallnode, fill=violetBoxBg, right=of thread0] {thread};
\node (thread2) [smallnode, fill=violetBoxBg, right=of thread1, xshift=2cm] {thread};
\node (rmem0) [smallnode, below=of thread0] {\textbf{RMEM}};
\node (rmem1) [smallnode, below=of thread1] {\textbf{RMEM}};
\node (rmem2) [smallnode, below=of thread2] {\textbf{RMEM}};
\draw[line] (thread0) -- (rmem0);
\draw[line] (thread1) -- (rmem1);
\draw[line] (thread2) -- (rmem2);

\node (smem0) [smallsmemnode, above=of block0] {\myKeyB{SMEM}};
\node (smem1) [smallsmemnode, above=of block1] {\myKeyB{SMEM}};
\node (smem2) [smallsmemnode, above=of block2] {\myKeyB{SMEM}};
\draw[line] (block0) -- (smem0);
\draw[line] (block1) -- (smem1);
\draw[line] (block2) -- (smem2);

\draw[arrow] (block0.south) to [out=270,in=90] (thread0.north);
\draw[arrow] (block0.south) to [out=300,in=110] (thread1.north);
\draw[arrow] (block0.south) to [out=330,in=130] (thread2.north);
\draw[dotted] ($(thread0.east) - (0, 0.3)$) to ($(thread1.west) - (0, 0.3)$);
\draw[dotted] ($(thread1.east) - (0, 0.3)$) to node[]{blockDim} ($(thread2.west) - (0, 0.3)$);
\end{tikzpicture}
\end{minipage}

\newpage
\myTitle{Basic Exo Features for GPU}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Parallel Loop}

Sequential loop \texttt{for iter in seq(lo, hi)} means ``schedule loop iterations lo ... hi-1 through time''.

Parallel loop: \texttt{for iter in cuda\_\emphtt{lane}(lo, hi)} means ``schedule each loop iteration on a different \emphtt{lane}''
\begin{itemize}
  \item \emphtt{lane} = \texttt{threads}, \texttt{warps}, \texttt{warpgroups}, \texttt{blocks}
  \item May be multidimensional (distribute N-dimensional work along linear list of threads/warps/warpgroups/blocks)
  \item Opens a ``\emphtt{lane} scope''; child statements executed by 1 \emphtt{lane}
  \item Some lanes require additional configuration, e.g. \texttt{blockDim}.
\end{itemize}

Vast majority of statements can only appear at thread scope; main exceptions:
\begin{itemize}
  \item Synchronization statements
  \item Accelerator instructions
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Fence Synchronization}

Fence is my generalized terminology (not cuda).

\texttt{Fence(\_,\_)} statement at \emphtt{lane} scope means ``all threads in the executing \emphtt{lane} wait for all other threads in the executing \emphtt{lane} to arrive''.
\begin{itemize}
  \item Ignore mystery arguments for now!
\end{itemize}

\vspace{2mm}
{\tiny
\begin{verbatim}
for io in cuda_blocks(..., blockDim = 256):
    Fence(...)  # All threads in block wait for each other
    for ii in cuda_warps(0, 8):
        Fence(...)  # Only threads within a single warp wait for each other

\end{verbatim}
}
\vspace{6mm}

\mySub{Distributed Memory}

Sometimes storage for a single logical tile needs to be split among multiple lanes, e.g.
\begin{itemize}
  \item Distributed between threads' registers\\(see next gemm example)
  \item Fromats for warp / warpgroup matrix accelerator (matrix ``fragments'' distributed between 32/128 threads)
  \item Cluster-distributed shared memory
\end{itemize}

Need to flesh out this idea!
\end{minipage}
\newpage
\myTitle{Exo CUDA GEMM}

\begin{minipage}[t]{0.5\textwidth}\codeminipage
(Adapting previous CPU gemm example, prior to substituting memory types and custom instruction)
\vspace{6mm}
{\tiny
\begin{verbatim}
def exo_cuda_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                  B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=redBoxBg]
\color{redBoxFg}
\begin{verbatim}
    for mo in cuda_blocks(0, M / 16, blockDim=256):
        for no in cuda_blocks(0, N / 16, blockDim=None):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
            # "distributed" storage among threads
            accum: f32[16, 16] @ CudaRegisters              # <<< c_accum_alloc
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
            for mi in cuda_threads(0, 16):
                for ni in cuda_threads(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    accum[mi, ni] = 0
            for ko in seq(0, K / 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                A_tile: f32[16, 8] @ CudaShared
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                for i0 in cuda_threads(0, 16):
                    for i1 in cuda_threads(0, 8):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                        A_tile[i0, i1] = A[i0 + 16 * mo, i1 + 8 * ko]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                B_tile: f32[8, 16] @ CudaShared
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                for i0 in cuda_threads(0, 8):
                    for i1 in cuda_threads(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                        B_tile[i0, i1] = B[i0 + 8 * ko, i1 + 16 * no]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                # Ignore weird syntax for now!
                Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
                for mi in cuda_threads(0, 16):              # <<< c_tile_reduce
                    for ni in cuda_threads(0, 16):
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                        for ki in seq(0, 8):
                            accum[mi, ni] += A_tile[mi, ki] * B_tile[ki, ni]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
            for mi in cuda_threads(0, 16):
                for ni in cuda_threads(0, 16):
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=redBoxBg]
\color{redBoxFg}
\begin{verbatim}
                    # Embarassing parallelism over output tiles
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                    C[mi + 16 * mo, ni + 16 * no] = accum[mi, ni]
\end{verbatim}
}
\vspace{6mm}
Disclaimer: This example is really not that efficient.
We can get better performance by assigning more than 1 value to compute per thread.
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\codeminipage
\begin{itemize}
  \item Each \redBox{block} assigned one output tile of \texttt{C}
  \item Arithmetic instructions executed with lane unit \blueBox{cuda\_thread}
  \item \greenBox{Fence} (syncthreads) outside \blueBox{cuda\_threads} loops, executed with lane unit \redBox{cuda\_block}
  \item Note \texttt{ko}, \texttt{ki} loops stayed sequential due to serial dependence along $k$ dimension.
\end{itemize}
\vspace{6mm}
{\tiny
\begin{verbatim}
def schedule_gemm(p, new_name, use_cuda):
    # ...
    c_accum_alloc = p.find("accum : f32")  # Cursor
    # ...
    c_tile_reduce = p.find("accum += _").parent().parent().parent()
\end{verbatim}
\begin{verbatim}
    # ...

\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
    p = set_memory(p, c_accum_alloc, CudaRegisters)
    p = set_memory(p, "A_tile", CudaShared)
    p = set_memory(p, "B_tile", CudaShared)
\end{verbatim}
\end{mdframed}
\begin{verbatim}

    # "x #n" means n-th loop with x as iteration variable ... can be improved
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=redBoxBg]
\color{redBoxFg}
\begin{verbatim}
    p = set_loop_mode(p, "mo", exo.loop_modes.CudaBlocks(m_tile * n_tile))
    p = set_loop_mode(p, "no", exo.loop_modes.CudaBlocks())
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=blueBoxBg]
\color{blueBoxFg}
\begin{verbatim}
    p = set_loop_mode(p, "i0 #0", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "i1 #0", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "i0 #1", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "i1 #1", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "mi #0", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "mi #1", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "mi #2", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "ni #0", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "ni #1", exo.loop_modes.cuda_threads)
    p = set_loop_mode(p, "ni #2", exo.loop_modes.cuda_threads)
\end{verbatim}
\end{mdframed}
\begin{verbatim}

\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
    p = insert_fence(p, c_tile_reduce.before(), exo.sync_types.cuda_syncthreads)
    p = insert_fence(p, c_tile_reduce.after(), exo.sync_types.cuda_syncthreads)
\end{verbatim}
\end{mdframed}
\begin{verbatim}
    # ...
\end{verbatim}
}
\end{minipage}

\newpage
\myTitle{Contrast with prior work: Triton}

\begin{minipage}[t]{0.7\textwidth}\fixminipage
Triton is another Python AST $\to$ CUDA language

In my terminology, the ``lane unit'' for a Triton function is a block.
\begin{itemize}
  \item One block executes one ``call'' of a Triton function.
\end{itemize}

Triton's responsibility: \textbf{automatically} distribute the work \myKeyB{within} one block to sub-block lanes (mostly threads) and automatically synchronize.
\begin{itemize}
  \item Recall: synchronization within a block is \myKeyB{frequent}
  \item Minimal control over memory movement, register/SMEM allocation
  \item Honestly, not a bad model though
\end{itemize}

User responsibility: manually distribute work \myKey{between} blocks.
\begin{itemize}
  \item Recall: synchronization between blocks is \myKey{minimal}
  \item Often one output tile assigned per block
\end{itemize}

With Exo, we're trying to build something much more \textbf{imperative}
\begin{itemize}
  \item Control (and predict!) memory and register pressure
  \item Fine grained control over each level of CUDA thread hierarchy
  \item Explicit use of accelerator instructions
\end{itemize}
\end{minipage}

\vspace{6mm}
\hfill(Also, Triton generates IR and Exo GPU will generate C and CUDA C++)

\newpage
\myTitle{Synchronization Correctness} \hfill \myKeyB{(This wall of text is important)}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
Currently, correctness in Exo mostly relies on each rewrite being proved correct (behavior preserving):
\begin{itemize}
  \item $proc_0 \to proc_1 \to proc_2 \to ... \to proc_N$
  \item Prove each $proc_i \to proc_{i+1}$ rewrite is correct
\end{itemize}
This won't work for parallelism.
With the current model, all intermediate procs must be correct, which is infeasible for a partially parallelized program.

Also, proof methods for parallel programs are very limited.
What do we do?
\begin{itemize}
  \item Isaac Newton Solution: invent new mathematics needed (too hard for my brain)
  \item Me Solution: reduce to existing mathematics
\end{itemize}
\myKey{The trick:} Each parallelized Exo proc has a single-threaded program hiding inside it ... just ignore the parallelization.
\begin{itemize}
  \item Rewrite rules will do this, so all existing Exo rewrite rules don't need changing.
  \item Rewrites \texttt{set\_loop\_mode} (parallelize loop) and \texttt{insert\_fence} accepted without proof.
  \item Defer synchronization checking to the end, when lowering.
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Multi-threaded semantics \textbf{(M-semantics)}}
\begin{itemize}
  \item Interpret parallel \texttt{for \_ in cuda\_\{lane\}(...)} loops as described before
  \item Perform synchronization when ordered to
  \item Does it work? Who knows.
\end{itemize}
\mySub{Single-threaded semantics \textbf{(S-semantics)}}
\begin{itemize}
  \item Ignore parallelism: interpret all parallel loops as sequential
  \item Ignore synchronization
  \item Does it work? We already proved it does, since the rewrites assume single-threaded semantics!
\end{itemize}
We prove that a parallelized Exo proc interpreted with M-semantics is \myKey{equivalent to \textit{itself}} interpreted with S-semantics.
\begin{itemize}
  \item Analyze the M-program as a \textit{sequential} program where we track metadata on the visibility of each variable to each thread.
  \item We only need to prove statements for a sequential program: no need for new math!
\end{itemize}
\end{minipage}
\newpage
\myTitle{Synchronization Environment}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
Usually when interpreting a single-threaded program, we track the values of each variable in the environment $\Sigma$

For our purposes, we need a ``synchronization environment'' $\Sigma^S$ parallel to $\Sigma$

We track for each variable in $\Sigma^S$
\begin{itemize}
  \item one ``write visibility set''
  \item multiple ``read visibility sets''
\end{itemize}
Each visibility set is a set of linearized thread IDs
\begin{itemize}
  \item linearized: $blockIdx \times blockDim + threadIdx$
\end{itemize}
Informally, a visibility set describes, at a certain program point, the set of threads that can deterministically ``observe'' that a prior write or read has happened.

Note there are multiple read visibility sets since it's valid to have multiple concurrent reads outstanding.

This will get \myKey{more complicated} later to model not-yet-described CUDA features.
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\textbf{With $T$ being the executing thread ID:}

\mySub{Behavior on Read} (checking to be described)

Add a new read visibility set $\{T\}$.
Note, a ``read'' is not read-only on $\Sigma^S$, unlike $\Sigma$.

\mySub{Behavior on Write}

Discard all visibility sets (read and write) and set the current write visibility set to $\{T\}$

\mySub{Behavior on Synchronization}

Only way that visibility sets grow

Each synchronization statement defines a ``first visibility set'' $V_1$ and ``second visibility set'' $V_2$;
informally, the set of threads that ``signal'' (arrive), and the set of threads that await.
\begin{itemize}
  \item For fences, $V_1 = V_2$
  \item Example, if $blockDim=512$ and $blockIdx=2$,\\a \texttt{\_\_syncthreads()} has\\$V_1 = V_2 = \{1024, 1025, 1026, ... , 1535 \}$.
\end{itemize}

For each visibility set $Q$ of all variables in $\Sigma^S$
\begin{itemize}
  \item Check if $Q \cap V_1$ is nonempty
  \item If so, $Q \leftarrow Q \cup V_2$
\end{itemize}
\end{minipage}

\newpage
\myTitle{Synchronization Checking}

\begin{minipage}[t]{0.7\textwidth}\fixminipage
With $T$ being the executing thread ID, and all visibility sets $W$, $R$ being for the variable being read/written:

\mySub{Checking on Read}

\myKey{RAW} check: $T \in W$, where $W$ is the write visibility set.

\mySub{Checking on Write}

Prior to overwriting the visibility sets, perform checks.

\myKey{WAW} check: $T \in W$, where $W$ is the write visibility set.

\myKey{WAR} check: For each read visibility set $R$, check $T \in R$.

\vspace{10mm}

\mySub{\textbf{IMPORTANT:}} $R$ and $W$ are \textbf{NOT} ``allowed to read'' and ``allowed to write'' sets.
It's more complicated than that.

\vspace{10mm}

\textit{Deeper side note, not for live presentation: \myKey{WAR} checking is a ``conjunction of disjunctions''; with the disjunction being how each read visibility set $R$ gets independently augmented by synchronization ($R \leftarrow R \cup V_2$), and the conjunction being $\forall R, T \in R$.
This is not really simplifiable to a single ``allowed to write'' set.}
\end{minipage}

\newpage
\myTitle{Incorrect GPU GEMM example}

\begin{minipage}[t]{0.4\textwidth}\codeminipage
Let's \redBox{delete} a fence from the previous CUDA gemm example and see what happens!
\vspace{6mm}
\tiny
\begin{verbatim}
def exo_cuda_gemm(M: size, N: size, K: size, A: f32[M, K] @ DRAM,
                  B: f32[K, N] @ DRAM, C: f32[M, N] @ DRAM):
    assert M % 16 == 0
    assert N % 16 == 0
    assert K % 8 == 0
    for mo in cuda_blocks(0, M / 16, blockDim=256):
        for no in cuda_blocks(0, N / 16, blockDim=None):
            accum: f32[16, 16] @ CudaRegisters
            for mi in cuda_threads(0, 16):
                for ni in cuda_threads(0, 16):
                    accum[mi, ni] = 0
            for ko in seq(0, K / 8):
                A_tile: f32[16, 8] @ CudaShared
                for i0 in cuda_threads(0, 16):
                    for i1 in cuda_threads(0, 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=yellowBoxBg]
\color{yellowBoxFg}
\begin{verbatim}
                        A_tile[i0, i1] = A[i0 + 16 * mo, i1 + 8 * ko]
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                B_tile: f32[8, 16] @ CudaShared
                for i0 in cuda_threads(0, 8):
                    for i1 in cuda_threads(0, 16):
                        B_tile[i0, i1] = B[i0 + 8 * ko, i1 + 16 * no]
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=greenBoxBg]
\color{greenBoxFg}
\begin{verbatim}
                Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}
\begin{verbatim}
                for mi in cuda_threads(0, 16):
                    for ni in cuda_threads(0, 16):
                        for ki in seq(0, 8):
\end{verbatim}
\begin{mdframed}[style=MyFrame, backgroundcolor=violetBoxBg]
\color{violetBoxFg}
\begin{verbatim}
                            accum[mi, ni] += A_tile[mi, ki] \
                                             * B_tile[ki, ni]
\end{verbatim}
\end{mdframed}
\begin{mdframed}[style=MyFrame, backgroundcolor=redBoxBg]
\color{redBoxFg}
\begin{verbatim}
                # Fence(cuda_sync, cuda_generic)
\end{verbatim}
\end{mdframed}
\begin{verbatim}
            for mi in cuda_threads(0, 16):
                for ni in cuda_threads(0, 16):
                    C[mi + 16 * mo, ni + 16 * no] = accum[mi, ni]
\end{verbatim}

\end{minipage}
\hfill
\begin{minipage}[t]{0.6\textwidth}\codeminipage
Example: track status of $A\_tile[0,0]$, assigned to thread $T = 0$ (assuming we are investigating block 0, assigned to $mo = 0, no = 0$)

\makebox[\textwidth][c]{
\begin{tabular}{l l}
\rowcolor{yellowBoxBg} $\Sigma_{ko}$ & $0$ \\
\rowcolor{yellowBoxBg} $\Sigma_{A\_tile[0,0]}$ & $A[0,0]$ \\
\rowcolor{yellowBoxBg} $T$ & $0$ \\
\rowcolor{yellowBoxBg} $\Sigma^S_{A\_tile[0,0]}$ & $W = \{0\}$\\
\rowcolor{greenBoxBg} $\Sigma_{ko}$ & $0$ \\
\rowcolor{greenBoxBg} $\Sigma_{A\_tile[0,0]}$ & $A[0,0]$ \\
\rowcolor{greenBoxBg} $V_1 = V_2 =$ & $\{0, 1, 2, ... , 255\}$ \\
\rowcolor{greenBoxBg} $\Sigma^S_{A\_tile[0,0]}$ & $W = \{0, 1, 2, ... , 255\}$\\
\rowcolor{violetBoxBg} $\Sigma_{ko}$ & $0$ \\
\rowcolor{violetBoxBg} $\Sigma_{A\_tile[0,0]}$ & $A[0,0]$ \\
\rowcolor{violetBoxBg} $T$ & $0, 16, 32, ... , 240$  // All threads with \texttt{mi = 0}\\
\rowcolor{blueBoxBg} \color{blueBoxFg} checking: & \color{blueBoxFg} $T \in W$ OK, $0, 16, 32, ... \in \{ 0, 1, ..., 255 \}$ \\
\rowcolor{violetBoxBg} $\Sigma^S_{A\_tile[0,0]}$ & $W = \{0, 1, 2, ... , 255\}$\\
\rowcolor{violetBoxBg} & $R_0 = \{0\}, R_1 = \{16\}, R_2 = \{32\} ... R_{15} = \{240\}$ \\
\rowcolor{yellowBoxBg} $\Sigma_{ko}$ & $1$ \\
\rowcolor{yellowBoxBg} $\Sigma_{A\_tile[0,0]}$ & $A[0,8]$ \\
\rowcolor{yellowBoxBg} $T$ & $0$ \\
\rowcolor{redBoxBg} \color{redBoxFg} checking: & \color{redBoxFg} $\forall R_i, T \in R_i$ FAIL, $0 \notin \{16\}, 0 \notin \{32\}, ...$
\end{tabular}
}
\end{minipage}

\newpage
\myTitle{CUDA Async Instructions}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
New accelerator instructions with the NVIDIA H100 (Hopper) are asynchronous.
They are issued by CUDA threads or warpgroups, but don't operate in the normal timeline:
\begin{itemize}
  \item TMA: copy tensor tiles GMEM$\leftrightarrow$SMEM
  \item wgmma: tiled matrix multiply-accumulate
\end{itemize}
This means the following won't work if the highlighted statements are async:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$B \leftarrow Y$}
  \item $C \leftarrow C + AB$ (incorrectly assumes $A,B$ written to)
\end{enumerate}
Two async instructions issued by the same thread/warpgroup don't even complete in the same order relative to each other, e.g., the following is a data race:
\begin{enumerate}
  \item \redBox{$A \leftarrow X$}
  \item \redBox{$A \leftarrow Y$}
\end{enumerate}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
These also ignore all ``ordinary'' synchronization.
So our previous formalization won't work for these.

Special-purpose synchronization statements must be used for these instructions.

Note: the effects of async instructions are also made visible by the ``stop the world'' barriers between grid launches on the same stream.

\mySub{Async Proxy}

In CUDA terminology, TMA and wgmma are considered to operate in the ``async proxy''.
Almost everything else is in the ``generic proxy'' (I'm ignoring the tensorMap proxy).

A ``proxy fence'' is needed to communicate from the generic proxy to the async proxy.

Typically, this is not needed for the opposite direction; synchronization that waits for a TMA or wgmma instruction generally documents it includes a proxy fence.
\end{minipage}

\newpage
\myTitle{wgmma: warpgroup matrix multiply accumulate}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
Conceptually, this is really simple.
Each \texttt{wgmma.mma\_async} instruction is executed by a warpgroup (128 aligned threads), and computes matrix tiles
\begin{itemize}
  \item $D \leftarrow AB$ or
  \item $D \leftarrow AB + D$
\end{itemize}
Where $D$ (accumulator) is stored distributed in the 128 threads' registers, $B$ is stored in shared memory, and $A$ may be in either format.

The complexity for this feature comes from
\begin{itemize}
  \item Input/output format details \textbf{(huuuge mess)}
  \item Synchronization
\end{itemize}
I'll just address the latter for now.
Although Exo will eventually have to figure out how to model the full set of matrix formats.
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ wgmma}

For registers, we need to issue \texttt{wgmma.fence} for wgmma to see register values written by threads.

For shared memory, we need to issue an async proxy fence, unless the memory was written by TMA (which is already in the async proxy).

Note: as this asynchronous instruction modifies registers $D$ directly (and optionally reads $A$ from registers), it's possible to have a race condition on a thread's registers!

\mySub{Synchronization wgmma $\to$ generic}

The completion mechanism is ``commit group''.

A \texttt{wgmma.commit\_group} instruction commits all prior uncommitted \texttt{wgmma.mma\_async} instructions to a commit group, then \texttt{wgmma.wait\_group N} waits for the MMAs of the $N^{th}$ prior commit group to complete (pipelining).

This is all only done within one warpgroup; a warpgroup cannot (directly) wait for another warpgroup's \texttt{wgmma.mma\_async} instructions to complete.
\end{minipage}

\newpage
\myTitle{TMA: tensor memory accelerator}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
Handled with \texttt{cp.async.bulk.tensor} instructions.

A \texttt{CUtensorMap} describes how to interpret some memory as a 1D-5D tensor (strides, etc.), and a box size (copied tile size).

The \texttt{cp.async.bulk.tensor} issues a copy between a densely-packed tile in shared memory and a strided tile in global memory
(both directions supported)

The tiles are C-style arrays
\begin{itemize}
  \item Whether this means ``row major'' or ``column major'' depends on how the programmar views the tensor, but regardless the tensor is in ``array of arrays'' layout.
  \item Transpose is not supported
\end{itemize}
\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (gmembig) [bignode, right=of smem, yshift=-3mm] {};
\node (gmem) [gmemnode, right=of smem, xshift=6mm] {\myKey{GMEM tile}};
\draw [arrow] ($(smem.east)-(0,0.2)$) -- ($(gmem.west)-(0,0.2)$);
\draw [arrow] ($(gmem.west)+(0,0.2)$) -- ($(smem.east)+(0,0.2)$);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\mySub{Synchronization generic $\to$ TMA}

Async proxy fence required to see memory written by generic instructions.

\mySub{Synchronization TMA $\to$ generic}

If the TMA copies SMEM to GMEM, the completion mechanism is commit\_group.

If the TMA copies GMEM to SMEM, the completion mechanism is mbarrier (split barrier)

Vastly simplified explanation:
\begin{itemize}
  \item mbarrier constructed in shared memory, configured with ``arrive count'' $A$
  \item Allows any number of CUDA threads to wait for $A$-many other CUDA threads in the same block.
    (This split barrier is usable separate from TMA)
  \item Can additionally specify \texttt{tx\_count}: wait for \textit{upcoming} TMA GMEM$\to$SMEM copies of total size \texttt{tx\_count} bytes.
\end{itemize}

\end{minipage}
\newpage
\myTitle{TMA Reduction}

\begin{minipage}[t]{0.5\textwidth}\fixminipage
TMA has an additional ``reduce'' functionality when copying shared memory to global memory.

When enabled, the SMEM tile to GMEM tile ``copy'' becomes a reduction into GMEM instead (for our purposes, we care about the \texttt{+=} reduce operator).

This reduction is \textit{atomic} (\texttt{relaxed.gpu} memory model) per tensor element.

Extremely OP Feature!

\begin{tikzpicture}[node distance=5mm]
\node (smem) [smemnode] {\myKeyB{SMEM tile}};
\node (op) [normalnode, below=of smem] {AtomicOp};
\draw [arrow] (smem) -- (op);
\node (gmembig) [bignode, below=of op] {};
\node (gmem) [gmemnode, below=of op, yshift=-6mm] {\myKey{GMEM tile}};
\draw [arrow] (gmem.east) to[out=0,in=0] (op.east);
\draw [arrow] (op.south) to (gmem.north);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\fixminipage
\end{minipage}
% CUDA Advanced Synchronization

% TMA

% wgmma

% Actor Kind

% Actor Signature

\end{document}
