% python3 code_to_tex.py su_samples.py su_samples && xelatex summer_of_wgmma.tex </dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{arraynode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{packednode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{featurenode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Summer Plans for Spork}

For now I'm just focusing on proposed changes to the Exo object language.
I also have to figure out how to update the synchronization checker to handle new features, and glue it to Exo.
After that, implementing algorithms in Exo-GPU and performance results (sounds like part of this work may be outsourced to Jason).

New/changed features:

\begin{itemize}
\item Instructions are currently conveyed as Python functions; as an alternative, we should also support ``instruction templates'' conveyed as Python classes.
These instruction templates can be instantiated to create a family of similar instrs, and give us more room syntactically to fit all the instruction metadata needed for Exo-GPU.
\item Move away from string interpolation and towards more ``structured'' interfaces for generating windows and C/C++ instruction syntax.
\item Changes to distributed memory to support multicasting and warp-shuffle instructions, i.e., cases where it's no longer true that each distributed shard is accessed solely by its owner.
\item Possibly, replace the actor kind idea with timelines: \textit{instruction timeline} (\lighttt{instr\_tl}) and \textit{usage timeline} (\lighttt{usage\_tl}), which are per-instruction and per-instruction-parameter attributes, respectively.
\item Improvements to Arrive/Await statements to support TMA and multicast: we need to allow arriving on multiple barriers, and associating a barrier with a specific instruction, and no prior instructions, in an instruction stream.
\end{itemize}

\filbreak
\myTitle{Chapters}

This document is meant more as a reference guide, and not something to read end-to-end.

{\sffamily
\myChapterLink{ch:InstrClass}{Instr Class}

\myChapterLink{ch:Windows}{Make Windows Better}

\myChapterLink{ch:Instrs}{Instrs \& Distributed Memory}

\myChapterLink{ch:Timelines}{Timelines \& Synchronization}

\myChapterLink{ch:ArriveAwaitPairing}{Arrive/Await Pairing}

\myChapterLink{ch:Multicasting}{Multicasting mbarriers}

\myChapterLink{ch:tma}{TMA Instruction Barrier Parameter}

\myChapterLink{ch:mbarrier}{Exo mbarrier Pairing Requirement}

\myChapterLink{ch:AbstractMachineGrammar}{Abstract Machine Grammar}
}

\newpage
\mySection{Instr Class}
\label{ch:InstrClass}

``New-style'' instruction templates are created as \lighttt{@instr}-decorated Python classes, rather than functions.
The class must have two functions
\begin{itemize}
  \item \lighttt{behavior}: Exo code specifying the behavior of the function.
  \item \lighttt{instance}: Python code executed by the compiler and codegen.
\end{itemize}

\input{su_samples/instr_class.0.tex}

\filbreak
The decorated class is not itself an instr, but an instr template.
All parameters in common between the \lighttt{behavior} and \lighttt{instance} functions (in this case, \lighttt{K}) must be substituted with concrete values.
These are ``template parameters''.
The other \lighttt{behavior} parameters are ``runtime parameters''.

\input{su_samples/instr_class.1.tex}

\filbreak
Besides allowing code-reuse through the template mechanism and class inheritance, conveying the \lighttt{instr} as a class gives us more room syntactically (in the \lighttt{instance} function) to convey the large amount of metadata for Exo-GPU instructions:
\begin{itemize}
  \item collective unit
  \filbreak
  \item include files and ``utils'' for C and CUDA C++
  \filbreak
  \item instruction timeline (n\'ee actor kind)
  \filbreak
  \item per-parameter information: usage timeline (``actor signature''), sync/async access
\end{itemize}

\filbreak
\mainSub{Exo Syntax}

Pass template parameters as keyword arguments, and runtime parameters as positional:

\input{su_samples/instr_class.2.tex}

\filbreak
\mainSub{LoopIR\_Unification Changes}

For convenience, usually the user shouldn't substitute template parameters manually.
Instead, we should program \lighttt{replace()} to be able to take an instr template, and deduce and substitute template parameters as part of unification.
It's an error if the substituted value is not a constant.

\filbreak
\mainSub{Include Files \& Utility Code}

We will have a lot of similar instructions that may share snippets of useful ``global'' source code.
As a replacement for \lighttt{c\_global}, the \lighttt{instance} function of an instr class configures these attributes defining source code needed for compiling the instrs:
\begin{itemize}
\item \lighttt{c\_includes: List[str]}: names of include files needed in C code (the \lighttt{.c} file)
\item \lighttt{c\_utils: List[str]}: C code snippets
\item \lighttt{cu\_includes: List[str]}: names of include files needed in CUDA C++ code (the \lighttt{.cuh} file)
\item \lighttt{cu\_utils: List[str]}: CUDA C++ code snippets
\end{itemize}
Duplicate include files/utils are discarded.

\filbreak
Because the header file (\lighttt{.cuh}) for the generated CUDA code is not really private, all the CUDA utilities are placed into a ``unique'' namespace (uniquify based on Python source file name).
This is aliased as \lighttt{namespace exo\_CudaUtil} in generated CUDA functions.

\filbreak
\mainSub{Instr Format Function}

The current system of generating C (or CUDA) code for instrs using \lighttt{str.format} is really unwieldly.
As an alternative, let's support codegen with a Python function that's given information on all runtime parameters and returns a C/C++ string.
This ties into my proposed changes to the window system that allow deeper introspection of windows.

\newpage
\mySection{Make Windows Better}
\label{ch:Windows}

Windows serve two purposes in Exo
\begin{itemize}
 \item In the common usage, we use windows to pass slices of tensors to instrs.
 Typically, the instr's code format does not actually create a window, and just formats using \lighttt{\textit{arg\_name}\_data}.
 I call this a \myKeyA{non-encoded} window.
 \item Much less commonly, we actually \myKeyA{encode} a window to a window struct, such as when we cross a true function boundary, or we compile a \lighttt{WindowStmt} (can of worms I won't get into in this document).
\end{itemize}
\filbreak
The uncommon encoded window case puts artificial constraints on our thinking.
I propose we make it much easier to unpack and inspect attributes of a window within the Exo compiler, with limited (or no) support for encoding the window, depending on memory type.
Currently, this is only possible for the limited case of \lighttt{\textit{arg\_name}\_data} (inspecting the offseted pointer for a window) and the weird \lighttt{\textit{arg\_name}\_int} thing.

\filbreak
Furthermore, we should have language-level support for expressing the different ``hardware modes'' for tensor dimensions (distributed memory, array, packed/vectorized), and enforcing commonplace restrictions (e.g. that an AVX vector isn't strided).
I think this is better than trying to repeatedly implement hacky assertions with \lighttt{stride}, etc., in \lighttt{Memory}/\lighttt{SpecialWindow} implementations.

\filbreak
\mainSub{Dimension Hardware Modes}

From left-to-right, a tensor's dimensions consist of \redBox{distributed}, \yellowBox{array}, and \blueBox{packed} (``vectorized'') dimensions.
The distributed dimensions are deduced by Exo-GPU, and mostly hidden from the window implementation; the Exo-GPU compiler rewrites the subtree to eliminate distributed dimensions prior to C/C++ codegen.
The shape of the \myKeyA{packed tensor} (dimension count and expected sizes) is defined by the \lighttt{Memory} type (e.g. [8], for \lighttt{\_\_m256}).
The remaining dimensions (in the middle) are array dimensions.

\myKeyA{AVX Example:}

\input{su_samples/avx_example.0.tex}

\filbreak

\myKeyA{CUDA Example:}

\input{su_samples/warp_example.0.tex}

\filbreak
For window parameters, we can explicitly tag the hardware modes, from left-to-right:
\begin{itemize}
  \item ``\lighttt{distributed:}'' rarely used; needed for multicast and warp shuffle
  \filbreak
  \item ``\lighttt{strided:}'' array dimension, with arbitrary stride
  \filbreak
  \item ``\lighttt{dense:}'' array dimension, densely strided
  \filbreak
  \item ``\lighttt{packed:}'' packed (vectorized) dimension
\end{itemize}

\filbreak
The compiler enforces when compiling each \lighttt{WindowExpr} that \lighttt{dense} dimensions are valid (this is currently handled repeatedly for each instr, using \lighttt{assert} and \lighttt{stride} expressions).

\filbreak
\myKeyA{Backwards Compatibility:} If not tagged, query the number of packed dimensions $P$ from the \lighttt{Memory} type, and assume the right-most up-to $P$ dimensions are \lighttt{packed}, and the rest \lighttt{strided}.

\filbreak
\myKeyA{AVX Example:}

\input{su_samples/avx_window.0.tex}

\filbreak
\myKeyA{TMA GMEM Example:}

\input{su_samples/tma_window.0.tex}

\filbreak
\myKeyA{TMA SMEM Example:}

\input{su_samples/tma_window.1.tex}

\filbreak
\mainSub{Expected Patterns}

For Exo-GPU, there's several usage patterns for windows:

\filbreak
\myKeyA{Pointer + strides:} Only thing that is supported today.
Allocated in C/C++ as a flattened array.
We can freely load/store scalars, and reduce the dimensionality of a window.
``Mutable'' pointer: we do pointer arithmetic to implement offsets.
These windows may be encoded as structs.

\input{su_samples/pointer_strides.0.tex}

\filbreak
\myKeyA{Register-like:} Allocated in C/C++ as a multidimensional array.
Cannot be encoded as a window struct, or passed to non-instr procs.

\input{su_samples/register_alloc.0.tex}

\filbreak
\myKeyA{Pointer + offsets:}
These windows can be encoded as structs, but with an ``immutable'' pointer: the offsets of the window are encoded explicitly.
The dimensionality of the window can't be changed (i.e. we support only intervals, not points).
This is primarily for CUtensorMap (TMA).

\input{su_samples/pointer_offsets.0.tex}

\filbreak
\mainSub{Windows in Exo Today}

Window are extremely hard-coded in Exo today.
Regardless of the memory type, a window gets generated based on the precision, the dimensionality, and const-ness.
This window struct is defined and populated with a data pointer and strides with no involvement of the underlying memory type. For example, for 2D const f32 windows, we have:

\input{su_samples/2f2c.0.tex}

\filbreak
For memory types, there's no support for customizing encoded windows, and very limited support for non-encoded windows, where there's an implicit contract between a memory type's \lighttt{window()} function and an instr's usage of  \lighttt{\{\textit{arg\_name}\_data\}}.

\filbreak
For example, the \lighttt{AVX2} memory type has completely invalid encoded window structs (because we try to assign a \lighttt{\_\_m256} pointer to a scalar \lighttt{float} pointer), and the implicit \lighttt{window()} contract is that an instr can access a \textit{single} \lighttt{\_\_m256} vector, since the \lighttt{window()} callback interprets all offsets except the last as array indices.

\input{su_samples/avx_before.0.tex}

\filbreak
Instrs have partial ability to prevent invalid usage by using \lighttt{stride()} expressions, e.g., the AVX window \lighttt{foo[1:9,3,6]} will be rejected (after we fix the assert bug) by \lighttt{assert stride(..., 0) == 1}.

\filbreak
The \lighttt{window()} function has to know how to decode the pointer attirbute from a window struct (\lighttt{.data}), and, indeed, is provided unfiltered access to the otherwise-private \lighttt{LoopIR.Tensor} type to detect whether this is needed:

\input{su_samples/is_win.0.tex}

\filbreak
Instrs use \lighttt{str.format} to generate C syntax.
For each window parameter, the formatter receives as keyword arguments:
\begin{itemize}
  \item \lighttt{\{\textit{arg\_name}\}}: the \myKeyA{encoded window} struct.
  \filbreak
  \item \lighttt{\{\textit{arg\_name}\_data\}}: the results given by \lighttt{window()}.
    This is a restricted case of what I'll call \myKeyA{window indexing}: accessing the value at a specific position of a window.
    For example, AVX instrs can access only one specific \lighttt{\_\_m256} vector.
  \filbreak
  \item \lighttt{\{\textit{arg\_name}\_int\}}: The raw C name of the input parameter.
\end{itemize}

\filbreak
\mainSub{Externalizing Windows}

This is a bit difficult as window generation is heavily coupled with the compiler internals, e.g.,

\begin{itemize}
  \item known strides
  \filbreak
  \item \lighttt{CIR}, for generating nice index/stride arithmetic expressions
  \filbreak
  \item assertions
  \filbreak
  \item somewhat messy interaction with scalar\_refs
\end{itemize}

\filbreak
For the most part, it'll be good to continue to use these internals and avoid having to re-implement basic window features over and over again, potentially in buggy ways (precedence errors, etc.).

\filbreak
I'm proposing that we view windows as a collection of features that are internally manipulated by the compiler (e.g. adding offsets when compiling a \lighttt{WindowExpr}).
We'll provide controlled interfaces to these features for \lighttt{MemWin} and \lighttt{instr}:
\begin{itemize}
  \item \myKeyA{Data pointer:} name of the allocated tensor, or window \lighttt{data} pointer attribute
  \filbreak
  \item \myKeyA{Array strides:} strides corresponding to array dimensions (optional).
    \textbf{Potential Breaking Change:} can we specify strides in units of \myKeyA{packed tensors}, rather than scalars?
    This will make querying the stride of a packed dimension meaningless.
  \filbreak
  \item \myKeyA{Array offsets:} explicit offsets for array dimensions that need to be accounted for to access the ``true'' window relative to the given data pointer. For the current pointer+strides-style windows, these are implicitly all 0, i.e., the \lighttt{data} pointer has already been offset.
  \filbreak
  \item \myKeyA{Packed offsets:} explicit offsets for packed dimensions (optional support). This doesn't make sense for cases where elements in the packed tensor cannot be randomly accessed (e.g. AVX vectors), but it will be useful when we implement swizzled matrices (where we can in fact random access elements, but have to be careful about it).
\end{itemize}

\filbreak
The \lighttt{MemWin} type can provide three (optional) interfaces to the underlying window system
\begin{itemize}
  \item \myKeyA{\texttt{packed\_tensor\_size}}: Given a precision (e.g. f32), give the number and expected sizes of packed dimensions. e.g. for AVX2, \lighttt{f32 -> [8]}, \lighttt{f64 -> [4]}. There may be multiple supported sizes, e.g. for CUDA mma, \lighttt{f32 -> [16, 8], [16, 4]}, \lighttt{f16 -> [16, 16], [16, 8]}.
  \item \myKeyA{\texttt{WindowEncoder}}: Convert window features to/from window struct. Analgous to \lighttt{\{\textit{arg\_name}\}}.
  \filbreak
  \item \myKeyA{\texttt{WindowIndexer}}: For use in compiling instrs to C. Given window features, generate a C expression resolving to values at a certain location in the window. Analagous to \lighttt{\{\textit{arg\_name}\_data\}}.
\end{itemize}

\filbreak
\mainSub{Window Encoder}

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=0mm]
\node(alloc) [normalnode] {Allocation / non-window param};
\node(defaults) [below=of alloc] {Built-in defaults:};
\node(default_dataptr) [below=of defaults] {C variable name};
\node(default_strides) [below=of default_dataptr] {C-style N-D array strides};
\node(default_array_offsets) [below=of default_strides] {all 0s};
\node(win) [normalnode, below=of default_array_offsets, yshift=-5mm] {WindowStmt / window parameter};
\node(decode) [memnode, text width=3cm, below=of win] {MemWin: Window Decode};
\node(dataptr) [featurenode, right=of default_dataptr, xshift=16mm] {dataptr};
\node(strides) [featurenode, below=of dataptr] {strides};
\node(array0) [arraynode, below=of strides] {array offsets};
\node(array1) [arraynode, right=of array0, xshift=16mm] {array offsets};
\node(array_intervals) [arraynode, below=of array1] {array interval sizes};
\node(packed) [packednode, below=of array_intervals] {packed offsets};
\node(packed_intervals) [packednode, below=of packed] {packed interval sizes};
\node(packed_tensor) [memnode, text width=6cm, below=of packed_intervals, yshift=-2mm, xshift=30mm] {MemWin: Packed tensor size};
\node(split) [left=of packed_tensor, xshift=-8mm] {split \yellowBox{array} \& \blueBox{packed} indices};
\node(window_expr) [normalnode, left=of split, xshift=-8mm] {WindowExpr};
\node(encode) [memnode, above=of array1, xshift=22mm, yshift=22mm, text width=5cm] {MemWin: Window Encode};
\node [right=of array_intervals] {(check dimension change support)};
\node [right=of packed] {(assert all 0)};
\node [right=of packed_intervals] {(assert complete intervals)};
\draw [arrow] (alloc.west) to[in=180, out=180] (defaults.west);
\draw [arrow] (win.west) to[in=180, out=180] (decode.west);
\draw [arrow] (default_dataptr.east) to[in=180, out=0] (dataptr.west);
\draw [arrow] (default_strides.east) to[in=180, out=0] (strides.west);
\draw [arrow] (default_array_offsets.east) to[in=180, out=0] (array0.west);
\draw [arrow] (decode.east) to[in=180, out=20] (dataptr.west);
\draw [arrow] (decode.east) to[in=180, out=0] (strides.west);
\draw [arrow] (decode.east) to[in=180, out=340] (array0.west);
\draw [arrow] (array0.east) to[in=180, out=0] (array1.west);
\draw [arrow] (window_expr.east) to (split.west);
\draw [arrow] (packed_tensor.west) to (split.east);
\draw [arrow] (split.north) to[out=90, in=180] (array1.west);
\draw [arrow] (split.north) to[out=90, in=180] (array_intervals.west);
\draw [arrow] (split.north) to[out=90, in=180] (packed.west);
\draw [arrow] (split.north) to[out=90, in=180] (packed_intervals.west);
\draw [arrow] (dataptr.east) to[out=0, in=180] ($(encode.west)+(0, 0.2)$);
\draw [arrow] (strides.east) to[out=0, in=180] ($(encode.west)+(0, -0.2)$);
\draw [arrow] (array1.east) to[out=0, in=270] ($(encode.south)+(-0.2, 0)$);
\draw [arrow] ($(array_intervals.east)+(0, 0.4)$) to[out=0, in=270] ($(encode.south)+(0.2, 0)$);
\end{tikzpicture}
\caption{Window encode data flow}
\end{figure*}

Each \lighttt{MemWin} type can customize its window encoder (window struct), or declare that there is no window encoder (a compiler error gets generated if we try to materialize the window struct).

\filbreak
Like windows currently, the window struct can be customized over precision, dimensionality, and const-ness.
The window encoder has to implement
\begin{itemize}
  \item \lighttt{decode\_dataptr():} extract the pointer member of the window struct (e.g. \lighttt{\{win\}.data})
  \filbreak
  \item \lighttt{decode\_array\_offset(n):} extract the offset for the $n^{th}$ array dimension (for current pointer+stride windows, this is always 0).
  \filbreak
  \item \lighttt{decode\_array\_stride(n):} extract the stride for the $n^{th}$ array dimension (e.g. \lighttt{\{win\}.strides[\{n\}]}). This is optional; if not implemented, then no other interfaces can query the stride.
  \filbreak
  \item \lighttt{encode\_window(...):} C syntax constructing the window struct, from the data pointer, array offsets, and array strides.
  The constructor may assume all packed dimensions are indexed with complete intervals (``\lighttt{:}'' or the equivalent); we have a built-in assumption that encoded window can't point to slices of a packed tensor.
\end{itemize}

\filbreak
For pointer+stride windows, \lighttt{encode\_window} will look something like

\input{su_samples/encode_window.0.tex}

\filbreak
By default, all indices in a window expression must be intervals, so the dimensionality of a window doesn't change.
The window encoder may optionally declare support for dimensionality changes.
In this case, the window encoder will also receive \lighttt{interval\_sizes: List[Optional[\textit{C expr}]]}, giving \lighttt{None} for point indices and the interval size for interval indices.

\filbreak
Finally, we have to provide a \myKeyA{separate dataptr} mode, where the data pointer is stored separately from the rest of the window struct.
This is an alternative to implementing \lighttt{decode\_dataptr}.
We need this for \lighttt{CUtensorMap}, which has to be stored in a different memory type than the offsets (grid constant vs registers).

\input{su_samples/tma_proc.0.tex}

\filbreak
Note: all of this occurs after distributed memory deduction, and removal of distributed dimensions.

\filbreak
\mainSub{Window Indexer}

\filbreak
The window indexer receives
\begin{itemize}
  \item the dataptr
  \filbreak
  \item a sequence of array indices
  \filbreak
  \item a sequence of packed indices
  \filbreak
  \item the array strides, if supported by the window encoder
\end{itemize}

\filbreak
and needs to return a C expression giving the data at the position given by concatenating the array and packed indices.
If the total number of indices is lower than the dimensionality of the window, the indexer can assume the remaining indices are full intervals (\lighttt{:}).
This is the ``points before intervals'' assumption.

\filbreak
It's somewhat up to each \lighttt{MemWin} implementation to define what the exact behavior of the window indexer is, and what to do if there are fewer indices given than there are window dimensions (if this is allowed at all).
For example, the previous \lighttt{AVX2} memory type will likely require that \lighttt{len(array\_indices)} is 1 less than the dimension, and that \lighttt{packed\_indices} is empty
(i.e. that you can only read exactly one packed tensor, i.e., \lighttt{\_\_m256} vector, and you can't index into the vector).

\filbreak
The compiler has more information and can generate better quality errors, so, we require the indexer to declare one of the following support modes for array indices and packed indices: none, any number, or exact dimension match.
This avoids us having to write repeated (and low quality / forgotten) error checking code.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=0mm]
\node(dataptr) [featurenode] {dataptr};
\node(strides) [featurenode, below=of dataptr] {strides};
\node(array0) [arraynode, below=of strides] {array offsets};
\node(decode) [memnode, text width=3cm, below=of array0, yshift=-12mm] {MemWin: Window Decode};
\node(defaults) [below=of decode] {(or built-in defaults)};
\node(array1) [arraynode, right=of array0, xshift=16mm, yshift=8mm] {array offsets};
\node(packed1) [packednode, below=of array1] {packed offsets};
\node(array1_intervals) [arraynode, below=of packed1] {array interval sizes};
\node(packed1_intervals) [packednode, below=of array1_intervals] {packed interval sizes};
\node(array2) [arraynode, right=of array1, xshift=16mm] {array offsets};
\node(packed2) [packednode, below=of array2] {packed offsets};
\node(array2_intervals) [arraynode, below=of packed2] {array interval sizes};
\node(packed2_intervals) [packednode, below=of array2_intervals] {packed interval sizes};
\node(window_expr) [normalnode, below=of packed1_intervals, yshift=-5mm] {WindowExpr};
\node(instr) [normalnode, below=of packed2_intervals, yshift=-5mm] {instr callback's index(...) args};
\node(index) [memnode, right=of array2, yshift=15mm, text width=52mm] {MemWin: Window Indexer};
\node [right=of array2] {(check MemWin supports idx count)};
\node [right=of packed2] {(check MemWin supports idx count)};
\node [right=of array2_intervals] {(check points before intervals)};
\node [right=of packed2_intervals] {(check points before intervals)};
\draw[arrow] ($(decode.north)$) -- ($(array0.south)$);
\draw[arrow] ($(decode.north) - (0.2, 0)$) -- ($(array0.south) - (0.2, 0)$);
\draw[arrow] ($(decode.north) + (0.2, 0)$) -- ($(array0.south) + (0.2, 0)$);
\draw[arrow] ($(window_expr.north) - (0.3, 0)$) -- ($(packed1_intervals.south) - (0.3, 0)$);
\draw[arrow] ($(window_expr.north) - (0.1, 0)$) -- ($(packed1_intervals.south) - (0.1, 0)$);
\draw[arrow] ($(window_expr.north) + (0.1, 0)$) -- ($(packed1_intervals.south) + (0.1, 0)$);
\draw[arrow] ($(window_expr.north) + (0.3, 0)$) -- ($(packed1_intervals.south) + (0.3, 0)$);
\draw[arrow] ($(instr.north) - (0.3, 0)$) -- ($(packed2_intervals.south) - (0.3, 0)$);
\draw[arrow] ($(instr.north) - (0.1, 0)$) -- ($(packed2_intervals.south) - (0.1, 0)$);
\draw[arrow] ($(instr.north) + (0.1, 0)$) -- ($(packed2_intervals.south) + (0.1, 0)$);
\draw[arrow] ($(instr.north) + (0.3, 0)$) -- ($(packed2_intervals.south) + (0.3, 0)$);
\draw[arrow] (array0.east) to[out=0, in=180] (array1.west);
\draw[arrow] (array1.east) -- (array2.west);
\draw[arrow] (packed1.east) -- (packed2.west);
\draw[arrow] (array1_intervals.east) -- (array2_intervals.west);
\draw[arrow] (packed1_intervals.east) -- (packed2_intervals.west);
\draw[arrow] (array1_intervals.east) to[out=0, in=180] (array2.west);
\draw[arrow] (packed1_intervals.east) to[out=0, in=180] (packed2.west);
\draw[arrow] (dataptr.east) to[out=10, in=180] ($(index.west) + (0, 0.2)$);
\draw[arrow] (strides.east) to[out=20, in=180] ($(index.west) + (0, -0.2)$);
\draw[arrow] ($(array2.east) + (0, 0.25)$) to[out=0, in=270] ($(index.south) + (-1.6, 0)$);
\draw[arrow] ($(packed2.east) + (0, 0.25)$) to[out=45, in=270] ($(index.south) + (-1.4, 0)$);
\end{tikzpicture}
\caption{Window index data flow; note, the MemWin's \greenBox{packed tensor size} informs the split between \yellowBox{array} and \blueBox{packed} indices, which is omitted for space.} \label{fig:windowIndex}
\end{figure*}

\filbreak
\mainSub{Instr Interface to Windows}

The proposed instr C codegen callback will receive a tuple of named parameter values.
Non-window values will probably stay as strings.
Window values could be passed as objects that support functions
\begin{itemize}
  \item \lighttt{to\_struct()}: Encode as window struct; only supported if the \lighttt{MemWin} type supplied a window encoder.
  \filbreak
  \item \lighttt{to\_dataptr()}: For separate-dataptr windows only, get the separate dataptr.
  \filbreak
  \item \lighttt{get\_stride(n)}: Get the $n^{th}$ stride value; only supported if the \lighttt{MemWin} type supplied a window encoder that supports strides, or if there is no window encoder at all (use default strides).
  \filbreak
  \item \lighttt{index(idx...)}: Index into the window expression passed as the instruction parameter.
    Only supported if the \lighttt{MemWin} type supplied a window indexer.
    The indices passed are not directly passed to the window indexer, but are combined with the offsets given by the window expression and (if it exists) underlying decoded window struct.
    This requires careful resolution of point and interval indices, which we will handle inside the compiler \textbf{(figure \ref{fig:windowIndex})}.
  \filbreak
  \item \lighttt{raw\_name():} Get the undecorated C name of the parameter passed as input.
\end{itemize}
\filbreak
We can implement the older \lighttt{str.format} interface as a translation layer around this new interface:
\begin{itemize}
  \item \lighttt{\textit{arg\_name}} = \lighttt{to\_struct()}
  \item \lighttt{\textit{arg\_name}\_data} = \lighttt{index()} with empty index list (if you need more control, don't use the legacy interface)
  \item \lighttt{\textit{arg\_name}\_int} = \lighttt{raw\_name()}
\end{itemize}

\filbreak
\mainSub{Predicate Helpers}

We need to provide both the window encoder and indexer with helpers that check
\begin{itemize}
  \item that an offset or stride value is a compile-time constant;
  \filbreak
  \item that an offset or stride value is divisible by a certain value;
  \filbreak
  \item the alignment of memory (this will actually require cooperative changes to multiple parts of the language).
\end{itemize}

\newpage
\mySection{Instrs \& Distributed Memory}
\label{ch:Instrs}

The vast majority of instructions follow the ``each thread collective owns its own distributed shard'' model, i.e., any window parameters passed in refer to a portion of just one distributed shard.
We explicitly annotate (with \redBox{\texttt{distributed:}}) window parameters that break this assumption.

\filbreak
Within the instruction's \lighttt{behavior} specification, we need to add new \lighttt{distribute(...)} statements to guide distributed memory deduction.
LoopIR\_unification should ignore the loop mode of the for loops (which already seems to be the case), and ignore the \lighttt{distribute(...)} statements \textbf{(figures \ref{fig:shfl} and \ref{fig:tma})}

\begin{figure*}[!b]
\input{su_samples/shfl_sync.0.tex}
\caption{Warp Shuffle Instr} \label{fig:shfl}
\end{figure*}

\filbreak
It's somewhat questionable what the reasonable interpretation of the ``remove distributed dimensions'' compiler pass is here, given the distributed dimensions are in this case actually valuable information.
The current use cases (warp shuffle, and TMA multicast) are implemented using CUDA intrinsics that allow for peeking into other distributed shards internally, without invoking the Exo compiler's complicated window system described previously.

\filbreak
So, although it's a bit cheesy, I propose we just blindly strip the distributed dimensions anyway (and possibly just re-write the instr's behavior as a no-op since the typechecking will be messed up).
This means that the window \lighttt{index(...)} function is aware of array and packed dimensions only, and it's the instr implementation's job to handle indexing into other distributed shards.
This should be OK as the goal at this late stage of the compilation is just to ``get the job done'' and generate C/C++ code;
just make a mental note we did this in case there's problems later.

\begin{figure*}[!b]
\input{su_samples/tma_instr.0.tex}
\caption{TMA Multicast Instr} \label{fig:tma}
\end{figure*}

\newpage
\mySection{Timelines \& Synchronization}
\label{ch:Timelines}

For now I'll just sketch out the basics of how the abstract machine tracks read/write visibility, and go over the changes entailed by moving from the actor kind / actor signature model to the new timeline model:
\begin{itemize}
  \item Each instruction has an ``instruction timeline'' (instr-tl) type, replacing the actor kind used to parameterize \lighttt{CudaAsync} blocks.
  \filbreak
  \item Each instruction parameter has a ``usage timeline'' (usage-tl) type, roughly replacing the actor signature.
  \filbreak
  \item Sync statements now take ``sync timeline'' (sync-tl) parameters, replacing the dual-use of actor kind (formerly parameterized both sync statements and \lighttt{CudaAsync} blocks).
  \filbreak
  \item There are slightly different semantics for guarding against write-after-read (WAR) hazards (``temporal-only'' synchronization), compared to RAW or WAW hazards.
  \filbreak
  \item TODO: still have to retrofit atomic reduce to this model.
\end{itemize}

\filbreak
\mainSub{Instruction Timelines}

Each instruction has an instr-tl type (see list in \myChapterLink{ch:AbstractMachineGrammar}{Abstract Machine Grammar}).

\filbreak
All Exo code has a certain instr-tl in scope: \lighttt{cpu\_in\_order\_instr} in CPU code, \lighttt{cuda\_in\_order\_instr} within a \lighttt{CudaDeviceFunction} block, and one of the remaining types in a \lighttt{CudaAsync(*\_instr)} block.
All instrs used must have the same instr-tl type as the current scope.

\filbreak
\mainSub{Usage Timelines}

In many cases, the instruction timeline would have been specific enough; however, sometimes, we need to distinguish differing synchronization behavior for different parameters of the same instruction, e.g.,
\begin{itemize}
  \item wgmma has different behavior for SMEM parameters (matrix B, and sometimes A), RMEM accumulators (D), and, if used, RMEM A parameters.
  \filbreak
  \item Some async instructions may take some register parameters synchronously, i.e., after issuing the instruction, you can immediately re-use the register without waiting for the instruction to complete.
\end{itemize}
\filbreak
Dealing with these special cases is the purpose of the per-instr-parameter usage-tl type (see \myChapterLink{ch:AbstractMachineGrammar}{Abstract Machine Grammar}, for list)

\filbreak
\mainSub{Qualitative Timelines}

A pair (instr-tl, usage-tl) is a qualitative timeline: \myKeyA{qual-tl}.

\filbreak
\mainSub{Timeline Signature}

We characterize ``who'' performed a read/write  using \myKeyA{timeline signatures}.
These are 3-tuples of (instr-tl, usage-tl, tid); tid being a linearized thread index \lighttt{blockIdx.x * blockDim.x + threadIdx.x}.

\filbreak
\mainSub{Visibility Record}

During synchronization checking, we interpret the program sequentially and associate \textit{mutable} read and write \myKeyA{visibility records} with each position.
A position is an instance of a scalar variable, or an indexed scalar within a tensor variable.
The visibility records consist of
\begin{itemize}
  \item $V_S$: \myKeyA{sync visibility set}: set of timeline signatures
  \filbreak
  \item $V_A$: \myKeyA{async visibility set}: set of timeline signatures, with $V_S \subseteq V_A$
  \filbreak
  \item $L_O$: \myKeyA{original qualitative timeline}: qual-tl
  \filbreak
  \item pending arrives: set of (queue barrier object, arrive count) pairs; not described in this section
\end{itemize}
\filbreak
and (to be detailed) they are used as follows:
\begin{itemize}
  \item We check for visibility of prior read/write visibility records associated with the position when interpreting a read/write of that location.
  \filbreak
  \item We further add a new read/write visibility record after checking the validity of a read/write.
  \filbreak
  \item The visibility sets grow conditionally when we interpret a synchronization statement.
\end{itemize}

\filbreak
\mainSub{Read/Write Configuration}

Each instruction configures:
\begin{itemize}
  \item its instruction timeline $L^i$
  \filbreak
  \item per-parameter usage timeline $L^u$
  \filbreak
  \item per-parameter ``out-of-order'' flag $OOO$
  \filbreak
  \item per-parameter extended instruction timelines: $L_X^i$: set of instr-tl; we require $L^i \in L_X^i$
  \filbreak
  \item per-parameter extended usage timelines: $L_X^u$: set of usage-tl; we require $L^u \in L_X^u$
\end{itemize}
\filbreak
In Exo source code, we assume $L^i$ is \lighttt{cpu\_instr} if not configured otherwise (for backwards compatibility).
We initialize all per-parameter info with defaults as a function of the instr-tl and parameter \lighttt{MemWin} type, but this can be overrided in \lighttt{InstrClass.instance} if needed.
This defaulting cuts down on boilerplate and makes it easier for us to make bulk changes to the model later.

\filbreak
\mainSub{Read/Write State}

When we interpret a read or write, let its $L^i$, $L^u$, $OOO$, $L_X^i$, and $L_X^u$ be as defined above if the read/write is performed through an instr parameter.
If the read/write is performed outside an instr, then $L^i$ is that of the current scope, and all other values get initialized by the \lighttt{MemWin} defaults.

\filbreak
The \myKeyA{initial timeline signature} $L^s$ is $(L^i, L^u, t)$, with t being the ID of the thread issuing the read/write\footnote{It's unclear what to do for instructions where it's not possible to identify the single thread responsible for a read or write, e.g. wgmma SMEM.
It's not clear whether synchronization should be modelled as an ``all'' or ``any'' requirement in this case. For example, if threads [128, 255] issue a wgmma, and it reads from SMEM a proxy-fenced value written by thread 133, is futher synchronization required (because not \textit{all} threads can see the value), or not required (because \textit{any} of the issuing threads, namely $133 \in [128, 255]$, can see the value)?}.

\filbreak
The \myKeyA{initial extended timeline signatures} $L_X^s$ are $L_X^i \times L_X^u \times \{ t \}$.\footnote{In many cases, $L_X^s = \{L^s\}$, but when $L_X^s$ contains more timeline signatures, this indicates the current instruction is able to implicitly ``handshake'' with a different instruction/usage type.}

\myKeyA{Read Checking:} Check that each write visibility record is \myKeyA{visible-to} the interpreted read, where visible-to means $V_S \cap L_X^s$ is non-empty.

\myKeyA{Write Checking:} Check that each read and write visibility record is visible-to the interpreted write.

\myKeyA{New Visibility Record:} Add a new read/write visibility record
\begin{itemize}
  \item $V_A = \{ L^s \}$
  \filbreak
  \item $V_S = V_A$ if $OOO$ is false, otherwise initialize $V_S$ to be empty
  \filbreak
  \item $L_O = (L^i, L^u)$
\end{itemize}

\filbreak
\mainSub{Sync Timeline}

We parameterize sync statements (fences, arrives, awaits) with sync timeline (sync-tl) parameters.
Each fence statement has both a first and second sync timeline $L_1$ and $L_2$.
An arrive statement has only $L_1$, and an await statement has only $L_2$.

A sync timeline $L_n$ contains
\begin{itemize}
  \item $L_n^F$: full timeline set: set of qual-tl
  \filbreak
  \item $L_n^T$: temporal timeline set: set of qual-tl, with $L_n^F \subseteq L_n^T$
  \filbreak
  \item $V_1$-transitive flag (only relevant for first sync timelines)
\end{itemize}
\filbreak
This will control what subset of reads/writes \myKeyA{synchronize-with} the sync statement.

\filbreak
\mainSub{Sync Statement Behavior}

In addition to $L_1$ and $L_2$ defined above, define $C$ for each interpreted sync statement as the thread IDs of the convergent thread collective executing the statement (e.g. a statement corresponding to \lighttt{\_\_syncwarp()} is executed by a thread collective of 32 threads).

\filbreak
Define these sets of timeline signatures:
\begin{itemize}
  \item The first visibility set $V_1 = L_1^F \times C$
  \item The second temporal visibility set $V_2^T = L_2^T \times C$
  \item The second full visibility set $V_2^F = L_2^F \times C$
\end{itemize}

\filbreak
A visibility record $(V_S, V_A, L_O)$ \myKeyA{synchronizes-with} a fence or arrive statement when
\begin{itemize}
  \item $V_1 \cap V_A$ is non-empty, if $L_1$ is $V_1$-transitive;
  \filbreak
  \item $V_1 \cap V_A$ is non-empty AND $L_O \in L_1^F$, otherwise.
\end{itemize}
\filbreak

\filbreak
An interpreted fence or await statement \myKeyA{augments} a visibility record by setting $V_A \leftarrow V_A \cup V_2$ and $V_S \leftarrow V_S \cup V_2$, where $V_2$ is $V_2^T$ (temporal) or $V_2^F$ (full) for read visibility records and write visibility records, respectively.

\filbreak
When we interpret a fence statement, it augments all visibility records that synchronize-with the fence.
When we interpret an await statement, it augments all visibility records that synchronized-with a corresponding arrive statement
(the next section defines what ``corresponding arrive'' means).
This synchronizes-with check has to be done at the time the arrive is interpreted, as visibility records are mutable.
Remarks:
\filbreak
\begin{itemize}
  \item ``synchronizes-with'' is defined in terms of the async visibility set $V_A$ while ``augments'' modifies both $V_A$ and $V_S$; in this way, timeline signatures can graduate from $V_A$ to $V_S$.
  \filbreak
  \item Guarding against a write-after-read (WAR) hazard has looser requirements than RAW or WAW hazards, in that we only need to ensure temporal separation of the prior read and subsequent write, and do not actually need to preserve or communicate any value in memory (e.g. we can tolerate stale caches, or, in CUDA terms, we can elide proxy fences).
  This is the reason for having separate $V_2^T$ and $V_2^F$.
  \filbreak
  \item Furthermore, note $V_2^F \subseteq V_2^T$ as a consequence of $L_n^F \subseteq L_n^T$.
  \filbreak
  \item $L_1^T$ is currently never used.
\end{itemize}

\newpage
\mySection{Arrive/Await Pairing}
\label{ch:ArriveAwaitPairing}

It's time to define the correspondence between arrive and await statements.
In Exo-GPU's synchronization checking model, each \myKeyA{queue barrier} $Q$ contains non-negative integers \myKeyA{arrive count} $Q^1$ and \myKeyA{await count} $Q^2$, both initially zero (this conceptual state is distinct from the physical state of the real CUDA barrier type, which is specified with a \lighttt{@memory} annotation).

\filbreak
Each \lighttt{barrier}-typed variable declares 1 or 2 (to be explained) sized arrays of \myKeyA{queue barriers}, which can be indexed in arrive/await statements. We analyze indexing with distributed memory deduction (see \lighttt{coll\_algebra.pdf}).

\input{su_samples/mbarrier_var.1.tex}

\filbreak
Unlike non-barrier allocations, though, there is no native unit; we unconditionally interpret all dimensions as distributed (I might change this in the future), and thus, all indices must follow the distributed memory deduction requirements (e.g. they have to be \lighttt{cuda\_threads} loop iteration variables).
In other words, each distributed shard of a queue barrier array is only one queue barrier.

\filbreak
All barrier variables \lighttt{varname: barrier[z1, ..., zN]} contain a ``front queue barrier array'' of size $z_1 \times ... \times z_N$.
For mbarriers only, we support a separate ``back queue barrier array'' of the same size, referred to with \lighttt{-varname} (the front queue barrier array is \lighttt{+varname} or simply \lighttt{varname}).
For a queue barrier $Q$ in either array, let $\widehat{Q}$ denote the queue barrier in the corresponding position of the other array.

\filbreak
We would get almost the same program by replacing each usage of \lighttt{-my\_bar} with usage of a new barrier variable \lighttt{my\_var\_2}.
However, there is some tight coupling between the front and back barriers that justifies this design (more on this when we talk about the mbarrier pairing requirement).

\filbreak
The back queue barrier array is optional; it doesn't exist if it's never referenced.

\filbreak
\mainSub{Valid Usage}

The purpose of a single queue barrier $Q$ is to synchronize one specific set of arriving threads with another specific set of awaiting threads, in a predictable manner, and with the barrier being reset to a known-good state at the end.
The requirements enforce that intended usage, to make lowering queue barriers to real barriers feasible:

\filbreak
\begin{itemize}
  \item Each queue barrier array must be used by at least one \lighttt{Arrive} and at least one \lighttt{Await} statement.
  \filbreak
  \item For a given queue barrier array\footnote{\textit{Not} barrier variable, i.e., it's valid for \lighttt{Await(+bar,...)} and \lighttt{Await(-bar,...)} to use different synchronization timelines.}, each \lighttt{Arrive} statement must use equivalent sync timelines (sync-tl); similarly for all \lighttt{Await} statements.
  \filbreak
  \item Each queue barrier $Q$ (distributed shard of a queue barrier array) must have one unique thread collective that executes \lighttt{Arrive} on $Q$; similarly for \lighttt{Await}.
    We enforce this statically by inspecting the collective tiling for each usage.
  \filbreak
  \item If both a front and back queue barrier array are present for a given barrier variable, the distributed memory deductions for the two must be equivalent.
  \filbreak
  \item When a barrier variable is deallocated, each queue barrier must have $Q^1 = Q^2$.
  \filbreak
  \item mbarriers must follow the \myChapterLink{ch:mbarrier}{mbarrier pairing requirement}
\end{itemize}

\filbreak
\mainSub{Arrive Interpretation, N = 1}

Assume for now that an arrive statement only takes one queue barrier as input (if not, multicasting is in effect, which is not yet described), with the syntax\\\lighttt{Arrive(L1: sync-tl, N: int) >> \{+|-|\}barrier\_name[idx...]}

\filbreak
We require $N = 1$ for an arrive statement now.
When we interpret such an arrive statement on some queue barrier $Q$:
\begin{itemize}
  \item For each visibility record that synchronizes-with the arrive, add $(Q, Q^1)$ : (queue barrier object, arrive count) to the set of pending arrives.
  \item Increment $Q^1$.
\end{itemize}

\filbreak
\mainSub{Await Interpretation}

Await statements always take only one queue barrier $Q$ as input, with syntax\\
\lighttt{Await(\{+|-|\}barrier\_name[idx...], L2: sync-tl, N: int)}.
The interpretation depends on $N$.

\filbreak
If $N \geq 0$, then this is an ``arrive-indexed'' barrier, which is only supported for commit groups. Do:
\begin{itemize}
  \item Augment all visibility records containing $(Q, x)$ with $x < Q^1 - N$ [NB: strict inequality]
  \item Set $Q^2 = \max(Q^1 - N, Q^2)$
\end{itemize}
This models the meaning of \lighttt{wait\_group N} in CUDA, i.e., wait for all but the last $N$-many \lighttt{commit\_group} instructions.

\filbreak
If $N < 0$, then this is an ``await-indexed'' barrier, which is only supported for mbarriers.\\
Let \lighttt{lag = \textasciitilde N} (complement, not negate), and do:
\begin{itemize}
  \item Augment all visibility records containing $(Q, x)$ with $x \leq Q^2 - \lighttt{lag}$ [trivially false if $Q^2 < \lighttt{lag}$]
  \item Set $Q^2 = Q^2 + 1$
\end{itemize}
Essentially, the first \lighttt{lag}-many await statements are no-ops, and then arrives and awaits pair up 1:1.

\newpage
\mySection{Multicasting mbarriers}
\label{ch:Multicasting}

See this \webText{Colfax Research Article}{https://research.colfax-intl.com/cutlass-tutorial-gemm-with-thread-block-clusters-on-nvidia-blackwell-gpus/}, especially ``Constructing bitmasks'', for background.
This is for tcgen05 (Blackwell), but the cluster and CTA mask content applies to wgmma as well.

\filbreak
CUDA supports arriving on mbarriers in other CTAs of the same cluster.
Each queue barrier (with \lighttt{CudaMbarrier} memory type) lowers to a ring buffer of mbarriers, and we use distributed memory to model the full ``array'' of per-CTA mbarriers in a cluster.
So, for \lighttt{CudaMbarrier}-type barriers, it would be intuitive to extend the distributed indices feature to express such multicasting, e.g., something like

\filbreak
\input{su_samples/intro_multicast_mbarrier.0.tex}

\filbreak
In the abstract machine, the meaning of this is instead of adding a single pending arrive $(Q, Q^1)$ to visibility records that synchronize-with the arrive, we add $(Q_n, Q^1)$ for each queue barrier $Q_n$ named in any of the expressions following \lighttt{>>} (removing duplicates).
We will give details on how the queue arrive count $Q^1$ behaves later.

\filbreak
If there's multiple \lighttt{Arrive} statements for the same queue barrier array, their multicasting must be the same (so the above example illustrating multiple possible patterns is not valid within a single Exo proc).

Multicasting poses some challenges for our model:
\begin{itemize}
\item Since a distributed index was replaced with \lighttt{:}, how do we adapt distributed memory deduction to match?
\filbreak
\item The arrive count $Q^1$ is per-queue-barrier state in our model, but having this vary dynamically in different CTAs is not really implementable (nor useful), so we have to ensure some notion of $Q^1$ uniformity for all queue barriers involved in the arrive statement.
\end{itemize}

\filbreak
\mainSub{Distributed Memory Deduction}

Each \lighttt{>> \{+|-|\}barrier-name[idxs...]} is a \myKeyA{barrier expression}.
Await statements may not take multiple barrier expressions.
Arrive statements may take multiple barrier expressions, subject to the rules:
\begin{itemize}
  \item The same queue barrier array must be referenced in each expression.
  \filbreak
  \item Each index is either a \lighttt{cuda\_threads}-iterator, or \lighttt{:}.
  \filbreak
  \item For each \lighttt{i}, at least one \lighttt{idxs} must have \lighttt{idxs[i]} being a \lighttt{cuda\_threads}-iterator\footnote{This is the reason for the ``redundant'' barrier expressions earlier. The expression \lighttt{>> ringbar[m\_cta, :]} alone would have had the lone \lighttt{:} flagged as an error.} (instead of \lighttt{:}).
    This is the \lighttt{i}-th shared \lighttt{cuda\_threads} iterator.
    If multiple \lighttt{idxs} have \lighttt{idxs[i]} being a \lighttt{cuda\_threads}-iterator, they must all agree.
\end{itemize}

\filbreak
We reduce the raw queue barrier expressions to a shared queue barrier array reference and \lighttt{cuda\_threads}-iterator list, and a per-barrier-expression ``multicast flags'' list (True means \lighttt{:}).
For example, in the case \lighttt{>> +ringbar[m\_cta, n\_cta] >> +ringbar[:, n\_cta]}, the state would be
\begin{itemize}
  \item queue barrier array: \lighttt{+ringbar}
  \filbreak
  \item iterators: \lighttt{[m\_cta, n\_cta]}
  \filbreak
  \item per-expression multicast flags: \lighttt{[False, False]}, \lighttt{[True, False]}
\end{itemize}

The queue barrier array and \lighttt{cuda\_threads}-iterators get used for distributed memory deduction (which ignores multicasting).
We say that a barrier expression \textit{multicasts} the $n^{th}$ iterator if its $n^{th}$ multicast flag is true (e.g., in the previous example, \lighttt{ringbar[:, n\_cta]} multicasts \lighttt{\textit{m}\_cta}).

\begin{figure*}[!b]
\input{su_samples/multicast_convergence.0.tex}
\caption{Illustration of mbarrier multicast convergence requirements} \label{fig:multicast_convergence}
\end{figure*}

\filbreak
\mainSub{Proposed Behavior \& Requirements}

We impose a convergence requirement for arrives.
For each \lighttt{cuda\_threads}-iterator that is multicast by some barrier expression, search the syntax tree for the \lighttt{cuda\_threads} loop that defines this iterator \textbf{(figure \ref{fig:multicast_convergence})}.
There must not be any statements along the path between this for statement and the arrive statement, except for
\begin{itemize}
  \item \lighttt{with} statements
  \filbreak
  \item \lighttt{cuda\_threads} loops
\end{itemize}

\filbreak
For $N=1$, the behavior of an arrive statement is:
\begin{itemize}
  \item Let $Q$ be the \myKeyA{home queue barrier}, i.e., the queue barrier given by the queue barrier array and iterators deduced above (this is \lighttt{+ringbar[m\_cta, n\_cta]} in all above examples).
    This corresponds to the mbarriers resident within the CTA that is executing the current arrive statement.
  \filbreak
  \item Let $Q^1$ be the queue arrive count for the home queue barrier.
  \filbreak
  \item For each read visibility record that synchronizes-with the arrive, and for each queue barrier $Q_n$ named in any barrier expression of the arrive, add $(Q_n, Q^1)$ : (queue barrier object, arrive count) to the visibility record's set of pending arrives.
  \filbreak
  \item Increment $Q^1$.
    The queue arrive count of all barriers $Q_n \ne Q$ isn't affected.
\end{itemize}

\filbreak
The last step in particular doesn't map too clearly to the underlying hardware.
As optional reading, in the next section, I'll explain the correspondence between this model and the hardware state, and the motivations and problems with this design.

\filbreak
\minorSub{Hardware State}

Some of the background is more deeply explained in \myChapterLink{ch:mbarrier}{Exo mbarrier Pairing Requirement}.

Let $C$ be the thread collective executing an arrive statement.
Ignoring multicasting, the relevant hardware features and their correspondence to the abstract machine state (queue barriers), are
\begin{itemize}
  \item A ring buffer of mbarriers (resident within one CTA) comprise a queue barrier of \lighttt{CudaMbarrier} MemWin type.
  \filbreak
  \item The expected thread arrival count of an mbarrier corresponds to the number of threads in the collective unit that executes the arrive statement, i.e., $\abs{C}$; each thread executes \lighttt{mbarrier.arrive} on the current mbarrier of the ring buffer.
  \filbreak
  \item The pending thread arrival count of an mbarrier increments for each CUDA thread that executes \lighttt{mbarrier.arrive} on the mbarrier; when this reaches the expected thread arrival count, it resets, and the phase of the barrier increments.
  The phase change is what signals awaiting threads to continue.
  \filbreak
  \item Incrementing $Q^1$ corresponds to incrementing the phase of the mbarrier and moving on to the next mbarrier in the ring buffer.
\end{itemize}

\filbreak
A cornerstone of Exo-GPU is the idea that we statically enforce convergence requirements using the collective unit type system.
For example, a \lighttt{Fence} inside a scope with collective unit \lighttt{cuda\_cta\_in\_cluster} will lower to \lighttt{\_\_syncthreads()} or similar CTA-wide synchronization.
This is guaranteed to be be executed uniformly by the CTA, since no variables in such a scope can depend on \lighttt{threadIdx.x}; it is literally impossible for the user to vary control flow as a function of \lighttt{threadIdx.x} except using a sub-CTA-level \lighttt{cuda\_threads} loop, which the collective unit type system will detect.
This is even the case if we combine Exo-GPU with Chexo\footnote{Chexo allows data-dependent branching. It will still be impossible to bypass the collective unit system with this, since in a scope without access to \lighttt{threadIdx.x}-dependent Exo variables, it will be impossible to have different threads read different data. Exceptions: if the program has a race condition (which would be detected by the abstract machine simulator), then the same read could yield nondeterministic values. The user could also cook up a custom instr that leaks \lighttt{threadIdx.x}, which isn't valid Exo (the user would have to lie about the behavior of such an instr).}.

\filbreak
Similarly, the collective unit system makes sure the earlier hardware state corresponds correctly with queue barrier state.
Each time a thread collective $C$ executes an mbarrier arrive statement, that corresponds to $\abs{C}$-many CUDA threads executing \lighttt{mbarrier.arrive} on an mbarrier.
This cycles the pending thread arrival count from 0 to $\abs{C}$, so the phase advances exactly once, which corresponds to the abstract machine incrementing $Q^1$ upon interpreting the Exo arrive statement.

\filbreak
This design is undermined for multicasting.
If we wanted to synchronize all CTAs in the cluster with all other CTAs in the cluster, then we could adapt this design by multiplying the thread arrival count by the cluster size, moving the arrive/await statements up to cluster-scope, and making a queue barrier in this case correspond to per-CTA mbarrier ring buffers duplicated across the cluster.
This would statically enforce convergence requirements.

\filbreak
Unfortunately, this can't work, because real programs don't do this all-to-all cluster sync, and use irregularly-shaped dependencies like ``wait for all CTAs with the same row OR the same column'' (this would not be a collective unit in the current type system, since it's not axis-aligned, nor does it allow for subdividing the cluster into disjoint thread sets).
We need to program dependencies at a finer granularity.
With the current Exo-GPU design, the only way to program this is to be able to directly refer to the CTA-dependent variables (\greenBox{\texttt{m\_cta}} and \violetBox{\texttt{n\_cta}} in the ongoing example) and that means the synchronization statements have to appear under the per-CTA loops that define those variables.

\filbreak
For the current design, the plan is to keep everything as it is in the non-multicast case, except
\begin{itemize}
  \item Let $B$ be the number of queue barriers being multicast to.
  \filbreak
  \item An Exo arrive operation corresponds to each thread in $C$ executing \lighttt{mbarrier.arrive} on an mbarrier selected from each mbarrier ring buffer (queue barrier) named in the barrier expressions of the arrive statement.
  This is a total of $B\times\abs{C}$-many \lighttt{mbarrier.arrive} executions; each thread issues $B$-many, each mbarrier is the target of $\abs{C}$-many.
  \filbreak
  \item Increase the expected thread arrival count of each mbarrier to $B\times\abs{C}$
\end{itemize}

\filbreak
Note in particular that the implementation for await statements isn't changed at all.
Awaits only require threads to wait for their own CTA's mbarrier.

\filbreak
The cleanliness of the non-multicast design breaks down here, because now the phase of an mbarrier increments only as a result of $B$-many Exo arrive statements, issued from $B$-many different CTAs, instead of just 1.
This motivates the convergence requirement: we need to ensure either all CTAs arrive on the mbarrier, or none of them do, so we don't leave mbarriers in some weird partial state.
This may make more sense in the example of the next section.

\filbreak
\minorSub{Divergence Issues}

We'll use this simplified 2-CTA example\footnote{This example doesn't exhibit the irregular synchronization pattern that demotivates the ``full cluster sync'' approach, but this is not relevant to the issue illustrated.} to illustrate the convergence requirement's importance:

\input{su_samples/multicast_2cta.0.tex}

\filbreak
The hardware state is
\begin{itemize}
  \item $B = 2$ (number of queue barriers, i.e. per-CTA mbarrier ring buffers).
  \filbreak
  \item Each Exo \lighttt{Arrive} operation causes $\abs{C}$-many \lighttt{mbarrier.arrive} instructions to be executed on an mbarrier from each CTA (so $2\abs{C}$ \lighttt{mbarrier.arrive} in total).
  \filbreak
  \item Expected thread arrival count is $2\abs{C}$.
\end{itemize}

\filbreak
The abstract machine will have state like this:

\input{su_samples/multicast_2cta.1.tex}

\filbreak
However, this doesn't correspond to the actual hardware behavior:

\input{su_samples/multicast_2cta.2.tex}

\filbreak
Note: here we surgically added an if statement directly branching on the CTA index.
In principle we could be lenient towards if and seq-for loops that don't depend on the CTA index.
This would not interact too well with Chexo, however, where dependencies can be indirect.

\filbreak
Earlier, we noted that incrementing $Q^1$ for only the home queue barrier isn't very accurate to the hardware.
In a way, a more hardware-centric model would be to increment $Q_n^1$ by $1/B$ for each queue barrier $Q_n$ involved in the arrive operation, as each Exo arrive operation contributes $1/B$ of the $B\times\abs{C}$ threads needed to advance the phase of an mbarrier.
But given the convergence requirement, the effect of incrementing $Q^1$ exactly once is the same as incrementing it by $1/B$, $B$-many times.

\begin{figure*}[!b]
\input{su_samples/multicast_loop_nest.1.tex}
\\
\\
\input{su_samples/multicast_loop_nest.0.tex}
\caption{Could we replace the bracketed sections with compound statements that don't involve parallel for loops?} \label{fig:multicast_loop_nest}
\end{figure*}

\filbreak
\minorSub{Opinion}

I'm really not a fan of this design because it seriously undermines one of the core ideas I'm peddling with Exo-GPU type system.
Probably I'm going to have to accept a ``worse is better'' approach here.
In a way, the cleanest design would be to have some sort of ``compound statement'' that appears at cluster scope and an alternate way to program the inter-CTA dependencies, instead of parallel-for loops \textbf{(figure \ref{fig:multicast_loop_nest})}.
I do not have any concrete ideas in this direction, or even confidence that this will be worth it from a language design standpoint and not cause all sorts of other issues down the line (including the interaction with \myChapterLink{ch:tma}{TMA Instruction Barrier Parameter}).

\newpage
\mySection{TMA Instruction Barrier Parameter}
\label{ch:tma}

TMA GMEM-to-SMEM instructions specifically require an mbarrier, and multicasting information (a.k.a. CTA mask) to operate.
The special \lighttt{expect-tx} mechanism, which is separate from the normal mbarrier arrive operation, also means the TMA operation synchronizes with only a specific mbarrier, i.e. something like this

\input{su_samples/tma_2_arrives.0.tex}

is not directly implementable.

\filbreak
What I plan to do to support this is
\begin{itemize}
  \item Add support for a trailing barrier expression \lighttt{>> \{+|-|\}barrier\_variable[idxs...]} to instruction calls (only valid for TMA-to-SMEM instructions for now).
  \filbreak
  \item Extend distributed memory deductions for instrs to handle this trailing barrier expression.
  \filbreak
  \item Extend the mbarrier pairing requirement to handle TMA [see \myChapterLink{ch:mbarrier}{Exo mbarrier Pairing Requirement}]
\end{itemize}

\filbreak
\mainSub{Instr Barrier Expression}

For instructions called with syntax \lighttt{instr(...) >>} $Q$, all visibility records generated by the instruction, when executed by the abstract machine, have $(Q, Q^1)$ immediately added to their pending arrives.
$Q^1$ is \textit{not} incremented, unlike for arrive statements.
The barrier expression may contain the \lighttt{:} index (multicasting), in which case the previous is done for each $Q$ named in the barrier expression.

\filbreak
Mbarriers will not support \lighttt{tma\_to\_smem} instr-tl in any supported first sync timeline\footnote{Exo-GPU as of 2025-06-16 does allow \lighttt{tma\_to\_smem\_async} as the first actor kind of an mbarrier; this isn't supposed to exist once the instr barrier parameter gets implemented.}, so a TMA-to-SMEM instruction will never synchronize-with a future arrive statement.

\filbreak
\mainSub{Distributed Memory Deduction}

Unlike for synchronization statements, we do not need to disambiguate \lighttt{:} indices.
Instead, we can use \lighttt{distribute(...)} statements in the instruction's \lighttt{behavior} specification to complete distributed memory deduction.

\input{su_samples/tma_instr_barrier.0.tex}

(TODO syntax for non-contiguous CTAs, e.g. every other CTA)

\filbreak
If the barrier argument is absent from the instruction definition (which is the case for all non-TMA-to-shared instructions), then trailing barrier expressions get diagnosed as a compile time error.
TODO it's a bit weird how the barrier appears inside the argument list for \lighttt{behavior(...)}, but outside as a trailing \lighttt{>> $Q$} in the instr call site ... my motivation for the \lighttt{>>} was to emphasize the non-normalness of barriers (they're meaningless in S-semantics), but maybe I'm just trying to look cool.

\newpage
\mySection{Exo mbarrier Pairing Requirement}
\label{ch:mbarrier}

Arrive-indexed queue barriers lower to commit groups trivially.
The process for lowering await-indexed queue barriers to mbarriers is more involved, as the mbarrier does not hold any state corresponding to the queue arrive count $Q^1$.
We will emulate this state using a ring buffer of mbarriers; informally, this means we have to limit the ``spread'' between the number of arrives and the number of awaits, so we don't overrun the ring buffer.

For a \lighttt{@CudaMbarrier} barrier variable with a back queue barrier array, the pairing requirement for each queue barrier $Q$ is
\begin{itemize}
  \item The $n^{th}$ \lighttt{Arrive} on $Q$ cannot \textit{finish} until the $n^{th}$ \lighttt{Await} on $\widehat{Q}$ finishes.
  \filbreak
  \item The $n^{th}$ \lighttt{Arrive} on $\widehat{Q}$ cannot \textit{finish} until the $n^{th}$ \lighttt{Await} on $Q$ finishes.
\end{itemize}
(these two statements are actually exactly the same).
\filbreak
If there is no back queue barrier array, the pairing requirement for each queue barrier $Q$ is:
\begin{itemize}
  \item The $n^{th}$ \lighttt{Arrive} on $Q$ cannot \textit{finish} until the $(n-1)^{th}$ \lighttt{Await} on $Q$ finishes.
\end{itemize}

\filbreak
``finish'' means that \textit{all} threads in the arriving/awaiting thread collective have passed the arrive/await statement.
See ``mbarrier Ring Buffer'' for why this requirement is needed for the ring buffer.

\filbreak
To enforce this, we require that each thread using the mbarrier follows one of these expected usage patterns; in all cases, the await-type statements ``guard'' the arrive-type statements and prevent there from being too many finished arrives:
\begin{itemize}
  \item \lighttt{Await} on $Q$, followed by a (possibly conditional) \lighttt{Arrive} on $\widehat{Q}$ [await-first usage]
  \filbreak
  \item If there is no back queue barrier array, then we support \lighttt{Await} on $Q$, followed by a (possibly conditional) \lighttt{Arrive} on $Q$ [await-first usage]
  \filbreak
  \item If there is no back queue barrier array, we also support \lighttt{Arrive} on $Q$ with $N=1$, followed by an unconditional \lighttt{Await} on $Q$ [arrive-first usage]
  \item (Note: in the current implementation, arrive-first and await-first patterns are not simultanously supported for a single barrier variable)
\end{itemize}
TODO: the ``conditional'' cases make no sense in light of the $Q^1 = Q^2$ requirement.

\filbreak
In all these supported cases, we require no other usage of $Q$ or $\widehat{Q}$ by the thread between the statement pair (except the special TMA case, to be discussed).
Unlike the other synchronization requirements, which are enforced by ``simulation'', this ``meta-synchronization'' requirement will be enforced by static analysis.
These rules allow us to safely lower queue barriers to mbarrier ring buffers.

\mainKey{Static Analysis:} for each \lighttt{Arrive} statement using $Q$, search forward/backwards \textit{syntactically} for the next/previous statement using $Q$ or $\widehat{Q}$ (forwards in the arrive-first case; backwards in the await-first case).
\filbreak
\begin{itemize}
  \item If the sync statement is not the correct sort of \lighttt{Await} statement, the analysis fails.
  \filbreak
  \item If the sync statement's \lighttt{CollTiling} is not equivalent to that of the \lighttt{Arrive}, the analysis fails (this is asserting the set of threads executing the two statements is the same).
  \filbreak
  \item If the \lighttt{Await} is guarded by an if statement that the \lighttt{Arrive} isn't, the analysis fails.
  \filbreak
  \item If either the \lighttt{Await} or \lighttt{Arrive} is in the body of a \textit{sequential} for loop that the other isn't, the analysis fails.
  \filbreak
  \item Note, \lighttt{with} statements and parallel-for loops are ignored, as these don't have effect on a single thread's execution (given the \lighttt{CollTiling} equality check passed)
\end{itemize}

TODO this is probably not exactly what's implemented.
And we really should not be describing this in terms of how the compiler works.
Alternatively, we could give up on static analysis and just add this to the abstract machine model, although I'd rather not, to avoid polluting the model (which maybe I'll sell to theorists) with messy implementation details.

\begin{figure*}[!b]
\input{su_samples/multicast_pairing_fail.0.tex}
\caption{Example of incorrect mbarrier pairing due to mismatched CTA masks} \label{fig:multicast_pairing_fail}
\end{figure*}

\begin{figure*}[!b]
\input{su_samples/tma_pairing_multicast.0.tex}
\caption{TMA instructions \& mbarrier pairing} \label{fig:tma_pairing_multicast}
\end{figure*}

\filbreak
\mainSub{Multicasting \& mbarrier Pairing}

The \lighttt{Arrive} statement may multicast to multiple CTAs.
If a barrier variable contains a back queue barrier array, then the set of CTAs multicast to for \lighttt{Arrive} statements must be the same for both queue barrier arrays \textbf{(figure \ref{fig:multicast_pairing_fail})}.
TODO unclear if this is actually needed.

\filbreak
\mainSub{TMA Instructions \& mbarrier Pairing}

TMA-to-SMEM instructions, which are the only instructions taking an mbarrier parameter, must appear only between a mbarrier-paired \lighttt{Await} and \lighttt{Arrive} statement.
Any control flow is permitted.
The queue barriers used by the TMA instructions must be a subset of those used in the \lighttt{Arrive} statement \textbf{(figure \ref{fig:tma_pairing_multicast})}.

\filbreak
\minorSub{Hardware Detail: mbarrier Ring Buffer}

Mbarriers do not directly encode anything akin to the Exo queue barrier's $Q^1$ and $Q^2$ (queue arrive count, queue await count).
We implement a queue barrier as a ring buffer of mbarriers.
Recall that
\begin{itemize}
  \item \lighttt{Await($Q$, ..., \textasciitilde lag)} waits for the $(Q^2 - \lighttt{lag})^{th}$ \lighttt{Arrive(...) >> $Q$} (await-indexed queue barrier).
  \filbreak
  \item The first \lighttt{lag}-many awaits are trivial (wait for nothing; no CUDA mbarrier operations)
  \filbreak
  \item The subsequent awaits are non-trivial (implemented with CUDA mbarrier waits)
\end{itemize}
\filbreak
Ignoring TMA and tx-count, each CUDA mbarrier contains
\begin{itemize}
  \item An expected thread\footnote{The ``thread'' is not part of the official CUDA name; I add this prefix to disambiguate with $Q^1$ (queue arrive count).} arrival count
  \filbreak
  \item A pending thread arrival count (initially 0)
  \filbreak
  \item A phase bit
\end{itemize}
Exo initializes the expected thread arrival count with the box size of the arriving collective unit.
In raw CUDA, any number of threads may wait for an mbarrier, so the awaiting collective unit isn't directly used.

\filbreak
The way the mbarrier works is each time a thread arrives on an mbarrier, it atomically
\begin{itemize}
\item Increments the pending thread arrival count
\filbreak
\item If the pending thread arrival count equals the expected arrival count, flip the phase bit and reset the pending thread arrival count to 0
\end{itemize}
\filbreak
With $r$ being the size of the mbarrier ring buffer and $C^1$ being the Exo arriving thread collective, the $Q^1$-th Exo arrive on a queue barrier corresponds to all threads in $C^1$ arriving on the $(Q^1 \% r)^{th}$ slot in the ring buffer (hence flipping the phase bit).
Awaiting threads (in thread collective $C^2$) detect the arrive has finished by checking that the phase bit is as expected.

\filbreak
This works based on induction, given $n-1$ non-trivial Exo awaits have already completed, we can detect the $n^{th}$ non-trivial Exo await just by distinguishing it from the previous state with the phase bit.
However, this will fail if the mbarrier phase bit flips again before all threads in $C^2$ detect this.
With the ring buffering, this would be the case when the $(n+r)^{th}$ arrive completes (i.e. all threads in $C^1$ execute the arrive instruction).

\filbreak
Hence, we need to guarantee that
\begin{equation*}
  n^{th} \text{ \textit{non-trivial} await on $Q$ not finished} \implies (n+r)^{th} \text{ arrive on $Q$ not finished}
\end{equation*}
or the contrapositive
\begin{equation*}
  (n+r)^{th} \text{ arrive on $Q$ finished} \implies n^{th} \text{ \textit{non-trivial} await on $Q$ finished}
\end{equation*}

\filbreak
where the $n^{th}$ operation being ``finished'' means all threads in $C^1$ or $C^2$ have passed the $n^{th}$ arrive or await operation, respectively.\footnote{I'm being really sloppy here, because the whole point of the Exo-GPU abstract machine is that threads barely exist at all. But the focus of this section is the limitations of lowering to CUDA code, so I'm giving it a free pass for now.}

\filbreak
\minorKey{Case 1: No Back Queue Barrier Array:}
The mbarrier pairing requirement guarantees
\begin{equation*}
  x^{th} \text{ arrive on $Q$ finished} \implies (x-1)^{th} \text{ await on $Q$ finished}
\end{equation*}

\filbreak
Use the definition of \textit{non-trivial} await:
\begin{equation*}
  x^{th} \text{ arrive on $Q$ finished} \implies (x-1-\lighttt{lag})^{th} \text{ non-trivial await on $Q$ finished}
\end{equation*}

\filbreak
Substitute $x=n+r$ and $r=\lighttt{lag}+1$ (the needed ring buffer size) to get the required guarantee:
\begin{equation*}
  (n+r)^{th} \text{ arrive on $Q$ finished} \implies n^{th} \text{ non-trivial await on $Q$ finished}
\end{equation*}

\filbreak
\minorKey{Case 2: Back Queue Barrier Array:}
Let \lighttt{F\_lag} be the \lighttt{lag} value used for the front \lighttt{Await} statements and \lighttt{B\_lag} be the \lighttt{lag} values used for the back \lighttt{Await} statements ($N =$ \lighttt{\textasciitilde lag}).
Assume here that $Q$ denotes a queue barrier from the front queue barrier array.

\filbreak
The mbarrier pairing requirements guarantee that
\begin{itemize}
  \item $x^{th}$ arrive on $Q$ finished $\implies$ the $x^{th}$ await on $\widehat{Q}$ finished
  \filbreak
  \item $y^{th}$ arrive on $\widehat{Q}$ finished $\implies$ the $y^{th}$ await on $Q$ finished
\end{itemize}
\filbreak
which we rewrite using the definition of non-trivial await:
\begin{enumerate}
  \item $x^{th}$ arrive on $Q$ finished $\implies$ the $(x-\lighttt{B\_lag})^{th}$ non-trivial await on $\widehat{Q}$ finished
  \filbreak
  \item $(x-\lighttt{B\_lag})^{th}$ non-trivial await on $\widehat{Q}$ finished $\implies$ the $(x-\lighttt{B\_lag})^{th}$ arrive on $\widehat{Q}$ finished
  \filbreak
  \item $y^{th}$ arrive on $\widehat{Q}$ finished $\implies$ the $(y-\lighttt{L\_lag})^{th}$ non-trivial await on $Q$ finished
  \filbreak
  \item $(y-\lighttt{L\_lag})^{th}$ non-trivial await on $Q$ finished $\implies$ the $(y-\lighttt{L\_lag})^{th}$ arrive on $Q$ finished
\end{enumerate}
where (b) and (d) were newly inserted.

\filbreak
Let $r = \lighttt{F\_lag} + \lighttt{B\_lag}$ (the needed ring buffer size) and substitute $x = n + r$, $y = n + \lighttt{L\_lag}$ and chain (a), (b), and (c) together to conclude:
\begin{equation*}
  (n+r)^{th} \text{ arrive on $Q$ finished} \implies n^{th} \text{ non-trivial await on $Q$ finished}
\end{equation*}
Substitute $x = n + \lighttt{B\_lag}$, $y = n+r$, to get the same conclusion from (c), (d), and (a) for $\widehat{Q}$.

\newpage
\mySection{Abstract Machine Grammar}
\label{ch:AbstractMachineGrammar}

\mainSub{Timeline Types}

\input{su_samples/tl_grammar.0.tex}

\filbreak
\mainSub{Abstract Machine Instructions}

\input{su_samples/abstract_machine_instructions.0.tex}

\filbreak
\mainSub{Abstract Machine Variables}

\input{su_samples/abstract_machine_variables.0.tex}

\filbreak
\mainSub{Exo to Abstract Machine}

The correspondence between Exo code and the abstract machine code isn't fully spec'd out yet, but the rough correspondence is

\begin{itemize}
  \item Each statement is executed by some thread collective \lighttt{int* C}, as defined by \lighttt{coll\_algebra.pdf}.
    This thread collective will be an additional input in the transition functions defined later.
  \filbreak
  \item Each live scalar value of data type (either a scalar data variable, or a scalar inside a tensor) comprises a \textit{position},
    which holds both a numeric value and metadata for synchronization checking.
    The numeric values are manipulated by read, assign, and reduce statements like in Exo.
    The synchronization metadata gets manipulated by \lighttt{Check*} statements added prior to data allocation, free, read, and modification statements,
    unless the position is \textit{sync-exempt} (to be discussed).
  \filbreak
  \item Control types are basically unchanged from Exo.
  \filbreak
  \item Allocations and frees for barrier types get replaced with \lighttt{AllocBarrierArray} and \lighttt{FreeBarrierArray}.
\end{itemize}

\filbreak
\mainKey{Sync-exempt:} Positions allocated in memory types that we are confident won't be involved in a race condition are sync-exempt.
Most importantly, this is the case for grid constant memory, since Exo has no way to make explicit the copy from CPU to GPU that happens in the CUDA driver (this copy is neither properly on the CPU timeline or any CUDA timeline).
For performance reasons, we might also do this for non-wgmma register types.

\filbreak
\mainKey{Data Allocation:} A shard of a data allocation is identified by the variable's name plus values for the distributed indices.
Each allocation of a non-sync-exempt data variable is followed by calls to \lighttt{CheckAllocShard} for each distributed shard, with \lighttt{C} being the thread collective that owns the shard.

\filbreak
\mainKey{Data Free:} Each free of a non-sync-exempt data variable is followed, \textit{after a Memory-type-defined delay}, with calls to \lighttt{CheckFreeShard} for each distributed shard, with \lighttt{C} being the thread collective that owns the shard.
For shared memory types, the free is deferred until the next shared memory allocation, or the end of the scope, whichever comes first.

\filbreak
\mainKey{Data Read:} Prior to statements that include a read of a non-sync-exempt position, issue a \lighttt{CheckRead} for that position, with the instr-tl defined by the current scope, and usage-tl and the out-of-order flag defined by the memory type defaults.

\filbreak
\mainKey{Data Mutate:} Prior\footnote{With strict control and data value separation, whether this is prior (as specified) or following the assign/reduce statement is immaterial, but with Chexo, this could be important, in case the mutation is to a variable used as an index value within the mutation statement.}
to an assign or reduce to a non-sync-exempt position, issue a \lighttt{CheckMutate} for that position, with instr-tl, usage-tl, and the out-of-order flag defined as for a data read.

\filbreak
\mainKey{Instr Call:} Add \lighttt{CheckInstr} prior to each call to an instruction, using the instruction info for the called instruction, and the same args as the call.

The optional \lighttt{barrier} value is initialized from the optional trailing barrier expression; see \myChapterLink{ch:tma}{TMA Instruction Barrier Parameter}.

\filbreak
\mainKey{Instr Info:} The \lighttt{name}, \lighttt{basetype}, and \lighttt{shape} are as defined by the instruction's \lighttt{behavior()} specification.
The instr-tl, usage-tl, extended instr-tl, extended usage-tl, and out-of-order flag are defined by the instruction's \lighttt{instance()} function.
The mode is \lighttt{ExemptArg()} for control variables and sync-exempt memory types, otherwise \lighttt{MutateArg()} if the instr modifies the argument, otherwise \lighttt{ReadArg()}.
TODO \lighttt{access\_by\_owner\_only}.

\filbreak
\mainKey{Synchronization:} Currently each Exo barrier variable corresponds to either one or two queue barrier arrays, depending on whether the back queue barrier array (\lighttt{-varname}) exists.
Replace each alloc/free of a barrier type variable with \lighttt{AllocBarrierArray} and \lighttt{FreeBarrierArray} issued for the queue barrier array(s) corresponding to the Exo barrier variable.

\filbreak
Translate synchronization statements to Fence, Arrive, and Await literally (see \myChapterLink{ch:Multicasting}{Multicasting mbarriers}, for multicast flags).

\filbreak
\mainSub{Definitions}

With $V_1 = L_1^F \times \lighttt{C}$ (F: full timeline set), a visibility record \myKeyA{synchronizes-with} ($L_1$: sync-tl, \lighttt{C}: thread collective) when the visibility record's $V_A$ and $L_O$ satisfy
\begin{itemize}
  \item $V_1 \cap V_A \ne \varnothing$, if $L_1$ is $V_1$-transitive.
  \filbreak
  \item $V_1 \cap V_A \ne \varnothing$ AND $L_O \in L_1^F$, otherwise.
\end{itemize}

\filbreak
A visibility record is \myKeyA{visible-to} a checked read, mutate, or free when, for each $t \in$ \lighttt{C}, $V_S \cap (L_X^i \times L_X^u \times \{ t \})$ is non-empty ($V_S$ being the \lighttt{sync\_visibility\_set} of the visibility record, and $L_X^i$, $L_X^u$, and \lighttt{C} being from the context of the \lighttt{CheckRead}, \lighttt{CheckMutate}, or \lighttt{CheckFreeShard}).

\filbreak
A visibility record is \myKeyA{augmented with} $V_2$: set of \lighttt{tl\_sig} by setting $V_S \leftarrow V_S \cup V_2$ and $V_A \leftarrow V_A \cup V_2$.

\filbreak
A program has \myKeyA{correct usage} when no assertion fails in any transition function, and look up in the environment never fails (e.g. due to out-of-bounds error).

\filbreak
Define \lighttt{\myKeyA{multicast\_ids}(queue\_barrier\_array\_id barriers, int* iterators, multicast\_flags multicast)} to be the set of \lighttt{id: queue\_barrier\_id} where
\begin{itemize}
  \item \lighttt{barriers} $=$ \lighttt{id.barriers}
  \filbreak
  \item Dimensionality of \lighttt{id.barriers} matches that of \lighttt{iterators} and \lighttt{multicast.flags}.
  \filbreak
  \item For each \lighttt{i}, \lighttt{iterators[i]} $=$ \lighttt{id.iterators[i]}, or \lighttt{multicast.flags[i]} is true.
\end{itemize}

\filbreak
\mainSub{Transition Functions}

The environment maps \lighttt{position\_id} to \lighttt{position} variable values, and \lighttt{queue\_barrier\_id} to \lighttt{queue\_barrier} variable values.
Besides the enviroment, the transition functions take an executing thread collective \lighttt{C}.

\filbreak
We don't define the behavior of statements that only affect the \lighttt{numeric\_value} of a position; we inherit this behavior from Exo.
Actually, if there's strict separation of control and data values, then for the purposes of checking correct usage of the abstract machine, we don't have to care about data values at all.
However, if we merge this with Chexo, then we will have to care about a subset of values, those integer data values used to index tensors.

\filbreak
\mainKey{\texttt{AllocBarrierArray(barriers, shape)}:} For each integer tuple \lighttt{idx} of the same dimension as \lighttt{shape} with \lighttt{0 $\le$ idx[i] $<$ shape[i]}, initialize the variable value for \lighttt{queue\_barrier\_id(barriers, idx)} to \lighttt{queue\_barrier(0, 0)}.

\filbreak
\mainKey{\texttt{FreeBarrierArray(barriers)}:} For each \lighttt{id: queue\_barrier\_id} with \lighttt{id.barriers} = \lighttt{barriers}, \myKeyA{assert} that the \lighttt{queue\_barrier} value for \lighttt{id} has $Q^1 = Q^2$. (\lighttt{arrive\_count = await\_count}).

\filbreak
\mainKey{Exo Alloc:} When allocating a new position with a normal Exo allocation statement, initialize all positions in the allocated scalar or tensor to an undefined \lighttt{numeric\_value}, an empty \lighttt{assignment\_record}, and empty \lighttt{owner\_tids}.

\filbreak
\mainKey{\texttt{CheckAllocShard(name, distributed\_iterators)}:} For each \lighttt{id: position\_id} with \lighttt{id.name} = \lighttt{name} and with \lighttt{distributed\_iterators} being a prefix of \lighttt{id.idx}, look up the \lighttt{position} variable value for \lighttt{id} and set \lighttt{owner\_tids} to \lighttt{C} (thread collective that owns this distributed shard).

\filbreak
\mainKey{\texttt{CheckFreeShard(name, distributed\_iterators, $L_X^i$, $L_X^u$)}:} For each \lighttt{id: position\_id} with \lighttt{id.name} = \lighttt{name} and with \lighttt{distributed\_iterators} being a prefix of \lighttt{id.idx}, look up the \lighttt{val: position} for that \lighttt{id} and
\begin{itemize}
  \item \myKeyA{assert} \lighttt{C} = \lighttt{val.owner\_tids}.
  \filbreak
  \item If \lighttt{val.assignment\_record.mutate} exists, \myKeyA{assert} that it is visible-to the free.
  \filbreak
  \item For each \lighttt{read\_vis\_record} in \lighttt{val.assignment\_record.reads}, \myKeyA{assert} that it is visible-to the free.
\end{itemize}

\filbreak
\mainKey{\texttt{CheckRead(name, idx, $L^i$, $L^u$, $L_X^i$, $L_X^u$, $OOO$)}:} Look up \lighttt{val: position} and,
\begin{itemize}
  \item If \lighttt{val.assignment\_record.mutate} exists, \myKeyA{assert} that it is visible-to the read.
  \filbreak
  \item Let $V_A = \{ L^i \} \times \{ L^u \} \times \lighttt{C}$
  \filbreak
  \item Let $V_S = \varnothing$ if $OOO$, otherwise $V_S = V_A$.
  \filbreak
  \item Add \lighttt{read\_vis\_record}$(V_S, V_A, L^i, \varnothing)$ to \lighttt{val.assignment\_records.reads}, where the last $\varnothing$ is an empty set of \lighttt{pending\_arrives}.
\end{itemize}

\filbreak
\mainKey{\texttt{CheckMutate(name, idx, $L^i$, $L^u$, $L_X^i$, $L_X^u$, $OOO$)}:} Look up \lighttt{val: position} and,
\begin{itemize}
  \item If \lighttt{val.assignment\_record.mutate} exists, \myKeyA{assert} that it is visible-to the mutate.
  \filbreak
  \item For all visibility records in\lighttt{val.assignment\_record.reads}, \myKeyA{assert} that it is visible-to the mutate.
  \filbreak
  \item Let $V_A = \{ L^i \} \times \{ L^u \} \times \lighttt{C}$
  \filbreak
  \item Let $V_S = \varnothing$ if $OOO$, otherwise $V_S = V_A$.
  \filbreak
  \item Set \lighttt{val.assignment\_record.mutate} to \lighttt{mutate\_vis\_record}$(V_S, V_A, L^i, \varnothing)$, where the last $\varnothing$ is an empty set of \lighttt{pending\_arrives}.
  \filbreak
  \item Clear \lighttt{val.assignment\_record.reads}.
\end{itemize}

\filbreak
\mainKey{\texttt{CheckInstr(instr\_info, args, ?barrier)}:} For each \lighttt{args[i]} of the instr call, if \lighttt{instr\_info.arg\_info[i].mode} is not \lighttt{ExemptArg()}, do, for each \lighttt{id: position\_id} naming a position passed in the window expression for the arg,
\begin{itemize}
  \item Look up \lighttt{val: position} for \lighttt{id}.
  \filbreak
  \item For the upcoming uses of visible-to, let $L^i$: instr-tl be that of \lighttt{instr\_info}, and let $L^u, L_X^i, L_X^u, OOO,$ and \lighttt{mode} be that of \lighttt{instr\_info.arg\_info[i]}.
  \filbreak
\item If \lighttt{instr\_info.arg\_info[i].access\_by\_owner\_only}, then let \lighttt{C} $=$ \lighttt{val.owner\_tids} for the uses of visible-to below; otherwise, inherit \lighttt{C} from the calling environment.
  \filbreak
  \item If \lighttt{val.assignment\_record.mutate} exists, \myKeyA{assert} that it is visible-to the read/mutate through the instr.
  \filbreak
  \item If \lighttt{mode} is \lighttt{MutateArg()}, for each read visibility record in \lighttt{val.assignment\_record.reads}, \myKeyA{assert} that it is visible-to the mutate.
  \filbreak
  \item Let $V_A = \{ L^i \} \times \{ L^u \} \times \lighttt{C}$ if no \lighttt{barrier} was specified, otherwise $V_A = \varnothing$.
  \filbreak
  \item Let $V_S = \varnothing$ if $OOO$, otherwise $V_S = V_A$.
  \filbreak
  \item Let $A_p = \varnothing$ if no \lighttt{barrier} was specified; otherwise, look up $Q$ (home queue barrier) from \lighttt{queue\_barrier\_id(barriers.barriers, barriers.iterators)}, and let\\
    $A_p = \{ (q, Q^1) \mid q \in \lighttt{multicast\_ids(barriers.barriers, barriers.iterators, barriers.multicast)} \}$
  \filbreak
  \item If \lighttt{mode} is \lighttt{ReadArg()}, add \lighttt{read\_vis\_record}$(V_S, V_A, L^i, A_p)$ to\\ \lighttt{val.assignment\_records.reads}.
  \filbreak
  \item If \lighttt{mode} is \lighttt{MutateArg()}, set \lighttt{val.assignment\_records.mutate} to\\ \lighttt{mutate\_vis\_record}$(V_S, V_A, L^i, A_p)$, and clear \lighttt{val.assignment\_records.reads}.
\end{itemize}

\filbreak
\mainKey{\texttt{Fence($L_1$, $L_2$)}:} For each \lighttt{asn: assignment\_record} in the entire environment,
\begin{itemize}
  \item Define $V_2^F = L_2^F \times \lighttt{C}$ (full timeline set).
  \filbreak
  \item Define $V_2^T = L_2^T \times \lighttt{C}$ (temporal timeline set).
  \filbreak
  \item If \lighttt{asn.mutate} exists and it synchronizes-with $L_1, \lighttt{C}$, augment it with $V_2^F$.
  \filbreak
  \item For each read visibility record in \lighttt{asn.reads} that synchronizes-with $L_1, \lighttt{C}$, augment it with $V_2^T$.
\end{itemize}

\filbreak
\mainKey{\texttt{Arrive($L_1$, $N$, barriers, iterators, multicasts):}} For each \lighttt{asn: assignment\_record} in the entire environment,
\begin{itemize}
  \item Assume $N=1$ (we reserved $N$ for future use if needed).
  \filbreak
  \item Look up $Q$ (home queue barrier) from \lighttt{queue\_barrier\_id(barriers, iterators)}.
  \item Let $A = \bigcup_{\blacktt{M} \in \blacktt{multicasts}} \{ (q, Q^1) \mid q \in \lighttt{multicast\_ids(barriers, iterators, M)} \}$.
  \filbreak
  \item If \lighttt{asn.mutate} exists and it synchronizes-with $L_1, \lighttt{C}$, set $A_p \leftarrow A_p \cup A$.
  \filbreak
  \item For each read visibility record in \lighttt{asn.reads} that synchronizes-with $L_1, \lighttt{C}$, set $A_p \leftarrow A_p \cup A$.
\end{itemize}
Finally, increment $Q^1$ (arrive count).

\filbreak
\mainKey{\texttt{Await(barriers, iterators, $L_2$, $N$):}} For each \lighttt{asn: assignment\_record} in the entire environment,
\begin{itemize}
  \item Define $V_2^F = L_2^F \times \lighttt{C}$ (full timeline set).
  \filbreak
  \item Define $V_2^T = L_2^T \times \lighttt{C}$ (temporal timeline set).
  \filbreak
  \item Let $q =$ \lighttt{queue\_barrier\_id(barriers, iterators)}.
  \filbreak
  \item Look up $Q$: \lighttt{queue\_barrier} from $q$.
  \filbreak
  \item If $N \ge 0$ (arrive-indexed), let $A = \{ (q, x) \mid 0 \le x < Q^1 - N, x \in \mathbb{Z} \}$.
  \filbreak
  \item If $N < 0$ (await-indexed), let \lighttt{lag = \textasciitilde}$N$, $A = \{ (q, x) \mid 0 \le x \le Q^2 - \lighttt{lag} \}$.
  \filbreak
  \item If \lighttt{asn.mutate} exists and satisfies $\lighttt{asn.mutate.}A_p \cap A \ne \varnothing$, then augment it with $V_2^F$.
  \filbreak
  \item For each read visibility record $v$ in \lighttt{asn.reads} that satisfies $v.A_p \cap A \ne \varnothing$, augment it with $V_2^T$.
\end{itemize}
Finally,
\begin{itemize}
  \item If $N \ge 0$ (arrive-indexed), set $Q^2 \leftarrow \text{max}(Q^1 - N, Q^2)$.
  \item If $N < 0$ (await-indexed), set $Q^2 \leftarrow Q^2 + 1$.
\end{itemize}

\end{document}
