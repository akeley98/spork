% python3 code_to_tex.py su_samples.py su_samples && xelatex summer_of_wgmma.tex </dev/null
\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{arraynode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{packednode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{featurenode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{memnode} = [rectangle, minimum width=2.25cm, minimum height=1cm, text centered, text width=2.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Summer Plans for Spork}

For now I'm just focusing on proposed changes to the Exo object language.
I also have to figure out how to update the synchronization checker to handle new features, and glue it to Exo.
After that, implementing algorithms in Exo-GPU and performance results (sounds like part of this work may be outsourced to Jason).

New/changed features:

\begin{itemize}
\item Instructions are currently conveyed as Python functions; as an alternative, we should also support ``instruction templates'' conveyed as Python classes.
These instruction templates can be instantiated to create a family of similar instrs, and give us more room syntactically to fit all the instruction metadata needed for Exo-GPU.
\item Move away from string interpolation and towards more ``structured'' interfaces for generating windows and C/C++ instruction syntax.
\item Changes to distributed memory to support multicasting and warp-shuffle instructions, i.e., cases where it's no longer true that each distributed shard is accessed solely by its owner.
\item Possibly, replace the actor kind idea with timelines: \textit{instruction timeline} (\lighttt{instr\_tl}) and \textit{usage timeline} (\lighttt{usage\_tl}), which are per-instruction and per-instruction-parameter attributes, respectively.
\item Improvements to Arrive/Await statements to support TMA and multicast: we need to allow arriving on multiple barriers, and associating a barrier with a specific instruction, and no prior instructions, in an instruction stream.
\end{itemize}

\newpage
\myTitle{Instr Class}

``New-style'' instruction templates are created as \lighttt{@instr}-decorated Python classes, rather than functions.
The class must have two functions
\begin{itemize}
  \item \lighttt{behavior}: Exo code specifying the behavior of the function.
  \item \lighttt{instance}: Python code executed by the compiler and codegen.
\end{itemize}

\input{su_samples/instr_class.0.tex}

\filbreak
The decorated class is not itself an instr, but an instr template.
All parameters in common between the \lighttt{behavior} and \lighttt{instance} functions (in this case, \lighttt{K}) must be substituted with concrete values.
These are ``template parameters''.
The other \lighttt{behavior} parameters are ``runtime parameters''.

\input{su_samples/instr_class.1.tex}

\filbreak
Besides allowing code-reuse through the template mechanism and class inheritance, conveying the \lighttt{instr} as a class gives us more room syntactically (in the \lighttt{instance} function) to convey the large amount of metadata for Exo-GPU instructions:
\begin{itemize}
  \item collective unit
  \filbreak
  \item include files and ``utils'' for C and CUDA C++
  \filbreak
  \item instruction timeline (n\'ee actor kind)
  \filbreak
  \item per-parameter information: usage timeline (``actor signature''), sync/async access
\end{itemize}

\filbreak
\mainSub{Exo Syntax}

Pass template parameters as keyword arguments, and runtime parameters as positional:

\input{su_samples/instr_class.2.tex}

\filbreak
\mainSub{LoopIR\_Unification Changes}

For convenience, usually the user shouldn't substitute template parameters manually.
Instead, we should program \lighttt{replace()} to be able to take an instr template, and deduce and substitute template parameters as part of unification.
It's an error if the substituted value is not a constant.

\filbreak
\mainSub{Include Files \& Utility Code}

We will have a lot of similar instructions that may share snippets of useful ``global'' source code.
As a replacement for \lighttt{c\_global}, the \lighttt{instance} function of an instr class configures these attributes defining source code needed for compiling the instrs:
\begin{itemize}
\item \lighttt{c\_includes: List[str]}: names of include files needed in C code (the \lighttt{.c} file)
\item \lighttt{c\_utils: List[str]}: C code snippets
\item \lighttt{cu\_includes: List[str]}: names of include files needed in CUDA C++ code (the \lighttt{.cuh} file)
\item \lighttt{cu\_utils: List[str]}: CUDA C++ code snippets
\end{itemize}
Duplicate include files/utils are discarded.

\filbreak
Because the header file (\lighttt{.cuh}) for the generated CUDA code is not really private, all the CUDA utilities are placed into a ``unique'' namespace (uniquify based on Python source file name).
This is aliased as \lighttt{namespace exo\_CudaUtil} in generated CUDA functions.

\filbreak
\mainSub{Instr Format Function}

The current system of generating C (or CUDA) code for instrs using \lighttt{str.format} is really unwieldly.
As an alternative, let's support codegen with a Python function that's given information on all runtime parameters and returns a C/C++ string.
This ties into my proposed changes to the window system that allow deeper introspection of windows.

\newpage
\myTitle{Make Windows Better}

Windows serve two purposes in Exo
\begin{itemize}
 \item In the common usage, we use windows to pass slices of tensors to instrs.
 Typically, the instr's code format does not actually create a window, and just formats using \lighttt{\textit{arg\_name}\_data}.
 I call this a \myKeyA{non-encoded} window.
 \item Much less commonly, we actually \myKeyA{encode} a window to a window struct, such as when we cross a true function boundary, or we compile a \lighttt{WindowStmt} (can of worms I won't get into in this document).
\end{itemize}
\filbreak
The uncommon encoded window case puts artificial constraints on our thinking.
I propose we make it much easier to unpack and inspect attributes of a window within the Exo compiler, with limited (or no) support for encoding the window, depending on memory type.
Currently, this is only possible for the limited case of \lighttt{\textit{arg\_name}\_data} (inspecting the offseted pointer for a window) and the weird \lighttt{\textit{arg\_name}\_int} thing.

\filbreak
Furthermore, we should have language-level support for expressing the different ``hardware modes'' for tensor dimensions (distributed memory, array, packed/vectorized), and enforcing commonplace restrictions (e.g. that an AVX vector isn't strided).
I think this is better than trying to repeatedly implement hacky assertions with \lighttt{stride}, etc., in \lighttt{Memory}/\lighttt{SpecialWindow} implementations.

\filbreak
\mainSub{Dimension Hardware Modes}

From left-to-right, a tensor's dimensions consist of \redBox{distributed}, \yellowBox{array}, and \blueBox{packed} (``vectorized'') dimensions.
The distributed dimensions are deduced by Exo-GPU, and mostly hidden from the window implementation; the Exo-GPU compiler rewrites the subtree to eliminate distributed dimensions prior to C/C++ codegen.
The shape of the \myKeyA{packed tensor} (dimension count and expected sizes) is defined by the \lighttt{Memory} type (e.g. [8], for \lighttt{\_\_m256}).
The remaining dimensions (in the middle) are array dimensions.

\myKeyA{AVX Example:}

\input{su_samples/avx_example.0.tex}

\filbreak

\myKeyA{CUDA Example:}

\input{su_samples/warp_example.0.tex}

\filbreak
For window parameters, we can explicitly tag the hardware modes, from left-to-right:
\begin{itemize}
  \item ``\lighttt{distributed:}'' rarely used; needed for multicast and warp shuffle
  \filbreak
  \item ``\lighttt{strided:}'' array dimension, with arbitrary stride
  \filbreak
  \item ``\lighttt{dense:}'' array dimension, densely strided
  \filbreak
  \item ``\lighttt{packed:}'' packed (vectorized) dimension
\end{itemize}

\filbreak
The compiler enforces when compiling each \lighttt{WindowExpr} that \lighttt{dense} dimensions are valid (this is currently handled repeatedly for each instr, using \lighttt{assert} and \lighttt{stride} expressions).

\filbreak
\myKeyA{Backwards Compatibility:} If not tagged, query the number of packed dimensions $P$ from the \lighttt{Memory} type, and assume the right-most up-to $P$ dimensions are \lighttt{packed}, and the rest \lighttt{strided}.

\filbreak
\myKeyA{AVX Example:}

\input{su_samples/avx_window.0.tex}

\filbreak
\myKeyA{TMA GMEM Example:}

\input{su_samples/tma_window.0.tex}

\filbreak
\myKeyA{TMA SMEM Example:}

\input{su_samples/tma_window.1.tex}

\filbreak
\mainSub{Expected Patterns}

For Exo-GPU, there's several usage patterns for windows:

\filbreak
\myKeyA{Pointer + strides:} Only thing that is supported today.
Allocated in C/C++ as a flattened array.
We can freely load/store scalars, and reduce the dimensionality of a window.
``Mutable'' pointer: we do pointer arithmetic to implement offsets.
These windows may be encoded as structs.

\input{su_samples/pointer_strides.0.tex}

\filbreak
\myKeyA{Register-like:} Allocated in C/C++ as a multidimensional array.
Cannot be encoded as a window struct, or passed to non-instr procs.

\input{su_samples/register_alloc.0.tex}

\filbreak
\myKeyA{Pointer + offsets:}
These windows can be encoded as structs, but with an ``immutable'' pointer: the offsets of the window are encoded explicitly.
The dimensionality of the window can't be changed (i.e. we support only intervals, not points).
This is primarily for CUtensorMap (TMA).

\input{su_samples/pointer_offsets.0.tex}

\filbreak
\mainSub{Windows in Exo Today}

Window are extremely hard-coded in Exo today.
Regardless of the memory type, a window gets generated based on the precision, the dimensionality, and const-ness.
This window struct is defined and populated with a data pointer and strides with no involvement of the underlying memory type. For example, for 2D const f32 windows, we have:

\input{su_samples/2f2c.0.tex}

\filbreak
For memory types, there's no support for customizing encoded windows, and very limited support for non-encoded windows, where there's an implicit contract between a memory type's \lighttt{window()} function and an instr's usage of  \lighttt{\{\textit{arg\_name}\_data\}}.

\filbreak
For example, the \lighttt{AVX2} memory type has completely invalid encoded window structs (because we try to assign a \lighttt{\_\_m256} pointer to a scalar \lighttt{float} pointer), and the implicit \lighttt{window()} contract is that an instr can access a \textit{single} \lighttt{\_\_m256} vector, since the \lighttt{window()} callback interprets all offsets except the last as array indices.

\input{su_samples/avx_before.0.tex}

\filbreak
Instrs have partial ability to prevent invalid usage by using \lighttt{stride()} expressions, e.g., the AVX window \lighttt{foo[1:9,3,6]} will be rejected (after we fix the assert bug) by \lighttt{assert stride(..., 0) == 1}.

\filbreak
The \lighttt{window()} function has to know how to decode the pointer attirbute from a window struct (\lighttt{.data}), and, indeed, is provided unfiltered access to the otherwise-private \lighttt{LoopIR.Tensor} type to detect whether this is needed:

\input{su_samples/is_win.0.tex}

\filbreak
Instrs use \lighttt{str.format} to generate C syntax.
For each window parameter, the formatter receives as keyword arguments:
\begin{itemize}
  \item \lighttt{\{\textit{arg\_name}\}}: the \myKeyA{encoded window} struct.
  \filbreak
  \item \lighttt{\{\textit{arg\_name}\_data\}}: the results given by \lighttt{window()}.
    This is a restricted case of what I'll call \myKeyA{window indexing}: accessing the value at a specific position of a window.
    For example, AVX instrs can access only one specific \lighttt{\_\_m256} vector.
  \filbreak
  \item \lighttt{\{\textit{arg\_name}\_int\}}: The raw C name of the input parameter.
\end{itemize}

\filbreak
\mainSub{Externalizing Windows}

This is a bit difficult as window generation is heavily coupled with the compiler internals, e.g.,

\begin{itemize}
  \item known strides
  \filbreak
  \item \lighttt{CIR}, for generating nice index/stride arithmetic expressions
  \filbreak
  \item assertions
  \filbreak
  \item somewhat messy interaction with scalar\_refs
\end{itemize}

\filbreak
For the most part, it'll be good to continue to use these internals and avoid having to re-implement basic window features over and over again, potentially in buggy ways (precedence errors, etc.).

\filbreak
I'm proposing that we view windows as a collection of features that are internally manipulated by the compiler (e.g. adding offsets when compiling a \lighttt{WindowExpr}).
We'll provide controlled interfaces to these features for \lighttt{MemWin} and \lighttt{instr}:
\begin{itemize}
  \item \myKeyA{Data pointer:} name of the allocated tensor, or window \lighttt{data} pointer attribute
  \filbreak
  \item \myKeyA{Array strides:} strides corresponding to array dimensions (optional).
    \textbf{Potential Breaking Change:} can we specify strides in units of \myKeyA{packed tensors}, rather than scalars?
    This will make querying the stride of a packed dimension meaningless.
  \filbreak
  \item \myKeyA{Array offsets:} explicit offsets for array dimensions that need to be accounted for to access the ``true'' window relative to the given data pointer. For the current pointer+strides-style windows, these are implicitly all 0, i.e., the \lighttt{data} pointer has already been offset.
  \filbreak
  \item \myKeyA{Packed offsets:} explicit offsets for packed dimensions (optional support). This doesn't make sense for cases where elements in the packed tensor cannot be randomly accessed (e.g. AVX vectors), but it will be useful when we implement swizzled matrices (where we can in fact random access elements, but have to be careful about it).
\end{itemize}

\filbreak
The \lighttt{MemWin} type can provide three (optional) interfaces to the underlying window system
\begin{itemize}
  \item \myKeyA{\texttt{packed\_tensor\_size}}: Given a precision (e.g. f32), give the number and expected sizes of packed dimensions. e.g. for AVX2, \lighttt{f32 -> [8]}, \lighttt{f64 -> [4]}. There may be multiple supported sizes, e.g. for CUDA mma, \lighttt{f32 -> [16, 8], [16, 4]}, \lighttt{f16 -> [16, 16], [16, 8]}.
  \item \myKeyA{\texttt{WindowEncoder}}: Convert window features to/from window struct. Analgous to \lighttt{\{\textit{arg\_name}\}}.
  \filbreak
  \item \myKeyA{\texttt{WindowIndexer}}: For use in compiling instrs to C. Given window features, generate a C expression resolving to values at a certain location in the window. Analagous to \lighttt{\{\textit{arg\_name}\_data\}}.
\end{itemize}

\filbreak
\mainSub{Window Encoder}

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=0mm]
\node(alloc) [normalnode] {Allocation / non-window param};
\node(defaults) [below=of alloc] {Built-in defaults:};
\node(default_dataptr) [below=of defaults] {C variable name};
\node(default_strides) [below=of default_dataptr] {C-style N-D array strides};
\node(default_array_offsets) [below=of default_strides] {all 0s};
\node(win) [normalnode, below=of default_array_offsets, yshift=-5mm] {WindowStmt / window parameter};
\node(decode) [memnode, text width=3cm, below=of win] {MemWin: Window Decode};
\node(dataptr) [featurenode, right=of default_dataptr, xshift=16mm] {dataptr};
\node(strides) [featurenode, below=of dataptr] {strides};
\node(array0) [arraynode, below=of strides] {array offsets};
\node(array1) [arraynode, right=of array0, xshift=16mm] {array offsets};
\node(array_intervals) [arraynode, below=of array1] {array interval sizes};
\node(packed) [packednode, below=of array_intervals] {packed offsets};
\node(packed_intervals) [packednode, below=of packed] {packed interval sizes};
\node(packed_tensor) [memnode, text width=6cm, below=of packed_intervals, yshift=-2mm, xshift=30mm] {MemWin: Packed tensor size};
\node(split) [left=of packed_tensor, xshift=-8mm] {split \yellowBox{array} \& \blueBox{packed} indices};
\node(window_expr) [normalnode, left=of split, xshift=-8mm] {WindowExpr};
\node(encode) [memnode, above=of array1, xshift=22mm, yshift=22mm, text width=5cm] {MemWin: Window Encode};
\node [right=of array_intervals] {(check dimension change support)};
\node [right=of packed] {(assert all 0)};
\node [right=of packed_intervals] {(assert complete intervals)};
\draw [arrow] (alloc.west) to[in=180, out=180] (defaults.west);
\draw [arrow] (win.west) to[in=180, out=180] (decode.west);
\draw [arrow] (default_dataptr.east) to[in=180, out=0] (dataptr.west);
\draw [arrow] (default_strides.east) to[in=180, out=0] (strides.west);
\draw [arrow] (default_array_offsets.east) to[in=180, out=0] (array0.west);
\draw [arrow] (decode.east) to[in=180, out=20] (dataptr.west);
\draw [arrow] (decode.east) to[in=180, out=0] (strides.west);
\draw [arrow] (decode.east) to[in=180, out=340] (array0.west);
\draw [arrow] (array0.east) to[in=180, out=0] (array1.west);
\draw [arrow] (window_expr.east) to (split.west);
\draw [arrow] (packed_tensor.west) to (split.east);
\draw [arrow] (split.north) to[out=90, in=180] (array1.west);
\draw [arrow] (split.north) to[out=90, in=180] (array_intervals.west);
\draw [arrow] (split.north) to[out=90, in=180] (packed.west);
\draw [arrow] (split.north) to[out=90, in=180] (packed_intervals.west);
\draw [arrow] (dataptr.east) to[out=0, in=180] ($(encode.west)+(0, 0.2)$);
\draw [arrow] (strides.east) to[out=0, in=180] ($(encode.west)+(0, -0.2)$);
\draw [arrow] (array1.east) to[out=0, in=270] ($(encode.south)+(-0.2, 0)$);
\draw [arrow] ($(array_intervals.east)+(0, 0.4)$) to[out=0, in=270] ($(encode.south)+(0.2, 0)$);
\end{tikzpicture}
\caption{Window encode data flow}
\end{figure*}

Each \lighttt{MemWin} type can customize its window encoder (window struct), or declare that there is no window encoder (a compiler error gets generated if we try to materialize the window struct).

\filbreak
Like windows currently, the window struct can be customized over precision, dimensionality, and const-ness.
The window encoder has to implement
\begin{itemize}
  \item \lighttt{decode\_dataptr():} extract the pointer member of the window struct (e.g. \lighttt{\{win\}.data})
  \filbreak
  \item \lighttt{decode\_array\_offset(n):} extract the offset for the $n^{th}$ array dimension (for current pointer+stride windows, this is always 0).
  \filbreak
  \item \lighttt{decode\_array\_stride(n):} extract the stride for the $n^{th}$ array dimension (e.g. \lighttt{\{win\}.strides[\{n\}]}). This is optional; if not implemented, then no other interfaces can query the stride.
  \filbreak
  \item \lighttt{encode\_window(...):} C syntax constructing the window struct, from the data pointer, array offsets, and array strides.
  The constructor may assume all packed dimensions are indexed with complete intervals (``\lighttt{:}'' or the equivalent); we have a built-in assumption that encoded window can't point to slices of a packed tensor.
\end{itemize}

\filbreak
For pointer+stride windows, \lighttt{encode\_window} will look something like

\input{su_samples/encode_window.0.tex}

\filbreak
By default, all indices in a window expression must be intervals, so the dimensionality of a window doesn't change.
The window encoder may optionally declare support for dimensionality changes.
In this case, the window encoder will also receive \lighttt{interval\_sizes: List[Optional[\textit{C expr}]]}, giving \lighttt{None} for point indices and the interval size for interval indices.

\filbreak
Finally, we have to provide a \myKeyA{separate dataptr} mode, where the data pointer is stored separately from the rest of the window struct.
This is an alternative to implementing \lighttt{decode\_dataptr}.
We need this for \lighttt{CUtensorMap}, which has to be stored in a different memory type than the offsets (grid constant vs registers).

\input{su_samples/tma_proc.0.tex}

\filbreak
Note: all of this occurs after distributed memory deduction, and removal of distributed dimensions.

\filbreak
\mainSub{Window Indexer}

\filbreak
The window indexer receives
\begin{itemize}
  \item the dataptr
  \filbreak
  \item a sequence of array indices
  \filbreak
  \item a sequence of packed indices
  \filbreak
  \item the array strides, if supported by the window encoder
\end{itemize}

\filbreak
and needs to return a C expression giving the data at the position given by concatenating the array and packed indices.
If the total number of indices is lower than the dimensionality of the window, the indexer can assume the remaining indices are full intervals (\lighttt{:}).
This is the ``points before intervals'' assumption.

\filbreak
It's somewhat up to each \lighttt{MemWin} implementation to define what the exact behavior of the window indexer is, and what to do if there are fewer indices given than there are window dimensions (if this is allowed at all).
For example, the previous \lighttt{AVX2} memory type will likely require that \lighttt{len(array\_indices)} is 1 less than the dimension, and that \lighttt{packed\_indices} is empty
(i.e. that you can only read exactly one packed tensor, i.e., \lighttt{\_\_m256} vector, and you can't index into the vector).

\filbreak
The compiler has more information and can generate better quality errors, so, we require the indexer to declare one of the following support modes for array indices and packed indices: none, any number, or exact dimension match.
This avoids us having to write repeated (and low quality / forgotten) error checking code.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=0mm]
\node(dataptr) [featurenode] {dataptr};
\node(strides) [featurenode, below=of dataptr] {strides};
\node(array0) [arraynode, below=of strides] {array offsets};
\node(decode) [memnode, text width=3cm, below=of array0, yshift=-12mm] {MemWin: Window Decode};
\node(defaults) [below=of decode] {(or built-in defaults)};
\node(array1) [arraynode, right=of array0, xshift=16mm, yshift=8mm] {array offsets};
\node(packed1) [packednode, below=of array1] {packed offsets};
\node(array1_intervals) [arraynode, below=of packed1] {array interval sizes};
\node(packed1_intervals) [packednode, below=of array1_intervals] {packed interval sizes};
\node(array2) [arraynode, right=of array1, xshift=16mm] {array offsets};
\node(packed2) [packednode, below=of array2] {packed offsets};
\node(array2_intervals) [arraynode, below=of packed2] {array interval sizes};
\node(packed2_intervals) [packednode, below=of array2_intervals] {packed interval sizes};
\node(window_expr) [normalnode, below=of packed1_intervals, yshift=-5mm] {WindowExpr};
\node(instr) [normalnode, below=of packed2_intervals, yshift=-5mm] {instr callback's index(...) args};
\node(index) [memnode, right=of array2, yshift=15mm, text width=52mm] {MemWin: Window Indexer};
\node [right=of array2] {(check MemWin supports idx count)};
\node [right=of packed2] {(check MemWin supports idx count)};
\node [right=of array2_intervals] {(check points before intervals)};
\node [right=of packed2_intervals] {(check points before intervals)};
\draw[arrow] ($(decode.north)$) -- ($(array0.south)$);
\draw[arrow] ($(decode.north) - (0.2, 0)$) -- ($(array0.south) - (0.2, 0)$);
\draw[arrow] ($(decode.north) + (0.2, 0)$) -- ($(array0.south) + (0.2, 0)$);
\draw[arrow] ($(window_expr.north) - (0.3, 0)$) -- ($(packed1_intervals.south) - (0.3, 0)$);
\draw[arrow] ($(window_expr.north) - (0.1, 0)$) -- ($(packed1_intervals.south) - (0.1, 0)$);
\draw[arrow] ($(window_expr.north) + (0.1, 0)$) -- ($(packed1_intervals.south) + (0.1, 0)$);
\draw[arrow] ($(window_expr.north) + (0.3, 0)$) -- ($(packed1_intervals.south) + (0.3, 0)$);
\draw[arrow] ($(instr.north) - (0.3, 0)$) -- ($(packed2_intervals.south) - (0.3, 0)$);
\draw[arrow] ($(instr.north) - (0.1, 0)$) -- ($(packed2_intervals.south) - (0.1, 0)$);
\draw[arrow] ($(instr.north) + (0.1, 0)$) -- ($(packed2_intervals.south) + (0.1, 0)$);
\draw[arrow] ($(instr.north) + (0.3, 0)$) -- ($(packed2_intervals.south) + (0.3, 0)$);
\draw[arrow] (array0.east) to[out=0, in=180] (array1.west);
\draw[arrow] (array1.east) -- (array2.west);
\draw[arrow] (packed1.east) -- (packed2.west);
\draw[arrow] (array1_intervals.east) -- (array2_intervals.west);
\draw[arrow] (packed1_intervals.east) -- (packed2_intervals.west);
\draw[arrow] (array1_intervals.east) to[out=0, in=180] (array2.west);
\draw[arrow] (packed1_intervals.east) to[out=0, in=180] (packed2.west);
\draw[arrow] (dataptr.east) to[out=10, in=180] ($(index.west) + (0, 0.2)$);
\draw[arrow] (strides.east) to[out=20, in=180] ($(index.west) + (0, -0.2)$);
\draw[arrow] ($(array2.east) + (0, 0.25)$) to[out=0, in=270] ($(index.south) + (-1.6, 0)$);
\draw[arrow] ($(packed2.east) + (0, 0.25)$) to[out=45, in=270] ($(index.south) + (-1.4, 0)$);
\end{tikzpicture}
\caption{Window index data flow; note, the MemWin's \greenBox{packed tensor size} informs the split between \yellowBox{array} and \blueBox{packed} indices, which is omitted for space.} \label{fig:windowIndex}
\end{figure*}

\filbreak
\mainSub{Instr Interface to Windows}

The proposed instr C codegen callback will receive a tuple of named parameter values.
Non-window values will probably stay as strings.
Window values could be passed as objects that support functions
\begin{itemize}
  \item \lighttt{to\_struct()}: Encode as window struct; only supported if the \lighttt{MemWin} type supplied a window encoder.
  \filbreak
  \item \lighttt{to\_dataptr()}: For separate-dataptr windows only, get the separate dataptr.
  \filbreak
  \item \lighttt{get\_stride(n)}: Get the $n^{th}$ stride value; only supported if the \lighttt{MemWin} type supplied a window encoder that supports strides, or if there is no window encoder at all (use default strides).
  \filbreak
  \item \lighttt{index(idx...)}: Index into the window expression passed as the instruction parameter.
    Only supported if the \lighttt{MemWin} type supplied a window indexer.
    The indices passed are not directly passed to the window indexer, but are combined with the offsets given by the window expression and (if it exists) underlying decoded window struct.
    This requires careful resolution of point and interval indices, which we will handle inside the compiler \textbf{(figure \ref{fig:windowIndex})}.
  \filbreak
  \item \lighttt{raw\_name():} Get the undecorated C name of the parameter passed as input.
\end{itemize}
\filbreak
We can implement the older \lighttt{str.format} interface as a translation layer around this new interface:
\begin{itemize}
  \item \lighttt{\textit{arg\_name}} = \lighttt{to\_struct()}
  \item \lighttt{\textit{arg\_name}\_data} = \lighttt{index()} with empty index list (if you need more control, don't use the legacy interface)
  \item \lighttt{\textit{arg\_name}\_int} = \lighttt{raw\_name()}
\end{itemize}

\filbreak
\mainSub{Predicate Helpers}

We need to provide both the window encoder and indexer with helpers that check
\begin{itemize}
  \item that an offset or stride value is a compile-time constant;
  \filbreak
  \item that an offset or stride value is divisible by a certain value;
  \filbreak
  \item the alignment of memory (this will actually require cooperative changes to multiple parts of the language).
\end{itemize}

\filbreak
\myTitle{Instrs \& Distributed Memory}

The vast majority of instructions follow the ``each thread collective owns its own distributed shard'' model, i.e., any window parameters passed in refer to a portion of just one distributed shard.
We explicitly annotate (with \redBox{\texttt{distributed:}}) window parameters that break this assumption.

\filbreak
Within the instruction's \lighttt{behavior} specification, we need to add new \lighttt{distribute(...)} statements to guide distributed memory deduction.
LoopIR\_unification should ignore the loop mode of the for loops (which already seems to be the case), and ignore the \lighttt{distribute(...)} statements \textbf{(figures \ref{fig:shfl} and \ref{fig:tma})}

\begin{figure*}[!b]
\input{su_samples/shfl_sync.0.tex}
\caption{Warp Shuffle Instr} \label{fig:shfl}
\end{figure*}

\filbreak
It's somewhat questionable what the reasonable interpretation of the ``remove distributed dimensions'' compiler pass is here, given the distributed dimensions are in this case actually valuable information.
The current use cases (warp shuffle, and TMA multicast) are implemented using CUDA intrinsics that allow for peeking into other distributed shards internally, without invoking the Exo compiler's complicated window system described previously.

\filbreak
So, although it's a bit cheesy, I propose we just blindly strip the distributed dimensions anyway (and possibly just re-write the instr's behavior as a no-op since the typechecking will be messed up).
This means that the window \lighttt{index(...)} function is aware of array and packed dimensions only, and it's the instr implementation's job to handle indexing into other distributed shards.
This should be OK as the goal at this late stage of the compilation is just to ``get the job done'' and generate C/C++ code;
just make a mental note we did this in case there's problems later.

\begin{figure*}[!b]
\input{su_samples/tma_instr.0.tex}
\caption{TMA Multicast Instr} \label{fig:tma}
\end{figure*}

\newpage
\myTitle{Timelines \& Synchronization}

For now I'll just sketch out the basics of how the abstract machine tracks read/write visibility, and go over the changes entailed by moving from the actor kind / actor signature model to the new timeline model:
\begin{itemize}
  \item Each instruction has an ``instruction timeline'' (instr-tl) type, replacing the actor kind used to parameterize \lighttt{CudaAsync} blocks.
  \filbreak
  \item Each instruction parameter has a ``usage timeline'' (usage-tl) type, roughly replacing the actor signature.
  \filbreak
  \item Sync statements now take ``sync timeline'' (sync-tl) parameters, replacing the dual-use of actor kind (formerly parameterized both sync statements and \lighttt{CudaAsync} blocks).
  \filbreak
  \item There are slightly different semantics for guarding against write-after-read (WAR) hazards (``temporal-only'' synchronization), compared to RAW or WAW hazards.
  \filbreak
  \item TODO: still have to retrofit atomic reduce to this model.
\end{itemize}

\filbreak
\mainSub{Instruction Timelines}

Each instruction has one of the following instr-tl types:

\begin{itemize}
  \item \lighttt{cpu\_instr}
  \filbreak
  \item \lighttt{cuda\_classic\_instr}
  \filbreak
  \item \lighttt{Sm80\_cp\_async\_instr}
  \filbreak
  \item \lighttt{tma\_to\_smem\_instr}
  \filbreak
  \item \lighttt{tma\_to\_gmem\_instr}
  \filbreak
  \item \lighttt{wgmma\_instr}
  \filbreak
  \item \lighttt{tcgen05\_instr}  [Blackwell Tensor Core, if we support it]
\end{itemize}
\filbreak
All Exo code has a certain instr-tl in scope: \lighttt{cpu\_instr} in CPU code, \lighttt{cuda\_classic\_instr} within a \lighttt{CudaDeviceFunction} block, and one of the remaining types in a \lighttt{CudaAsync(*\_instr)} block.
All instrs used must have the same instr-tl type as the current scope.

\filbreak
\mainSub{Usage Timelines}

In many cases, the instruction timeline would have been specific enough; however, sometimes, we need to distinguish differing synchronization behavior for different parameters of the same instruction, e.g.,
\begin{itemize}
  \item wgmma has different behavior for SMEM parameters (matrix B, and sometimes A), RMEM accumulators (D), and, if used, RMEM A parameters.
  \filbreak
  \item Some async instructions may take some register parameters synchronously, i.e., after issuing the instruction, you can immediately re-use the register without waiting for the instruction to complete.
\end{itemize}
\filbreak
Dealing with these special cases is the purpose of the per-instr-parameter usage-tl type:
\begin{itemize}
  \item \lighttt{cuda\_ram\_usage}: non-register parameters (``random access'' supported)
  \filbreak
  \item \lighttt{cuda\_sync\_rmem\_usage}: synchronous access to registers
  \filbreak
  \item \lighttt{cuda\_async\_a\_rmem\_usage}: async access to registers holding $A$ matrix fragments
  \filbreak
  \item \lighttt{cuda\_async\_d\_rmem\_usage}: async access to registers holding $D$ matrix fragments
  \filbreak
  \item (possibly TMEM usage-tl types as well, for Blackwell)
\end{itemize}

\filbreak
\mainSub{Timeline Signature}

We characterize ``who'' performed a read/write  using \myKeyA{timeline signatures}.
These are 3-tuples of (instr-tl, usage-tl, tid); tid being a linearized thread index \lighttt{blockIdx.x * blockDim.x + threadIdx.x}.

\filbreak
\mainSub{Visibility Record}

During synchronization checking, we interpret the program sequentially and associate \textit{mutable} read and write \myKeyA{visibility records} with each memory location.
The visibility records consist of
\begin{itemize}
  \item $V_S$: \myKeyA{sync visibility set}: set of timeline signatures
  \filbreak
  \item $V_A$: \myKeyA{async visibility set}: set of timeline signatures, with $V_S \subseteq V_A$
  \filbreak
  \item $L_O^i$: \myKeyA{original instruction timeline}
  \filbreak
  \item pending arrives: set of (queue barrier object, arrive count) pairs; not described in this section
\end{itemize}
\filbreak
and (to be detailed) they are used as follows:
\begin{itemize}
  \item We check for visibility of prior read/write visibility records associated with the memory location when interpreting a read/write of that location.
  \filbreak
  \item We further add a new read/write visibility record after checking the validity of a read/write.
  \filbreak
  \item The visibility sets grow conditionally when we interpret a synchronization statement.
\end{itemize}

\filbreak
\mainSub{Read/Write Configuration}

Each instruction configures:
\begin{itemize}
  \item its instruction timeline $L^i$
  \filbreak
  \item per-parameter usage timeline $L^u$
  \filbreak
  \item per-parameter ``out-of-order'' flag $OOO$
  \filbreak
  \item per-parameter extended instruction timelines: $L_X^i$: set of instr-tl; we require $L^i \in L_X^i$
  \filbreak
  \item per-parameter extended usage timelines: $L_X^u$: set of usage-tl; we require $L^u \in L_X^u$
\end{itemize}
In Exo source code, we assume $L^i$ is \lighttt{cpu\_instr} if not configured otherwise (for backwards compatibility).
We initialize all per-parameter info with defaults as a function of the instr-tl and parameter \lighttt{MemWin} type, but this can be overrided in \lighttt{InstrClass.instance} if needed.
This defaulting cuts down on boilerplate and makes it easier for us to make bulk changes to the model later.

\filbreak
\mainSub{Read/Write State}

When we interpret a read or write, let its $L^i$, $L^u$, $OOO$, $L_X^i$, and $L_X^u$ be as defined above if the read/write is performed through an instr parameter.
If the read/write is performed outside an instr, then $L^i$ is that of the current scope, and all other values get initialized by the \lighttt{MemWin} defaults.

\filbreak
The \myKeyA{initial timeline signature} $L^s$ is $(L^i, L^u, t)$, with t being the ID of the thread issuing the read/write\footnote{It's unclear what to do for instructions where it's not possible to identify the single thread responsible for a read or write, e.g. wgmma SMEM.
It's not clear whether synchronization should be modelled as an ``all'' or ``any'' requirement in this case. For example, if threads [128, 255] issue a wgmma, and it reads from SMEM a proxy-fenced value written by thread 133, is futher synchronization required (because not \textit{all} threads can see the value), or not required (because \textit{any} of the issuing threads, namely $133 \in [128, 255]$, can see the value)?}.

\filbreak
The \myKeyA{initial extended timeline signatures} $L_X^s$ are $L_X^i \times L_X^u \times \{ t \}$.\footnote{In many cases, $L_X^s = \{L^s\}$, but when $L_X^s$ contains more timeline signatures, this indicates the current instruction is able to implicitly ``handshake'' with a different instruction/usage type.}

\myKeyA{Read Checking:} Check that each write visibility record is \myKeyA{visible-to} the interpreted read, where visible-to means $V_S \cap L_X^s$ is non-empty.

\myKeyA{Write Checking:} Check that each read and write visibility record is visible-to the interpreted write.

\myKeyA{New Visibility Record:} Add a new read/write visibility record
\begin{itemize}
  \item $V_A = \{ L^s \}$
  \filbreak
  \item $V_S = V_A$ if $OOO$ is false, otherwise initialize $V_S$ to be empty
  \filbreak
  \item $L_O^i = L^i$
\end{itemize}

\filbreak
\mainSub{Sync Timeline}

We parameterize sync statements (fences, arrives, awaits) with sync timeline (sync-tl) parameters.
Each fence statement has both a first and second sync timeline $L_1$ and $L_2$.
An arrive statement has only $L_1$, and an await statement has only $L_2$.

A sync timeline $L_n$ contains
\begin{itemize}
  \item $L_n^F$: full timeline set: $L_n^{Fi} \times L_n^{Fu}$, where these are a set of instr-tl and usage-tl, respectively.
  \filbreak
  \item $L_n^T$: temporal timeline set: $L_n^{Ti} \times L_n^{Tu}$, of the same type as above, with $L_n^F \subseteq L_n^T$
  \filbreak
  \item $V_1$-transitive flag (only relevant for first sync timelines)
\end{itemize}
\filbreak
This will control what subset of reads/writes \myKeyA{synchronize-with} the sync statement.

\filbreak
\mainSub{Sync Statement Behavior}

In addition to $L_1$ and $L_2$ defined above, define $C$ for each interpreted sync statement as the thread IDs of the convergent thread collective executing the statement (e.g. a statement corresponding to \lighttt{\_\_syncwarp()} is executed by a thread collective of 32 threads).

\filbreak
Define these sets of timeline signatures:
\begin{itemize}
  \item The first visibility set $V_1 = L_1^F \times C$
  \item The second read visibility set $V_2^r = L_2^T \times C$
  \item The second write visibility set $V_2^w = L_2^F \times C$
\end{itemize}

\filbreak
A visibility record $(V_S, V_A, L_O^i)$ \myKeyA{synchronizes-with} a fence or arrive statement when
\begin{itemize}
  \item $V_1 \cap V_A$ is non-empty, if $L_1$ is $V_1$-transitive;
  \filbreak
  \item $V_1 \cap V_A$ is non-empty AND $L_O^i \in L_1^{Fi}$, otherwise.
\end{itemize}
\filbreak

\filbreak
An interpreted fence or await statement \myKeyA{augments} a visibility record by setting $V_A \leftarrow V_A \cup V_2$ and $V_S \leftarrow V_S \cup V_2$, where $V_2$ is $V_2^r$ or $V_2^w$ for read visibility records and write visibility records, respectively.

\filbreak
When we interpret a fence statement, it augments all visibility records that synchronize-with the fence.
When we interpret an await statement, it augments all visibility records that synchronized-with a corresponding arrive statement
(the next section defines what ``corresponding arrive'' means).
This synchronizes-with check has to be done at the time the arrive is interpreted, as visibility records are mutable.

Remarks:
\begin{itemize}
  \item ``synchronizes-with'' is defined in terms of the async visibility set $V_A$ while ``augments'' modifies both $V_A$ and $V_S$; in this way, timeline signatures can graduate from $V_A$ to $V_S$.
  \item Guarding against a write-after-read (WAR) hazard has looser requirements than RAW or WAW hazards, in that we only need to ensure temporal separation of the prior read and subsequent write, and do not actually need to preserve or communicate any value in memory (e.g. we can tolerate stale caches, or, in CUDA terms, we can elide proxy fences).
  This is the reason for having separate $V_2^w$ and $V_2^r$.
  \item Furthermore, note $V_2^w \subseteq V_2^r$ as a consequence of $L_n^F \subseteq L_n^T$.
  \item $L_1^T$ is currently never used.
\end{itemize}

\filbreak
\myTitle{Arrive/Await Pairing and Multicast}

It's time to define the correspondence between arrive and await statements.
In Exo-GPU's synchronization checking model, each \myKeyA{queue barrier} $Q$ contains non-negative integers \myKeyA{arrive count} $Q^1$ and \myKeyA{await count} $Q^2$, both initially zero (this conceptual state is distinct from the physical state of the real CUDA barrier type, which is specified with a \lighttt{@memory} annotation).
Although we currently have no explicit syntax for it, allocated barrier variables are arrays of independent queue barriers, which can be indexed in arrive/await statements.
We analyze indexing with distributed memory deduction (see coll\_algebra.pdf).

\input{su_samples/mbarrier_var.1.tex}

\filbreak
Unlike non-barrier allocations, though, there is no native unit; we unconditionally interpret all dimensions as distributed (I might change this in the future), and thus, all indices must follow the distributed memory deduction requirements (e.g. they have to be \lighttt{cuda\_threads} loop iteration variables).
In other words, each distributed shard of the barrier array is only one queue barrier.

\filbreak
Furthermore, for mbarriers only, we support \lighttt{ReverseArrive} and \lighttt{ReverseAwait} statements, which behave and pair with each other just as \lighttt{Arrive} and \lighttt{Await} statements do, except that reversed statements use a hidden ``reverse barrier'' array parallel to the primary ``forward barrier'' array.
For a queue barrier $Q$ in either array, let $\widehat{Q}$ denote the queue barrier in the corresponding position of the other array.

\filbreak
We would get almost the same program by replacing each \lighttt{ReverseArrive} and \lighttt{ReverseAwait} on some barrier \lighttt{my\_bar} with \lighttt{Arrive} and \lighttt{Await} on a new barrier variable \lighttt{my\_var\_2}.
However, there is some tight coupling between the ``forward'' and ``reverse'' barriers that justifies this design (more on this when we talk about the mbarrier pairing requirement).

\filbreak
\mainSub{Valid Usage}

The purpose of a single queue barrier $Q$ is to synchronize one specific set of arriving threads with another specific set of awaiting threads, in a predictable manner, and with the barrier being reset to a known-good state at the end.
The requirements enforce that intended usage, to make lowering queue barriers to real barriers feasible:

\filbreak
\begin{itemize}
  \item Each barrier variable must be used by at least one \lighttt{Arrive} and at least one \lighttt{Await} statement.
  \filbreak
  \item Only mbarriers may use \lighttt{ReverseArrive} and \lighttt{ReverseAwait}; both must be present or both not present.
  \filbreak
  \item For a given barrier variable, each \lighttt{Arrive} statement must use equivalent sync timelines (sync-tl); similarly for all \lighttt{Await}, \lighttt{ReverseArrive}, and \lighttt{ReverseAwait} statements.
  \filbreak
  \item Each queue barrier $Q$ (distributed shard of the barrier variable) must have one unique thread collective that executes \lighttt{Arrive} on $Q$; similarly for \lighttt{Await}, \lighttt{ReverseArrive}, and \lighttt{ReverseAwait} (if present). We enforce this statically by inspecting the collective tiling for each usage.
  \filbreak
  \item When a barrier variable is deallocated, each queue barrier must have $Q^1 = Q^2$.
  \filbreak
  \item mbarriers must follow the mbarrier pairing requirement, to be described later.
\end{itemize}

\filbreak
\mainSub{Arrive Interpretation, N = 1}

Assume for now that an arrive statement only takes one queue barrier as input (if not, multicasting is in effect, which is not yet described), with the syntax\\\lighttt{[Reverse]Arrive(L1: sync-tl, N: int) >> barrier\_name[idx...]}

\filbreak
Further assume $N = 1$.
When we interpret such an arrive statement on some queue barrier $Q$, for each visibility record that synchronizes-with the arrive,
\begin{itemize}
  \item Add $(Q, Q^1)$ : (queue barrier object, arrive count) to the set of pending arrives
  \item Increment $Q^1$
\end{itemize}

\filbreak
\mainSub{Await Interpretation}

Await statements always take only one queue barrier $Q$ as input, with syntax\\
\lighttt{[Reverse]Await(barrier\_name[idx...], L2: sync-tl, N: int)}
The interpretation depends on $N$.

\filbreak
If $N \geq 0$, then this is an ``arrive-indexed'' barrier, which is only supported for commit groups. Do:
\begin{itemize}
  \item Augment all visibility records containing $(Q, x)$ with $x < Q^1 - N$ [NB: strict inequality]
  \item Set $Q^2 = \max(Q^1 - N, Q^2)$
\end{itemize}
This models the meaning of \lighttt{wait\_group N} in CUDA, i.e., wait for all but the last $N$-many \lighttt{commit\_group} instructions.

\filbreak
If $N < 0$, then this is an ``await-indexed'' barrier, which is only supported for mbarriers.\\
Let \lighttt{lag = \textasciitilde N} (complement, not negate), and do:
\begin{itemize}
  \item Augment all visibility records containing $(Q, x)$ with $x \leq Q^2 - \lighttt{lag}$ [trivially false if $Q^2 < \lighttt{lag}$]
  \item Set $Q^2 = Q^2 + 1$
\end{itemize}
Essentially, the first \lighttt{lag}-many await statements are no-ops, and then arrives and awaits pair up 1:1.

\filbreak
\mainSub{Arrive Interpretation, N = \textasciitilde 0}

This is a special case for satisfying the $Q^1 = Q^2$ requirement, only supported for mbarriers.
When we interpret \lighttt{[Reverse]Arrive(L1, \textasciitilde 0) >> $Q$}, it's like running
\lighttt{[Reverse]Arrive(L1, 1) >> $Q$} in a loop, with the iteration count being
\begin{itemize}
  \item $Q^1 - Q^2$, if there is never any usage of \lighttt{ReverseArrive} or \lighttt{ReverseAwait} for the barrier variable that $Q$ is from.
  \item $Q^1 - \widehat{Q}^2$ otherwise.
\end{itemize}
This has the effect of equalizing $Q^1$ and $Q^2$, or $Q^1$ and $\widehat{Q}^2$, in the two respective cases.
The iteration count will never be negative; this is a result of the to-be-described mbarrier pairing requirement.

\filbreak
\mainSub{Exo mbarrier Pairing Requirement}

Arrive-indexed queue barriers lower to commit groups trivially.
The process for lowering await-indexed queue barriers to mbarriers is more involved, as the mbarrier does not hold any state corresponding to the queue arrive count $Q^1$.
We will emulate this state using a ring buffer of mbarriers; informally, this means we have to limit the ``spread'' between the number of arrives and the number of awaits, so we don't overrun the ring buffer.

For a \lighttt{@CudaMbarrier} queue barrier $Q$, if \lighttt{ReverseArrive} and \lighttt{ReverseAwait} are used, then the pairing requirement is
\begin{itemize}
  \item The $n^{th}$ (forward) \lighttt{Arrive} on $Q$ cannot \textit{finish} until the $n^{th}$ \lighttt{ReverseAwait} on $\widehat{Q}$ finishes.
  \filbreak
  \item The $n^{th}$ \lighttt{ReverseArrive} on $\widehat{Q}$ cannot \textit{finish} until the $n^{th}$ (forward) \lighttt{Await} on $Q$ finishes.
\end{itemize}
\filbreak
Otherwise
\begin{itemize}
  \item The $n^{th}$ \lighttt{Arrive} on $Q$ cannot \textit{finish} until the $(n-1)^{th}$ \lighttt{Await} on $Q$ finishes.
\end{itemize}

\filbreak
``finish'' means that \textit{all} threads in the arriving/awaiting thread collective have passed the arrive/await statement.
We will describe later why this requirement is needed for the ring buffer.

\filbreak
To enforce this, we require that each thread using the mbarrier follows one of these expected usage patterns:
\begin{itemize}
  \item \lighttt{Await} on $Q$, followed by a (possibly conditional) \lighttt{ReverseArrive} on $\widehat{Q}$ [await-first usage]
  \filbreak
  \item \lighttt{ReverseAwait} on $\widehat{Q}$, followed by a (possibly conditional) \lighttt{Arrive} on $Q$ [await-first usage]
  \filbreak
  \item If there is no \lighttt{ReverseAwait} or \lighttt{ReverseArrive}, then we support \lighttt{Await} on $Q$, followed by a (possibly conditional) \lighttt{Arrive} on $Q$ [await-first usage]
  \filbreak
  \item If there is no \lighttt{ReverseAwait} or \lighttt{ReverseArrive}, we also support \lighttt{Arrive} on $Q$, followed by an unconditional \lighttt{Await} on $Q$ [arrive-first usage]
  \item (Note: in the current implementation, arrive-first and await-first patterns are not simultanously supported for a single barrier variable)
\end{itemize}

\filbreak
In all these supported cases, we require no other usage of $Q$ or $\widehat{Q}$ by the thread between the statement pair.
Unlike the other synchronization requirements, which are enforced by ``simulation'', this ``meta-synchronization'' requirement will be enforced by static analysis.
These rules allow us to safely lower queue barriers to mbarrier ring buffers.

Static analysis: for each \lighttt{[Reverse]Arrive} statement using $Q$, search forward/backwards \textit{syntactically} for the next/previous statement using $Q$ or $\widehat{Q}$ (forwards in the arrive-first case; backwards in the await-first case).
\filbreak
\begin{itemize}
  \item If the sync statement is not the correct sort of \lighttt{[Reverse]Await} statement, the analysis fails.
  \filbreak
  \item If the sync statement's \lighttt{CollTiling} is not equivalent to that of the \lighttt{[Reverse]Arrive}, the analysis fails (this is asserting the set of threads executing the two statements is the same).
  \filbreak
  \item If the \lighttt{[Reverse]Await} is guarded by an if statement that the \lighttt{[Reverse]Arrive} isn't, the analysis fails.
  \filbreak
  \item If either the \lighttt{[Reverse]Await} or \lighttt{[Reverse]Arrive} is in the body of a \textit{sequential} for loop that the other isn't, the analysis fails.
  \filbreak
  \item Note, \lighttt{with} statements and parallel-for loops are ignored, as these don't have effect on a single thread's execution (given the \lighttt{CollTiling} equality check passed)
\end{itemize}

\filbreak
\mainSub{Arrive Multicast}

See this \webText{Colfax Research Article}{https://research.colfax-intl.com/cutlass-tutorial-gemm-with-thread-block-clusters-on-nvidia-blackwell-gpus/}, especially ``Constructing bitmasks'', for background.
This is for tcgen05 (Blackwell), but the cluster and CTA mask content applies to wgmma as well.

\filbreak
CUDA supports arriving on mbarriers in other CTAs of the same cluster.
Each queue barrier (with \lighttt{CudaMbarrier} memory type) lowers to a ring buffer of mbarriers, and we use distributed memory to model the full ``array'' of per-CTA mbarriers in a cluster.
So it would be intuitive to extend the distributed indices feature to express such multicasting, e.g., something like

\filbreak
\input{su_samples/intro_multicast_mbarrier.0.tex}

\filbreak
In the abstract machine, the meaning of this is instead of adding a single pending arrive $(Q, Q^1)$ to visibility records that synchronize-with the arrive, we add $(Q, Q^1)$ for each queue barrier $Q$ named in any of the expressions following \lighttt{>>} (removing duplicates).

\filbreak
If there's multiple \lighttt{Arrive} or \lighttt{ReverseArrive} statements for the same barrier variable, their multicasting must be the same (so the above example illustrating multiple possible patterns is not valid within a single Exo proc).

Multicasting poses some challenges for our model:
\begin{itemize}
\item Since a distributed index was replaced with \lighttt{:}, how do we adapt distributed memory deduction to match?
\filbreak
\item The arrive count $Q^1$ is per-queue-barrier state in our model, but having this vary dynamically in different CTAs is not really implementable (nor useful), so we have to ensure $Q^1$ is uniform for all queue barriers involved in the arrive statement.
\filbreak
\item How does this interact with mbarrier pairing?
\end{itemize}

\filbreak
\mainKey{Distributed Memory Deduction:} Each \lighttt{>> barrier-name[idxs...]} is a \myKeyA{barrier expression}.
Await statements may not take multiple barrier expressions.
Arrive statements may take multiple barrier expressions, subject to the rules:
\begin{itemize}
  \item All barrier names, and length of the indices list, must be identical.
  \filbreak
  \item Each index is either a \lighttt{cuda\_threads}-iterator, or \lighttt{:}.
  \filbreak
  \item For each \lighttt{i}, at least one \lighttt{idxs} must have \lighttt{idxs[i]} being a \lighttt{cuda\_threads}-iterator\footnote{This is the reason for the ``redundant'' barrier expressions earlier. The expression \lighttt{>> ringbar[m\_cta, :]} alone would have had the lone \lighttt{:} flagged as an error.} (instead of \lighttt{:}).
    This is the \lighttt{i}-th shared \lighttt{cuda\_threads} iterator.
    If multiple \lighttt{idxs} have \lighttt{idxs[i]} being a \lighttt{cuda\_threads}-iterator, they must all agree.
\end{itemize}

\filbreak
We reduce the raw queue barrier expressions to a shared barrier variable name and \lighttt{cuda\_threads}-iterator list, and a per-barrier-expression ``multicast flags'' list (True means \lighttt{:}).
For example, in the case \lighttt{>> ringbar[m\_cta, n\_cta] >> ringbar[:, n\_cta]}, the state would be
\begin{itemize}
  \item variable name: \lighttt{ringbar}
  \filbreak
  \item iterators: \lighttt{[m\_cta, n\_cta]}
  \filbreak
  \item per-expression multicast flags: \lighttt{[False, False]}, \lighttt{[True, False]}
\end{itemize}

The variable name and \lighttt{cuda\_threads} iterators get used for distirbuted memory deduction (which ignores multicasting).
We say that a barrier expression \textit{multicasts} the $n^{th}$ iterator if its $n^{th}$ multicast flag is true (e.g., in the previous example, \lighttt{ringbar[:, n\_cta]} multicasts \lighttt{\textit{m}\_cta}).

\filbreak
\mainKey{Arrive Count:} I'm not super happy with this solution, but this is what I have so far.

\filbreak
These multicast barriers are a bit awkward because, on the one hand, they are cluster-wide operations, and so should go \textit{outside} the per-CTA loops, but on the other hand, they have to be \textit{inside} the per-CTA loops so we have access to the \lighttt{cuda\_threads}-iterators needed to program the multicasting behavior (see the Colfax article earlier to see that the ``same \lighttt{m\_cta} OR same \lighttt{n\_cta}'' use case is real; we can't just trivially support ``multicast to all CTAs'').

\filbreak
For now my compromise is to require multicast \lighttt{[Reverse]Arrive} statements to appear in a \lighttt{cuda\_threads} loop nest containing only the arrive statement.
We search outwards and enforce this until all multicast variables are found.
This allows the abstract machine to execute the loop nest as a single compound statement; each queue barrier $Q$ involved has $Q^1$ incremented exactly once in the compound statement.

\filbreak
\input{su_samples/multicast_loop_nest.0.tex}

\filbreak
\input{su_samples/multicast_loop_nest.1.tex}

\filbreak
(TODO: either find a better way to model this, or carefully specify the behavior of this compound statement in the context of the rest of the abstract machine).

\filbreak
The palatability of this solution hinges on whether we can add good scheduling operators or scheduling libraries to help users deal with this issue.

\filbreak
\mainKey{Mbarrier Pairing:} I have not fully figured out the best way to adapt the mbarrier pairing requirement.
Currently, I plan to support multicast for barriers using \lighttt{ReverseArrive} and \lighttt{ReverseAwait}, and perhaps assert that a queue barrier $Q$ is \lighttt{Arrive}'d on iff $\widehat{Q}$ is \lighttt{ReverseArrive}'d on:

\input{su_samples/multicast_pairing_fail.0.tex}

\filbreak
\mainSub{TMA Instruction Barrier Parameter}

TMA GMEM-to-SMEM instructions specifically require an mbarrier, and multicasting information (a.k.a. CTA mask) to operate.
The special \lighttt{expect-tx} mechanism, which is separate from the normal mbarrier arrive operation, also means the TMA operation synchronizes with only a specific mbarrier, i.e. something like this

\input{su_samples/tma_2_arrives.0.tex}

is not directly implementable.

\filbreak
What I plan to do to support this is
\begin{itemize}
  \item Add support for a trailing barrier expression \lighttt{>> barrier\_variable[idxs...]} to instruction calls (only valid for TMA-to-SMEM instructions for now).
  \filbreak
  \item Extend distributed memory deductions for instrs to handle this trailing barrier expression.
  \filbreak
  \item Extend the mbarrier pairing requirement to handle TMA
\end{itemize}

\filbreak
\mainKey{Instr Barrier Expression:} For instructions called with syntax \lighttt{instr(...) >>} $Q$, all visibility records generated by the instruction, when executed by the abstract machine, have $(Q, Q^1)$ immediately added to their pending arrives.
$Q^1$ is \textit{not} incremented, unlike for arrive statements.
The barrier expression may contain the \lighttt{:} index (multicasting), in which case the previous is done for each $Q$ named in the barrier expression.

\filbreak
Mbarriers will not support \lighttt{tma\_to\_smem} instr-tl in any supported first sync timeline\footnote{Exo-GPU as of 2025-06-16 does allow \lighttt{tma\_to\_smem\_async} as the first actor kind of an mbarrier; this isn't supposed to exist once the instr barrier parameter gets implemented.}, so a TMA-to-SMEM instruction will never synchronize-with a future arrive statement.

\filbreak
There is currently no syntax for instrs to access the reverse barrier array (the queue barriers used by \lighttt{ReverseArrive} and \lighttt{ReverseAwait}).
This may be an argument for considering the whole \lighttt{ReverseArrive} and \lighttt{ReverseAwait} thing to be a confusing design.
However, I consider this concept (of tight coupling between the forward and reverse directions) to be essential, but maybe we could have better syntax for it, e.g. \lighttt{barrier\_name} and \lighttt{\textasciitilde barrier\_name}?

\filbreak
\mainKey{Distributed Memory Deduction:} Unlike for synchronization statements, we do not need to disambiguate \lighttt{:} indices.
Instead, we can use \lighttt{distribute(...)} statements in the instruction's \lighttt{behavior} specification to complete distributed memory deduction.

\input{su_samples/tma_instr_barrier.0.tex}

(TODO syntax for non-contiguous CTAs, e.g. every other CTA)

\filbreak
If the barrier argument is absent from the instruction definition (which is the case for all non-TMA-to-shared instructions), then trailing barrier expressions get diagnosed as a compile time error.
TODO it's a bit weird how the barrier appears inside the argument list for \lighttt{behavior(...)}, but outside as a trailing \lighttt{>> $Q$} in the instr call site ... my motivation for the \lighttt{>>} was to emphasize the non-normalness of barriers (they're meaningless in S-semantics), but maybe I'm just trying to look cool.

\filbreak
\mainKey{Mbarrier Pairing:} Extend mbarrier pairing to support the pattern

\begin{itemize}
  \item \lighttt{ReverseAwait} on \lighttt{barrier\_name[idxs...]}
  \filbreak
  \item TMA instructions with \lighttt{>> barrier\_name[idxs...]}, possibly with some indices replaced with \lighttt{:}
  \filbreak
  \item \lighttt{Arrive} on \lighttt{barrier\_name[idxs...]}
\end{itemize}
What makes this messy is the interaction with the not-fully-worked-out multicast requirements on mbarrier pairing.
For example, in something like
\filbreak
\input{su_samples/tma_pairing_multicast.0.tex}
\filbreak
\input{su_samples/tma_pairing_multicast.1.tex}
\filbreak
a reasonable expectation would be for the \lighttt{ReverseArrive(cuda\_classic, 1)} to use \lighttt{>> ringbar[m\_cta, :] >> ringbar[:, n\_cta]}, this being the union of the CTAs multicast to in the producer code.
However, if we wrap the TMA instructions with conditionals, this might not be the case anymore.
This may be an argument that statically enforcing mbarrier restrictions isn't the right way to go, and maybe this should be part of the abstract machine's requirements (synchronization checking).

\filbreak
For the sake of time, it may be best to just acknowledge this as a known limitation of the model.

\filbreak
\minorSub{mbarrier Ring Buffer}

%% \filbreak
%% \myTitle{WIP: mbarrier Details}

%% Arrive-indexed queue barriers lower to commit groups trivially.
%% The process for lowering await-indexed queue barriers to mbarriers is more involved, as the mbarrier does not hold any state corresponding to the queue arrive count $Q^1$.
%% Each mbarrier contains:
%% \begin{itemize}
%%   \item expected thread arrival count\footnote{Just ``arrival count'' in the PTX docs; I use the ``thread'' prefix to distinguish from the queue arrival count $Q^1$.}; Exo initializes this to the number of threads in the box of the arriving thread collective
%%   \item pending thread arrival count; initially 0
%%   \item the completed-phase parity bit; initially 1
%% \end{itemize}
%% (ignore state corresponding to TMA, tcgen05, etc. as they're not relevant to this discussion).

%% \filbreak
%% In CUDA terminology, each mbarrier has a hidden ``phase'' integer 0, 1, 2, 3...; each time a thread executes arrive-on an mbarrier, it atomically
%% \begin{itemize}
%%   \item increments the pending thread arrival count;
%%   \filbreak
%%   \item increments (completes) the phase and resets the pending arrival count to 0, if the pending thread arrival count matched the expected thread arrival count;
%%   \filbreak
%%   \item sets the parity bit to the parity of the phase just \textit{completed} [i.e., to the \textit{opposite} of the parity of the current phase]
%% \end{itemize}
%% \filbreak
%% The fact that only the parity is encoded is what makes encoding $Q^1$ tricky.
%% There is no direct way to directly distinguish phase $p$ from phase $p + 2$.

%% \filbreak
%% Let's briefly restate the behavior required of await-indexed queue barriers:
%% \begin{itemize}
%%   \item When all threads in the executing thread collective reach \lighttt{Arrive(sync-tl, 1) >> $Q$}, then increment $Q^1$.
%%   \item No thread proceeds past the $Q^2$-th execution [0-indexed] of \lighttt{Await($Q$, sync-tl, \textasciitilde lag)} until $Q^1 > Q^2 - \lighttt{lag}$.
%%   The first \lighttt{lag}-many awaits do nothing; call the awaits after that \myKeyA{non-trivial} awaits, so the $n^{th}$ non-trivial await waits for $Q^1 > n$.
%% \end{itemize}
%% \filbreak
%% We'll implement the queue barrier as a ring buffer of $r$-many mbarriers ($r$ to be determined), and increment the phase of each mbarrier in a cycle.
%% The total of the phases encodes $Q^1$; e.g., if $r = 3$, and the phases are [2, 2, 1], then $Q^1 = 5$.
%% Alternatively, the phase of the $j^{th}$ mbarrier is $\lceil (Q^1 - j) / r \rceil$.
%% \filbreak
%% With this invariant in mind, we lower the queue barriers to mbarriers:
%% \begin{itemize}
%%   \item Implement \lighttt{[Reverse]Arrive} as arrive-on the mbarrier at position $Q^1~\%~r$ in the ring buffer; when all threads in the thread collective execute the arrive-on, the phase increments, because the expected thread arrival count was initialized to the number of threads in the thread collective (i.e. collective unit box size).
%%   \filbreak
%%   \item Each thread executing the $n^{th}$ non-trivial \lighttt{[Reverse]Await} has to detect that $Q^1 > n$. Let $j = n~\%~r$, then, we need to wait for the phase $p = \lceil (n - j) / r \rceil$ of the $j^{th}$ mbarrier to complete.
%%   Detect this indirectly by checking the parity is $p~\%~2$.

%%   We can implement with a per-thread, per-mbarrier, alternating ``expected phase'' bit, because all this really boils down to is that each thread's $n^{th}$ non-trivial arrive on $Q$ will be its $\lfloor n / r \rfloor$-th await on the $j^{th}$ mbarrier, and it waits for the completed phase parity to be $(\lfloor n / r \rfloor)~\%~2$.
%% \end{itemize}
%% \filbreak
%% The indirect detection of the mbarrier phase fails if the phase of the mbarrier advances to $p+1$ before all awaiting threads detect phase $p$, as the phase bit will flip back.
%% I don't know if there's an official name for this error, but I call it a \myKeyA{barrier overrun}.
%% Besides avoiding phase ambiguity, we are required to avoid barrier overrun to uphold this (weaker) PTX requirement:

%% \filbreak
%% ``For each phase of the mbarrier object, at least one \lighttt{test\_wait} or \lighttt{try\_wait} operation [Exo \lighttt{[Reverse]Await}] must be performed which returns True for waitComplete before an arrive-on operation in the subsequent phase.''

%% \filbreak
%% To avoid barrier overrun, we need to enforce that at least one thread of the arriving thread collective won't arrive on a physical mbarrier during phase $p+1$ until all awaiting threads have detected the completion of phase $p$. In Exo terms, the $(n+r)^{th}$ arrive on $Q$ cannot complete until the $n^{th}$ \myKeyA{non-trivial} await on $Q$ has fully finished (recall $r$ is the mbarrier ring buffer size). Handwavey, but this will inductively ensure that when we try to detect the completion of phase $p$, the only possibilities are that phase $p$ just completed, or phase $p-1$ just completed, and we avoid the ambiguous $p \pm 2\mathbb{N}$ cases.

%% \filbreak
%% In the next section, we'll calculate the ring buffer size $r$ and discuss how to prevent barrier overruns.
%% Unlike the rest of synchronization checking, which is done with the simulator, I plan to use static analysis to enforce this ``meta-synchronization'' requirement (correctly synchronizing our usage of synchronization primitives).

\end{document}
