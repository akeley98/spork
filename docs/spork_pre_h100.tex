\input{whitepaper_common.tex}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\begin{document}
\myTitle{Exo GPU (Spork) Status 2025-04-22}

As of today, I have a pretty much working prototype of Exo-GPU that supports A100 (sm\_80) features (non-bulk async copies of 4, 8, or 16 bytes, and warp-level tensor cores).
It is ``working'' to the extent that it's able to compile correct programs, but there is no synchronization checking, and limited checking for other kinds of mistakes.

The prototype's support for H100 (sm\_90a) features (TMA and wgmma.mma\_async) is fairly broken right now.
I'm able to compile a basic gemm for H100, but its performance is much worse than hand-written code.

\filbreak
\mainSub{GPU-specific Features Implemented}

\mainKey{Nascent Type Systems:} Nothing formal or anything, but the Exo CUDA backend is able to statically analyze for each statement:
\begin{itemize}
  \item \myKeyA{Collective Unit (coll\_unit):} arrangement of cooperating threads, e.g. \lighttt{cuda\_warp} (32 aligned threads); \lighttt{cuda\_warpgroup} (128 aligned threads).
  A single instance of a collective unit is a \myKeyA{thread collective}.
  \item \myKeyA{Collective Tiling (CollTiling):} tiling of thread collectives across threads of the cluster/CTA; may be non-trivial due to masked-out threads (warp specialization).
  \item \myKeyA{Actor Kind:} cpu, cuda\_classic, or one of the CudaAsync actor kinds.
  \item \myKeyA{Actor Signature:} Each read/write is annotated with an actor signature.
\end{itemize}

\filbreak
We enforce actor kind and collective unit requirements:
\begin{itemize}
  \item Memory types specify read, write, and allocation permissions per actor kind.
  \item Non-\lighttt{instr} writes and reduces must use a single thread.
  \item An \lighttt{instr} specifies its own custom requirements, and an \myKeyA{actor signature} per parameter.
\end{itemize}

\filbreak
\mainKey{CudaDeviceFunction Block:} Subtree compiled to a CUDA device function, with \lighttt{blockDim}, \lighttt{clusterDim}, and \lighttt{blocks\_per\_sm} (occupancy) specified.
The LoopIR-to-C compiler automatically handles compiling a CUDA device function, and passing arguments from host code to device code.
This changes the \myKeyA{actor kind} from CPU to cuda\_classic for child statements.

\filbreak
\mainKey{Grid Constants:} Scalars or fixed-size arrays copied from the CPU to the device function.

\filbreak
\mainKey{Shared Memory:} SMEM requires special compiler support to allocate.
I currently have a stack allocator, where allocate/free means increment/decrement the stack pointer (with special handling for the case that frees aren't in LIFO order).
The compiler deduces the maximum SMEM usage and requests that amount at CUDA device function launch time.
We will have to consider the synchronization challenges this poses -- shared memory must not be freed until all usages have retired.

\filbreak
\mainKey{CudaAsync Blocks:} Changes the actor kind from cuda\_classic to one of the async actor kinds.

\filbreak
\mainKey{cuda\_tasks Loops:} Distribute independent work items across clusters or CTAs.

\filbreak
\mainKey{cuda\_threads Loops:} Subdivide thread collective into child thread collectives with the specified collective unit.
This changes the \myKeyA{collective tiling} of child statements.

\filbreak
\mainKey{Split Barriers:} I can lower \lighttt{Arrive} and \lighttt{Await} statements to \myKeyA{commit group} or \myKeyA{mbarrier} synchronization, for both sm\_80 and sm\_90a features.

\filbreak
\mainKey{sm\_80 (A100) Features:} Non-bulk \lighttt{cp.async} and warp-level MMA.
I have a working gemm for sm\_80 using these features and \myKeyA{mbarrier}.

\filbreak
\mainKey{sm\_90a (H100) Features:} \lighttt{cp.async.bulk} (TMA) and \lighttt{wgmma.mma\_async}.
I only have a tiny subset of instructions (just for tf32) implemented.
TMA mostly works okay (minus required error checking), wgmma not so much.
I have a rant (wgmma\_dirty\_laundry.pdf) about some of the wgmma problems.

\filbreak
\mainKey{New Output Files:} If any CUDA device functions are compiled, the compiler additionally emits \lighttt{.cu} and \lighttt{.cuh} files.
The implementation of the CUDA device functions goes into the \lighttt{.cuh} (header) file, so the user may extract and re-use the generated device functions as they see fit, independent of Exo-generated CPU code
(dynamic linking support for CUDA is very poor).
In the likely case that the user doesn't want to DIY, they may rely on stub code in the \lighttt{.c} and \lighttt{.cu} files to launch the CUDA device functions for them.

\filbreak
\minorSub{Non-GPU-specific Features Implemented (Optional Reading)}

These features were added to support Exo-GPU, but conceivably could be useful in other contexts.

\filbreak
\minorKey{Loop Mode Object:} The binary \lighttt{seq}/\lighttt{par} choice for loop mode is generalized and externalized as \lighttt{LoopMode} objects, which themselves can be parameterized (e.g. the \lighttt{unit} for \lighttt{cuda\_threads} loops).

\filbreak
\minorKey{Pragma Unroll:} Building off the above feature, \lighttt{Seq} loop modes may have an optional \lighttt{pragma\_unroll} parameter, which causes the generated loop to include \lighttt{\#pragma unroll}.

\filbreak
\minorKey{Per-Memory Window Type:} Exo 1 generated window structs for each (primitive type $\times$ dimensionality $\times$ mutability) combination.
For example, for (f32, 2D, read-only), it generates:
{\color{lightttColor}
\begin{verbatim}
struct exo_win_2f32c{
    const float * const data;
    const int_fast32_t strides[2];
};
\end{verbatim}
}
\filbreak
Essentially, Exo 1 assumes all memory can be addressed with a C pointer (I call this the window \myKeyA{dataptr}) and freely-specified strides (I call this the window \myKeyA{layout}).
This assumption breaks down for the GPU, and is even invalid for Exo 1 (e.g. for AVX).

\filbreak
I made an effort to allow memory types to generate their own custom window structs (defining both a \myKeyA{dataptr} and a \myKeyA{layout}).
Memory and window definitions are put in the header (\lighttt{.h}) file iff the memory type is exposed in the header file; otherwise, the definitions go into implementation files (\lighttt{.c} and \lighttt{.cuh}).

\filbreak
\minorKey{Separate windowptr:} Custom window types may specify that the dataptr and layout be created as separate variables, instead of together as a struct.
We need this for CUtensorMap windows: the CUtensorMap needs to be in grid constant memory, while associated layout information needs to be in registers (local variables).
This is a bit fragile: it's a rarely used feature, and the C backend has to be aware of this possibility everywhere.

\filbreak
\minorKey{SpecialWindow:} Memory types are now generalized to MemWin types (Memory or SpecialWindow).
A SpecialWindow type is like a Memory type in that it's used to constrain the parameters for an \lighttt{instr}.
However, variables constructed as a SpecialWindow must be created from existing allocations as a window statement (TBD replace WindowStmt?), instead of allocated.

\filbreak
This exists for now to support CUtensorMap, as in the following example:
{\color{lightttColor}
\begin{verbatim}
@proc
def xgemm_Sm90_wgmma(M: size, N: size, K: size,
                     A: f32[M,K] @ CudaGmemLinear,
                     B: f32[N,K] @ CudaGmemLinear,
                     C: f32[N,M] @ CudaGmemLinear):
    A_tensorMap = A[:,:] @ Sm90_tensorMap(128, smem_m, smem_k)
    B_tensorMap = B[:,:] @ Sm90_tensorMap(128, smem_n, smem_k)
    # ...
\end{verbatim}
}

\filbreak
Note the \lighttt{@ Sm90\_tensorMap(...)} annotation distinguishes this case from an ordinary WindowStmt, which does not override the MemWin type.
Similar to memory types, there are per-actor-kind creation, read, and write permissions.
The CUtensorMap can only be created in CPU code, and read and written through TMA code.

\filbreak
\minorKey{MemWin Template:} You can now define parameterized MemWin types with functions that return an inner MemWin class.
For example, the above \lighttt{Sm90\_tensorMap} example is defined as
{\color{lightttColor}
\begin{verbatim}
@memwin_template
def Sm90_tensorMap(swizzle, *smem_box):
    # ...
    class CUtensorMap(SpecialWindow):
        # define MemWin class as usual
    return CUtensorMap
\end{verbatim}
}
Where in the earlier example, \lighttt{swizzle = 128} and \lighttt{smem\_box = (smem\_m, smem\_k)} or \lighttt{(smem\_n, smem\_k)}.
This is cached: identically parameterized MemWins will be identical Python types.

\filbreak
\minorKey{Instr Classes:} Since CUDA instructions have so many parameters (e.g. per-parameter actor signatures), and we may have to generate a large number of similar instructions for different permutations of matrix sizes, I've implemented an alternative class-based interface for \lighttt{@instr}.
The class defines two special functions:
\begin{enumerate}
  \item \lighttt{behavior}: This is parsed as Exo code, defining the body of the \lighttt{instr} proc, in the same manner as the current function-based \lighttt{@instr} interface.
  \filbreak
  \item \lighttt{instance}: This is an executed Python member function, taking \lighttt{self} and a subset of parameters used in \lighttt{behavior}.
\end{enumerate}

\filbreak
All parameters for \lighttt{instance} must be constants in Exo object code.
The \lighttt{instance} function defines
\begin{enumerate}
  \item The Python format string for the C instruction, and C globals (pre-existing \lighttt{@instr} functionality)
  \filbreak
  \item Required include files and utility code for \lighttt{.cuh} file
  \filbreak
  \item Instruction required collective unit \& actor kind
  \filbreak
  \item Per-parameter MemWin type and actor signature
\end{enumerate}

\filbreak
See \lighttt{LoopIR.InstrInfo}.

\filbreak
\minorKey{With Context:} Overloaded \lighttt{with <ctx>:} statement.
For metaprogramming, we reserve \lighttt{with python:} and \lighttt{with exo:};
anything else is evaluated as a with-context object, which could be \lighttt{CudaDeviceFunction}, \lighttt{CudaWarps}, or \lighttt{CudaAsync}.

\filbreak
There is currently no LoopIR node for this.
Maybe we should add one, but that would entail auditing a huge amount of WIP code (e.g. the new analysis).

\filbreak
What I do for now is wrap the with-context object inside a \lighttt{LoopIR.Const}, which is wrapped as the \lighttt{cond} of \lighttt{LoopIR.If}.
I detect this arrangement with the special helper \lighttt{is\_if\_holding\_with}.
This hack works because all with-context objects are truthy, and most of the code treats this as \lighttt{if True}.
I just have special detection where the with-context is relevant, and also in \lighttt{simplify}, so the ``redundant'' \lighttt{if True} isn't simplified away.

\filbreak
\myTitle{Roadmap \& Time Management}

We're finally at the point where I have at least the skeleton of all Exo-GPU features implemented and lowered to CUDA code, so the handwave-y whiteboarding phase is over and we can study real examples.
The focus now will be on expanding the feature set, adding safety checks, and fixing issues where GPU code isn't meshing well with Exo's language design.
Also, dealing with the consequences of transitioning from ``prototype'' code to ``potentially forever'' code.

\filbreak
I predict I have four months (May-August, inclusive) to get solid work done without being derailed by classes.
My current hope is to get Exo-GPU fully implemented by then (for certain definitions of ``fully''), and just have to split my attention between paper writing and classes afterwards.
Likely, I'll have to put my senior engineer hat on, and make some needed tradeoffs to get this project delivered in time.

\filbreak
I anticipate four major blocks of work.

\filbreak
\mainSub{Mechanical Issues (Object Language Features)}

\begin{itemize}
  \item Clarify collective units and collective tilings: these make sense in my head, but the implementation is pretty obscure right now, and I don't have good error messages.
  \filbreak
  \item Similar issues exist with ``distributed memory'', where different slices of a tensor are physically stored in different thread collectives (e.g. a tensor distributed among the threads of a CTA).
  \filbreak
  \item Fix issues with windowing.
  \filbreak
  \item Fix issues with wgmma.
  \filbreak
  \item Support clusters and multicast.
\end{itemize}

\filbreak
\mainSub{Simple Synchronization Checking}

I already have a simulator for the synchronization model (SyncEnv) I proposed, which tracks the history of each read and write done.
The simulator is in C++ and for performance and sanity reasons, I'd like to keep it so.

\filbreak
The model needs improvements:
\begin{itemize}
  \item Reason about atomic reductions.
  \filbreak
  \item Reason about ``temporal only'' synchronization for write-after-read hazards, where we do not need to issue a memory fence as no actual value is communicated.
\end{itemize}
\filbreak
From there my plan is to compile Exo into an IR of sorts and feed it to the simulator, run on specific test cases.
As Exo currently exists, the actual data doesn't matter, so the test cases would consist only of sizes.
However, one of the benefits of this simple simulator is that it should be adaptible to relaxed restrictions on Exo programs.

\filbreak
Alternatively, I could interpret Exo using the JS backend .. if it can interface with C++.
Compile the simulator to asm.js, perhaps?

\filbreak
\minorSub{Abstracted Synchronization Checking (Optional?)}

Honestly, there is a high chance that this will be cut entirely from the schedule, with four months invested only in the previous two tasks.
But my original hope was to learn more about formal methods and use the simple synchronization model only as the basis for a more static-analysis-like system.
(I mean that was the original motivation of going to grad school, to improve my brain and learn skills and such).

\filbreak
There are some features that may make static analysis almost intractible.
In particular, split barriers may be difficult to model, as what values they protect are a function of the execution path at runtime.
Something jrk suggested was to evolve from the simple checking model, where common patterns are identified and simplified to code summaries that can be ``simulated'' much faster.

\filbreak
Another problem with this goal is that it interacts poorly with the goal of making Exo more expressive and moving away from restrictive rules about data/index value separation, etc.

\filbreak
If I pursue abstracted synchronization checking at all, I might give the simulator two interfaces:
\begin{itemize}
  \item A memory listener interface, allowing per-memory-access simulation suitable for any program.
  \item An IR-based interface, which the simulator could simplify, but would only support the subset of programs compilable to the restricted IR.
\end{itemize}

\filbreak
\mainSub{Paper Issues}

This is work I anticipate doing end of this year, overlapped with taking classes.

\filbreak
\mainKey{Replicating Algorithms:} Probably a painstaking process of reverse engineering existing algorithms (cutlass GEMM, FlashAttention-3, etc.) and scheduling an Exo program to match them exactly.
I may have to create some instrumentation system (or re-use Cutlass SyncLog) to understand how the algorithms work, and check that my Exo versions perform the same steps exactly.

\filbreak
\mainKey{Semantics \& Formal Stuff:} I have to learn to speak this language.
I'm particularly worried about convincing people (including myself) that my system of representing barriers in Exo-GPU even works.

\filbreak
\mainKey{Related Work:} I kept saying I would look into this (CuTe, cuTile, Cypress, Descend, Hoare Logic for GPUs, and so on) but never did ... in truth I should probably not put this off until the end, although I feel a lot of implicit time pressure for the project.

\filbreak
\myTitle{Discussion Issues}

\mainSub{Distributed Memory}

Each tensor allocation has two important \myKeyA{collective units}
\begin{itemize}
  \item The \myKeyA{alloc unit}: the collective unit for the alloc statement
  \item The \myKeyA{native unit}: collective unit physically needed to allocate the memory type (defined by \lighttt{MemWin} sub-class).
\end{itemize}
If these are not matched, we need to deduce 1 or more of the left-most dimensions as \myKeyA{distributed dimensions}.
The remaining dimensions describe the size of the tensor allocated in each native thread collective.
These \myKeyA{distributed slices} allocated by each native thread collective together comprise the full logical tensor.

\filbreak
The distributed dimensions are deduced from usage. Example from xgemm\_Sm80:

\filbreak
\blacktt{with CudaDeviceFunction(\yellowBox{blockDim=256}):}\\
\blacktt{~~for m2 in cuda\_tasks(0, M / 192):}\\
\blacktt{~~~~for n2 in cuda\_tasks(0, N / 256):}\\
\graytt{~~~~~~\# Each CTA generates a (192, 256) tile of C}\\
\blacktt{~~~~~~A\_smem: f32[3, 192, 16] @ \blueBox{CudaSmemLinear}}\\
\blacktt{~~~~~~B\_smem: f32[3, 16, 256] @ \blueBox{CudaSmemLinear}}\\
\blacktt{~~~~~~D\_rmem: f32[\greenBox{2}, \violetBox{4}, 6, 8, 16, 8] @ Sm80\_RmemMatrixD}\graytt{~~\# sm\_80 warp matrix}\\
\graytt{~~~~~~\# Each warp zeros its accumulators}\\
\blacktt{~~~~~~for \greenBox{mw} in cuda\_threads(0, 2, \greenBox{unit=4 * cuda\_warp}):}\graytt{~~\#~\greenBox{128} threads}\\
\blacktt{~~~~~~~~for \violetBox{nw} in cuda\_threads(0, 4, \violetBox{unit=cuda\_warp}):}\graytt{~~~~\#~\violetBox{32} threads}\\
\blacktt{~~~~~~~~~~for m\_seq in seq(0, 6):}\\
\blacktt{~~~~~~~~~~~~for n\_seq in seq(0, 8):}\\
\blacktt{~~~~~~~~~~~~~~Sm80\_mma\_zero\_d\_tf32(D\_rmem[\greenBox{mw}, \violetBox{nw}, m\_seq, n\_seq, 0:16, 0:8])}\\

\filbreak
The alloc unit for \lighttt{A\_smem} and \lighttt{B\_smem} (one CTA of \yellowBox{256} threads) matches the native unit for \texttt{\blueBox{CudaSmemLinear}}, so no dimensions are distributed.
However, the native unit for \lighttt{D\_rmem} is a warp.
The \greenBox{\texttt{mw}}, \violetBox{\texttt{nw}} loops divide the CTA into a $2 \times 4$ grid of warps ($2 \times 4 \times 32 = \yellowBox{256}$).
One distributed slice of size \lighttt{[6, 8, 16, 8]} is allocated in each warp.

\filbreak
If multiple usages of the memory occur (as is the case for the full gemm algorithm), then each distributed slice must be accessed consistently by the same thread collective.
I have code for this, although it's extremely restrictive, and doesn't handle the full generality that I intend.

\filbreak
In the current design, the distributed dimensions are not explicitly encoded in Exo object code; it's deduced by the compiler.
This means we have to clearly describe how the language deduces these distributed dimensions, and be able to give clear messages when this deduction fails.
I feel there's something that could be produced from the following two requirements:

\begin{itemize}
\item Each indexing variable is generated as an expression of \lighttt{threadIdx} or \lighttt{blockIdx}.
In the above example, \lighttt{\greenBox{mw} = threadIdx.x / 128}, and \lighttt{\violetBox{nw} = (threadIdx.x \% 128) / 32}.
So to enforce the ``consistent thread collective'' requirement, we need to check the index expressions are always logically (but not necessarily syntactically) equivalent.
Note that the expressions are \textit{not} used in the lowered code (at least not here).
\filbreak
\item We can view each index variable as a ``tiling operator'' that takes a larger number of threads and divides it into a smaller number of threads.
From the usage (e.g. \lighttt{[\greenBox{mw}, \violetBox{nw}, ...]}), the compiler needs to deduce a valid ``chain'' of tiling operators converting from the alloc unit to the native unit.
In this case, \yellowBox{256}, \greenBox{mw: $256 \to 128$}, \violetBox{nw: $128 \to 32$}.
\end{itemize}

\filbreak
We ought to be able to do this even if the operators are out of order, as in the following example yielding
\yellowBox{256}, \violetBox{nw: $128 \to 32$}, \greenBox{mw: $256 \to 128$}:

\blacktt{~~~~~~D\_rmem: f32[\violetBox{4}, \greenBox{2}, 6, 8, 16, 8] @ Sm80\_RmemMatrixD}\graytt{~~\# sm\_80 warp matrix}\\
\graytt{~~~~~~\# Each warp zeros its accumulators}\\
\blacktt{~~~~~~for \greenBox{mw} in cuda\_threads(0, 2, \greenBox{unit=4 * cuda\_warp}):}\graytt{~~\#~\greenBox{128} threads}\\
\blacktt{~~~~~~~~for \violetBox{nw} in cuda\_threads(0, 4, \violetBox{unit=cuda\_warp}):}\graytt{~~~~\#~\violetBox{32} threads}\\
\blacktt{~~~~~~~~~~for m\_seq in seq(0, 6):}\\
\blacktt{~~~~~~~~~~~~for n\_seq in seq(0, 8):}\\
\blacktt{~~~~~~~~~~~~~~Sm80\_mma\_zero\_d\_tf32(D\_rmem[\violetBox{nw}, \greenBox{mw}, m\_seq, n\_seq, 0:16, 0:8])}\\

\filbreak
\minorSub{(Optional) Distributed Memory Additional Notes}

In the previous example, the distributed dimensions were deduced from a window expression passed to an \lighttt{instr}, \lighttt{Sm80\_mma\_zero\_d\_tf32}.
This works because the collective unit for the \lighttt{instr} (\lighttt{cuda\_warp}) matches the native unit for the allocation.
This won't be the case for multicast and swizzling instructions, which will take windows containing distributed fragments from multiple native thread collectives (for example, 1 SMEM tile from each CTA in the cluster).
I haven't figured out how to handle this yet.

An alternative design would be to encode distributed memory information directly.
The current deduction-based design may be an artificial challenge, but I currently don't want to move to explicit typing because of the differing usage patterns for existing Exo scheduling (S-semantics) and synchronization checking (M-semantics):
\begin{itemize}
  \item Normal Exo scheduling requires that each scheduling operation is correct in isolation under S-semantics (so the full chain of equivalence is preserved). In particular, the program \textit{always typechecks} after each scheduling operation.
  \item For parallelism checking, we only check equivalence between S-semantics and M-semantics for the final scheduled proc. Intermediate procs are never checked for correctness.
\end{itemize}
If we encoded distributed dimensions -- which is fundamentally a parallelism concept -- as part of the LoopIR type system, it'll break the \textit{always typechecks} invariant, as intermediate procs won't have correct parallelism.
So currently, I propose we don't.

\filbreak
I freely admit this rationale may be bogus though; the other reason is just that this lightens the cognitive load for scheduling in the common case.

\filbreak
\mainSub{Window Improvements}

This is a bit of a tough topic, because the original Exo window system was so overfit to hard-wired (C pointer, strides)-based windows, which isn't scaling that well to Exo-GPU.
At the same time, doing a ``perfect job'' could be a huge time sink that I can't afford.
I'm hoping to farm up some pragmatic ideas for addressing these issues:

\filbreak
\mainKey{window() Callback:} The most surface-level issue is just that the \lighttt{MemWin.window()} callback is really limited, operating mainly on C strings, and being originally designed to only support the case of offseting a pointer.
It also exposes a raw undocumented \lighttt{LoopIR} type to the user (\lighttt{basetyp}) which is needed to special-case on the value of \lighttt{basetype.is\_win()}.
I perceive similar C-string limitations for the \lighttt{str.format} based \lighttt{instr} code generation.

\filbreak
\mainKey{Dense Dimensions:} For certain usages, we need to be able to enforce that up to N-many rightmost dimensions of a window are tightly packed.
Currently, this only seems to be possible for $N=0$ or $N=1$.

\filbreak
\mainKey{Alignment:} We need to be able to reason globally about the alignment of pointers.
For example, the f32 \lighttt{instr} for \lighttt{cp.async} is defined to copy 1, 2, or 4 f32 values:
{\color{lightttColor}
\begin{verbatim}
@instr
class Sm80_cp_async_f32(cp_async_impl):
    def behavior(size: size, smem: [f32][size] @ CudaSmemLinear,
                 gmem: [f32][size] @ CudaGmemLinear):
        assert stride(smem, 0) == 1
        for i in seq(0, size):
            smem[i] = gmem[i]

    def instance(self, size):
        self.instance_impl(4 * size)  # Omitted, codegen 4, 8, or 16 byte cp.async
\end{verbatim}
}
The 8 or 16 byte cp.async versions won't work if the generated pointers are not 8 or 16 byte aligned, but we currently have no way to enforce that (e.g. we crash if we issue \lighttt{Sm80\_cp\_async\_f32(smem[\redBox{2:6}], gmem[0:4], size=4)}).

\filbreak
\mainKey{Hardware Vectors:} Related to the density and alignment issues, we will more commonly see a pattern where the rightmost dimensions of a tensor correspond to ``hardware vectors'', which map to fixed-size vectors/matrices supported by hardware accelerators.
We already support this with AVX, e.g. the following code

\blacktt{~~~~foo: f32[2, 4, \blueBox{8}] @ AVX2}

lowers to

\blacktt{~~~~\blueBox{\_\_m256} f32[2][4];}

where the final \blueBox{8} dimension is mapped to the lanes of the \lighttt{\_\_m256} register.

\filbreak
Similarly, the previous example of the \lighttt{D\_rmem} accumulator

\blacktt{~~~~D\_rmem: f32[\violetBox{4}, \greenBox{2}, 6, 8, \blueBox{16, 8}] @ Sm80\_RmemMatrixD}\graytt{~~\# sm\_80 warp matrix}

lowers to

\blacktt{~~~~unsigned D\_rmem[6][8]\blueBox{[4]};}\graytt{~~// Quirk: tf32 values stored bit-cast to u32}

where the dimensions of the Exo \lighttt{D\_rmem} tensor are categorized, from left-to-right, as

\filbreak
\begin{itemize}
  \item \myKeyA{Distributed Dimensions:} \lighttt{[\violetBox{4}, \greenBox{2},...}, which are absent in the lowered code.
  \filbreak
  \item \myKeyA{Ordinary Dimensions:} \lighttt{...6, 8, ...}, which are lowered directly.
  \filbreak
  \item \myKeyA{Hardware Vector Dimensions:} \lighttt{..., \blueBox{16, 8}]}, which are lowered to a hardware-defined vector. In this case, it's lowered to 4 32-bit registers. sm\_80 tensor cores distribute the 128 values of a $16 \times 8$ tile across 32 threads of a warp, so $128 / 32 = 4$ registers are used.
\end{itemize}

\filbreak
We may want to add support for this hardware vector pattern to core Exo, rather than enforce this over and over again as asserts and other checks placed into MemWin code.
In particular, hardware vectors actually sort of don't interact well with \lighttt{stride} expressions (another case where window = \{C pointer + strides\} is baked into the Exo language) which we would otherwise use to implement these checks.

\filbreak
Strides are already a bit messy for AVX (since the underlying C code strides in units of AVX vectors -- 16 or 32 bytes -- and not scalars), but it breaks down even more for CUDA warp/warpgroup matrices, where values are stored in different registers across different threads (neither concept matches \lighttt{stride} well, as these are not addressible).

\filbreak
\mainSub{Synchronization Issues}

The design for synchronization statements (\lighttt{Fence}, \lighttt{\{Reverse\}Arrive}, \lighttt{\{Reverse\}Await}) is unlike the rest of Exo in that they're really declarative and rely on a monolithic (non-extensible) compiler to lower correctly.
The user specifies the \textit{intent} of the synchronization via the \myKeyA{actor kind} parameters, and the (implicit) \myKeyA{thread collective} used to execute the synchronization statements.

\filbreak
The idea is that working at this level makes synchronization checking feasible, in a way it maybe wouldn't be for analyzing raw CUDA synchronization primitives (which are not really defined in any formal, precise manner).
However, this leaves some issues:

\filbreak
\begin{itemize}
  \item Practical issue: Convincing people this is not bogus, and actually maps to the hardware reality.
  \filbreak
  \item What I really care about: Convincing myself that codegen is correct, and actually implements what I intend for all supported combinations of actor kind and collective units (and not just for a few simple test cases).
  Unsupported cases must be clearly rejected.
  \filbreak
  \item Meta-synchronization: My proposed synchronization checking only reasons about whether numerical \textit{values} are synchronized correctly, and doesn't reason at all that access to synchronization primitives are themselves synchronized correctly.
    It's the Exo-GPU compiler's job to follow requirements such as ``For each phase of the mbarrier object, at least one test\_wait or try\_wait operation must be performed which returns \lighttt{True} for \lighttt{waitComplete} before an arrive-on operation in the subsequent phase'' which is a non-trivial requirements when separate pools of threads are performing the arrive-on and wait operations.
    This will require some reasoning on my part, and likely some restrictions on synchronization patterns.
\end{itemize}

\filbreak
I'll have to figure out how to address these issues in a paper.

\filbreak
Finally, I'm really depressed about how it's \textit{actually impossible} to guarantee correct usage of wgmma through C++, which somewhat undermines the point of a USL.
The main issue is that wgmma is defined to operate on registers asynchronously, but nvcc knows nothing of this, and may generate PTX code that accesses wgmma registers without synchronization, e.g.:

\filbreak
\lighttt{~~~~mov.b32 	\redBox{\%f1854}, \yellowBox{\%r55};}\\
\lighttt{~~~~wgmma.mma\_async.sync.aligned.m64n128k8.f32.tf32.tf32 \{\redBox{\%f1854}, ...}\\
\lighttt{~~~~mov.b32 	\yellowBox{\%r276}, \redBox{\%f1854};}

\filbreak
Here the C++ compiler for whatever reason migrated a single C++ variable from \lighttt{\yellowBox{\%r55}} to \lighttt{\redBox{\%f1854}} to \lighttt{\yellowBox{\%r276}}.

\filbreak
The PTX-to-SASS compiler recognizes this failure mode and conservatively inserts syncs, so supposedly your code will still run correctly (just much more slowly).
This also can impact seemingly correct code, which is a separate issue.

\filbreak
However, recently I've had some code with fail to work entirely, and I'm not sure if it's my fault somewhere else, or a failure in the compiler's conservative synchronization.
This fills me with paranoia.
I'm trying (rather angrily) to get some understanding of how the C++-to-PTX compiler works and how to avoid its failure modes, but in the end, I can't give a guarantee of correctness.

\filbreak
This would potentially be solvable if we generated PTX directly (instead of going through CUDA C++) but I highly doubt this is reasonably in-scope for this project.

\end{document}
