% python3 code_to_tex.py coll_samples.py coll_samples && xelatex </dev/null coll_algebra.tex

\input{whitepaper_common.tex}

\tikzstyle{redsmallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowsmallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greensmallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluesmallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetsmallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]

\tikzstyle{colltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=black, fill=white, text=black]
\tikzstyle{redcolltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowcolltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greencolltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluecolltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetcolltiling} = [rectangle, minimum width=16mm, minimum height=1.6cm, text width=16mm, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]


\begin{document}
\myTitle{Exo GPU (Spork) Collective Algebra}

Thoughts on how to formally-ish model subdividing groups of threads, thread indexing, and thread convergence (e.g. for barriers or warp/warpgroup tensor core MMA).
This gets used to describe how distributed memory works in Exo-GPU.

\filbreak
\mainKey{Collective Unit:} A ``unit of measurement'' for threads arranged in a certain shape.
For example ``32 consecutive threads'' is a collective unit.
This would be a warp if aligned, but in Spork, alignment is not generally a property of a collective unit.
We only check alignment as an additional requirement when matching collective units for lowering \lighttt{instr} and barriers.

\filbreak
Nevertheless, we use familiar terms like ``warp'' for user-facing code; this is only an abuse-of-notation in the rare case that the user chooses to misalign threads.

\filbreak
Collective units need not be contiguous.
For example, a quadpair -- two groups of four threads each, offset by 16 from each other -- is a collective unit, and is distinct in shape from 8 contiguous threads.

\filbreak
\mainKey{Thread Collective:} A grouping of threads dynamically created at runtime.
The shape of the group is described by a collective unit.
For example, threads 0-31, threads 32-63, threads 64-95 ... are all distinct thread collectives that share the collective unit ``warp''.
Threads [0, 3] $\cup$ [16, 19], threads [4, 7] $\cup$ [20, 23], etc. are distinct thread collectives with collective unit ``quadpair''.

\filbreak
A parallel-for loop causes a distinct thread collective to be assigned to execute each iteration.

\filbreak
\mainKey{Top-level Collective:} A CUDA cluster, including the degenerate case of a single CTA if \lighttt{clusterDim = 1} (as is usually the case).
This terminology can generalize to other APIs.

\filbreak
\mainKey{Collective Tiling:} A collective tiling either describes the top-level collective, or describes how the thread collectives for executing statements within a parallel for loop or specialization statement are generated.
Collective tilings are arranged in a parent-child relationship, with each parallel for loop creating a new child that suballocates threads from the parent, and with the top-level parent being the top-level collective.

\filbreak
The primary purpose of this document is to build up to a full description of the collective tiling, and the principles (but not necessarily the imperative details) of how it's constructed.
\textbf{Throughout the Exo codebase -- for both internal and user-facing code -- } we will use ``\blacktt{coll}'' consistently as an abbreviation for ``collective''.
The word ``collective'' is too long, and if we don't create an official abbreviation, the temptation too create incompatible, ad-hoc abbreviations is just too high.

\filbreak
\myTitle{Domain}

The first thing we'll do is arrange the threads of the top-level collective into an N-dimensional ``domain grid''.
The \myKeyA{full domain} describes the size along each dimension of this grid.
For a CUDA cluster, this is $D = (\lighttt{clusterDim}, \lighttt{blockDim})$.
This could generalize to non-CUDA APIs, but there's a built-in assumption that the left-most dimension is the ``most significant'', so we \myKeyA{linearize} thread indices in lexicographical order.

A domain must consist only of positive integers, and \textit{should} contain only values at least 2.
However, 1 values may result when substituting parameters (as would occur in the common case that \lighttt{clusterDim = 1}).
We can fix this later, as part of domain completion.

\filbreak
The thread count of a domain is the product of its coordinates.
A \myKeyA{partial domain} is a domain whose total thread count is lower than that of the top-level collective.
For example, \lighttt{(blockDim,)} is a partial domain if \lighttt{clusterDim != 1}.

\filbreak
Jumping ahead somewhat, physical threads are arranged in the domain grid by the top-level collective tiling's \myKeyA{intra-box expressions}.
For a CUDA cluster, these expressions are (\lighttt{blockIdx.x \% clusterDim}, \lighttt{threadIdx.x}).

\filbreak
\minorSub{Domain Completion (Detail)}

Collective units and collective tilings are described in terms of their domain.
Often, their domains are not the same.
We reconcile this with domain completion, translating both to a common domain.
Domain completion consists of (up to) 3 steps in order.

\filbreak
Here, we only describe the steps for modifying the domain.
Whenever we modify the $n^{th}$ coordinate of a domain, we modify in parallel the $n^{th}$ coordinate of each attribute of a collective unit or collective tiling.
We'll describe this later.

\filbreak
\minorKey{Partial Prepend:} If the source domain is partial, we prepend $\frac{t}{s}$, where $t$ is the thread count of the full domain and $s$ is the thread count of the partial domain.
If this is not an integer, this step fails.

Side note, this is the main place where the left-most $\equiv$ ``most significant'' assumption is made.

\filbreak
\minorKey{Coordinate Removal:} Each domain coordinate that's 1 is removed.

\filbreak
\minorKey{Coordinate Splitting:} Domain coordinate values $c$ may be split into two consecutive coordinates $(\frac{c}{f}, f)$, where $f$ is the \myKeyA{splitting factor}.

\filbreak
\minorSub{Domain Completion Example}

Let's say we have two domains

\textbf{A:} (1, 64, 16)\\
\textbf{B:} (16, 128)

with \textbf{A} being a \myKeyA{partial domain} ($s = 1024$ threads) and \textbf{B} being a \myKeyA{full domain} ($t = 2048$ threads).
This is not very realistic for the CUDA use case; it's just for illustration.

\filbreak
\minorKey{Partial Prepend:} Since \textbf{A} is a partial domain, we prepend $2 = \frac{t}{s}$.

$\downarrow$ {\color{grayttColor} (1, 64, 16)} \\
\textbf{A:} (\blueBox{2}, 1, 64, 16)

$\downarrow$ {\color{grayttColor} (16, 128)} \\
\textbf{B:} (16, 128)

\filbreak
\minorKey{Coordinate Removal:} Remove the redundant 1 in \textbf{A}.

$\downarrow$ {\color{grayttColor} (2, \redBox{1}, 64, 16)} \\
\textbf{A:} (2, 64, 16)

$\downarrow$ {\color{grayttColor} (16, 128)} \\
\textbf{B:} (16, 128)

\filbreak
\minorKey{Coordinate Splitting:} Multiple splits are required to reconcile \textbf{A} and \textbf{B}.

$\downarrow$ {\color{grayttColor} (2, \violetBox{64}, 16)} \\
\textbf{A:} (2, \violetBox{8, 8}, 16) \\
Splitting factor: \violetBox{8}

$\downarrow$ {\color{grayttColor} (\yellowBox{16}, \greenBox{128})} \\
\textbf{B:} (\yellowBox{2, 8}, \greenBox{8, 16}) \\
Splitting factors: \yellowBox{8}, \greenBox{16}

\filbreak
\myTitle{Collective Unit}

A collective unit consists of a \myKeyA{domain} and \myKeyA{box} size.
The box is $\textit{Optional}[\mathbb{Z}]^N$-valued, where $N$ is the dimension of the domain.
If the box contains a None coordinate, it's said to be \myKeyA{agnostic} on that dimension, and can match any integer value.

\filbreak
Given a certain domain, the collective unit for a thread collective is defined by the said domain and the shape of the filled axis-aligned box the threads form when arranged in the domain grid.\footnote{If the threads don't form a filled box, no collective unit with the given domain can match the thread collective.}
For example, with the domain being (\lighttt{clusterDim}, \lighttt{blockDim}), the box for a single warp is (1, 32), while the box for ``one warp chosen from each CTA in the cluster'' is (\lighttt{clusterDim}, 32) \textbf{(figure \ref{fig:units})}.

\filbreak
Due to domain completion, the ``warp'' collective unit can be described with domain=(\lighttt{blockDim},), box=(32,).
This is (almost) equivalent to the warp given in the example.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=2mm]
\node(t000) [smallnode] {0,0};
\node(t001) [smallnode, right=of t000] {0,1};
\node(t002) [smallnode, right=of t001] {0,2};
\node(t031) [smallnode, right=of t002, xshift=4mm] {0,31};
\draw [dotted] (t002) -- (t031);
\node(t032) [bluesmallnode, right=of t031] {0,32};
\node(t033) [bluesmallnode, right=of t032] {0,33};
\node(t034) [bluesmallnode, right=of t033] {0,34};
\node(t063) [bluesmallnode, right=of t034, xshift=4mm] {0,63};
\draw [dotted] (t034) -- (t063);

\node(t100) [greensmallnode, below=of t000] {1,0};
\node(t101) [greensmallnode, right=of t100] {1,1};
\node(t102) [greensmallnode, right=of t101] {1,2};
\node(t131) [greensmallnode, right=of t102, xshift=4mm] {1,31};
\draw [dotted] (t102) -- (t131);
\node(t132) [bluesmallnode, right=of t131] {1,32};
\node(t133) [bluesmallnode, right=of t132] {1,33};
\node(t134) [bluesmallnode, right=of t133] {1,34};
\node(t163) [bluesmallnode, right=of t134, xshift=4mm] {1,63};
\draw [dotted] (t134) -- (t163);

\node(t200) [smallnode, below=of t100] {2,0};
\node(t201) [smallnode, right=of t200] {2,1};
\node(t202) [smallnode, right=of t201] {2,2};
\node(t231) [smallnode, right=of t202, xshift=4mm] {2,31};
\draw [dotted] (t202) -- (t231);
\node(t232) [bluesmallnode, right=of t231] {2,32};
\node(t233) [bluesmallnode, right=of t232] {2,33};
\node(t234) [bluesmallnode, right=of t233] {2,34};
\node(t263) [bluesmallnode, right=of t234, xshift=4mm] {2,63};
\draw [dotted] (t234) -- (t263);

\node(t300) [smallnode, below=of t200] {3,0};
\node(t301) [smallnode, right=of t300] {3,1};
\node(t302) [smallnode, right=of t301] {3,2};
\node(t331) [smallnode, right=of t302, xshift=4mm] {3,31};
\draw [dotted] (t302) -- (t331);
\node(t332) [bluesmallnode, right=of t331] {3,32};
\node(t333) [bluesmallnode, right=of t332] {3,33};
\node(t334) [bluesmallnode, right=of t333] {3,34};
\node(t363) [bluesmallnode, right=of t334, xshift=4mm] {3,63};
\draw [dotted] (t334) -- (t363);

\node(legend) [smallnode, text width=15cm, draw=none, above=of t032] {intra-box exprs = \blacktt{(blockIdx.x \% clusterDim, threadIdx.x)}};
\end{tikzpicture}
\caption{With \lighttt{clusterDim = 4}, \lighttt{blockDim = 64}, and domain = $(4, 64)$, we highlight examples of thread collectives with collective units \greenBox{single warp} (box $(1, 32)$) and \blueBox{warp per CTA} (box $(\lighttt{clusterDim}, 32)$). Note a single row of this grid corresponds to one CTA in a cluster.}
\label{fig:units}
\end{figure*}

\filbreak
\minorSub{Box Domain Completion}

When doing domain completion, the corresponding steps for the box attribute are:

\filbreak
\minorKey{Partial Prepend:} If we are matching collective units (e.g. to enforce an \lighttt{instr}'s requirements), prepend 1.
Otherwise, prepend None (agnostic dimension).

\filbreak
\minorKey{Coordinate Removal:} All removed coordinate values must be 1.

\filbreak
\minorKey{Coordinate Splitting:} A None value is split into (None, None).
An integer value $c$ splits into $(1, c)$ if $c < f$; otherwise, it splits into $(\frac{c}{f}, f)$.
If the results are not integers, the domain completion fails.

\filbreak
If we were accepting unaligned thread collectives, collective unit domain completion (as an unintended side-effect) makes the collective unit stricter, as some unaligned thread collectives will no longer form valid boxes in the new domain.
Domain completion won't affect matching of aligned thread collectives.

\filbreak
\mainSub{Collective Unit Code Details}

The domain and box are tuples of \lighttt{CollSizeExpr}, which are converted to integers by looking up the \myKeyA{collective environment} (\lighttt{CollParam -> int}), which maps \lighttt{clusterDim} and \lighttt{blockDim} to concrete values.
The box may contain None (agnostic dimensions).
The syntax \lighttt{scale * unit} means \lighttt{unit} with the $0^{th}$ box coordinates scaled by \lighttt{scale}.
(e.g. \lighttt{8 * warp} means 8 warps, i.e. \lighttt{CollUnit((blockDim,), (8 * 32,), ...)})

\filbreak
\myTitle{Collective Tiling}

The \lighttt{CollTiling} describes which threads in the top-level collective (cluster/CTA) are active and how they are arranged into thread collectives.
Each \lighttt{cuda\_threads} loop and \lighttt{with CudaWarps} statement defines a new collective tiling for its body.
Summarizing for now, the collective tiling contains:
\begin{itemize}
  \item \myKeyA{Domain:} $\mathbb{Z}^N_+$, as described; this is always a complete domain
  \item \myKeyA{Box:} $\mathbb{Z}^N_+$, ``shape'' of active threads comprising thread collectives
\end{itemize}
\filbreak
which together describe the (not-necessarily-aligned) \myKeyA{collective unit} of the thread collectives; further contains
\begin{itemize}
  \item \myKeyA{Parent:} \lighttt{Optional[CollTiling]}, the source of thread collectives that are subdivided to yield the new thread collectives (None indicates top-level collective).
  \filbreak
  \item \myKeyA{Tile:} $\mathbb{Z}^N_+$, subdivision tile size
  \filbreak
  \item \myKeyA{Offset:} $\mathbb{Z}^N_{\ge 0}$, location of box (of active threads) within the tile
  \filbreak
  \item \myKeyA{Tile Count:} $\mathbb{Z}_{\ge 0}$
\end{itemize}
which together define the repeating pattern of active/non-active threads; finally, the collective tiling contains
\filbreak
\begin{itemize}
  \item \myKeyA{Intra-Box Exprs:} \lighttt{CollIndexExpr}$^N$
\end{itemize}
which defines a bijective mapping between active threads and the integer coordinates within the box (for CUDA, this is a function of \lighttt{threadIdx.x} and \lighttt{blockIdx.x}).

\filbreak
\mainSub{Parallel Loop Behavior}

I'll describe how a collective tiling assigns physical threads to ``iterations'' of a \lighttt{for j in cuda\_tasks(0, c)} loop, and which are masked out (inactive).

\filbreak
\mainKey{Invariants:} Let $T$ be the \myKeyA{tile} size of this collective tiling, and $B^P$ the \myKeyA{box} size of the \textit{parent} collective tiling.
In the non-degenerate case, there is exactly one dimension index that is the \myKeyA{tiled dimension} $d$, where $T_d \ne B^P_d$.
The tile count is $c$ (loop iteration count), with $c \le B^P_d / T_d$.
In the degenerate case, $T = B^P$, and $c = 0$ or $c = 1$.

\filbreak
Further, let $B$ and $O$ be the \myKeyA{box} and \myKeyA{offset} of this collective tiling.\\
For all dimensions $x$, $0 < B_x \le B_x + O_x \le T_x$.

\filbreak
(Assume for now that \minorKey{domain completion} was done, so the dimensionality of $T$, $B$, $O$, $B^P$ match).

\filbreak
\mainKey{Tile Size Calculation}

A \lighttt{cuda\_threads} loop defines a new collective tiling in-scope in the loop body, using the collective tiling of the outer scope:

\input{coll_samples/cuda_threads.0.tex}

In common cases, the collective unit of the loop body is the same as the \lighttt{unit} parameter specified.
However, the collective unit may be modified to adapt to the parent collective tiling.
The precise tile size calculation for the new collective tiling is:
\begin{itemize}
  \item Let $B^U$ be the box of the \lighttt{unit} parameter, after domain completion.
    The box may be agnostic on some dimensions.
    Note the domain $D$ for the \lighttt{unit} parameter, parent collective tiling, and new collective tiling are by definition identical after domain completion.
  \filbreak
  \item Define the tiled dimension $d$ as the index $i$ such that
  \begin{itemize}
    \item $B^U_i$ is not \lighttt{None} (i.e. the \lighttt{unit} parameter is not agnostic on the $i^{th}$ dimension)
    \item $B^U_i \ne D_i$
    \item $B^U_i \ne B^P_i$
  \end{itemize}
  \filbreak
  \item If $d$ is not defined, then this is a degenerate tiling. Set $T = B^P$.
  \filbreak
  \item If $d$ is ambiguous, the \lighttt{cuda\_threads} loop fails to compile.
  \filbreak
  \item If the tiling is not degenerate, set $T_i = B^P_i$ for $i \ne d$ and set $T_d = B^U_d$.
  \filbreak
  \item Set $B = T$, and $O$ to all 0s, i.e. all threads in the tile are active. This can be changed inside a \lighttt{with CudaWarps} block.
\end{itemize}

\filbreak
Note the collective unit for the loop body is primarily defined from the parent ($B^P$); it's only modified one the one dimension that ``matters'' ($d$).

\filbreak
In \textbf{(figure \ref{fig:warp_tiling})}, domain completion causes the unit \lighttt{cuda\_warp}, which originally had domain \lighttt{(blockDim,)} and box \lighttt{(32,)}, to be completed to domain $D$ = \lighttt{(clusterDim, blockDim)} and box $B^U$ = \lighttt{(None, 32)}.
This leads to the true collective unit for the loop body to have domain \lighttt{(clusterDim, blockDim)} and box \lighttt{(\violetBox{4}, 32)}.
This 4 is inherited from $B^P$ = \lighttt{(\violetBox{4}, 128)}.
The for loop ``does the right thing'' implicitly for the \lighttt{blockIdx} dimension, which is not the focus of the \lighttt{cuda\_warps} loop.

\filbreak
\mainKey{Tiling Pattern:} Let $I^P$ be the intra-box exprs of threads in the parent box.
Then the tile index of a thread is $j = \lfloor I^P_d / T_d \rfloor$ ($j = 0$ for the degenerate case).
This expression is the \lighttt{CollIndexExpr} for the iteration variable $j$.
Threads, if not disabled by the rules below, are assigned to execute iteration $j$.

\filbreak
\mainKey{Specialization:} The box $B$ and offset $O$ together define the subset of threads that are active in each tile.
We define the intra-box exprs of this collective tiling $I$ from that of the parent collective tiling $I^P$ as
\begin{itemize}
  \item $I_x = I^P_x - O_x$ for $x \ne d$ (all coordinates in degenerate case).
  \item $I_d = I^P_d~\%~T_d - O_d$
\end{itemize}
If, for any dimension $x$, $I_x < 0$ or $I_x \ge B_x$, then the thread with that intra-box expr is disabled
\textbf{(figure \ref{fig:cta_tiling})}.

\filbreak
\mainKey{Excess Threads:} Threads with $j \ge c$ are disabled \textbf{(figure \ref{fig:warp_tiling})}.
Note this is inevitable if $T_d$ does not evenly divide $B^P_d$ (not shown in figure).

\filbreak
\mainKey{Thread Pitch:} This is the offset, in number of (linearized) threads, between the 0th thread of a thread collective and the 0th thread of the adjacent thread collective in a tiling.
For degenerate tilings, or tilings with no more than 1 tile, this is defined to be 0.
Otherwise, this is $T_d \times D_{d+1} \times ... \times D_{N-1}$ where $D$ is the domain.

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=2mm]
\node(t00L) [colltiling] {$I^P$=0,0\\$I$=0,\redBox{-64}\\$j$=0};
\node(t00R) [colltiling, right=of t00L, xshift=4mm] {$I^P$=0,31\\$I$=0,\redBox{-33}\\$j$=0};
\draw [dotted] (t00L) -- (t00R);
\node(t01L) [colltiling, right=of t00R] {$I^P$=0,32\\$I$=0,\redBox{-32}\\$j$=0};
\node(t01R) [colltiling, right=of t01L, xshift=4mm] {$I^P$=0,63\\$I$=0,\redBox{-1}\\$j$=0};
\draw [dotted] (t01L) -- (t01R);
\node(t02L) [yellowcolltiling, right=of t01R] {$I^P$=\blueBox{0,64}\\$I$=0,0\\$j$=0};
\node(t02R) [yellowcolltiling, right=of t02L, xshift=4mm] {$I^P$=0,95\\$I$=0,31\\$j$=0};
\draw [dotted] (t02L) -- (t02R);
\node(t03L) [colltiling, right=of t02R] {$I^P$=0,96\\$I$=0,\redBox{32}\\$j$=0};
\node(t03R) [colltiling, right=of t03L, xshift=4mm] {$I^P$=0,127\\$I$=0,\redBox{63}\\$j$=0};
\draw [dotted] (t03L) -- (t03R);

\node(t10L) [colltiling, below=of t00L] {$I^P$=1,0\\$I$=0,\redBox{-64}\\$j$=1};
\node(t10R) [colltiling, right=of t10L, xshift=4mm] {$I^P$=1,31\\$I$=0,\redBox{-33}\\$j$=1};
\draw [dotted] (t10L) -- (t10R);
\node(t11L) [colltiling, right=of t10R] {$I^P$=1,32\\$I$=0,\redBox{-32}\\$j$=1};
\node(t11R) [colltiling, right=of t11L, xshift=4mm] {$I^P$=1,63\\$I$=0,\redBox{-1}\\$j$=1};
\draw [dotted] (t11L) -- (t11R);
\node(t12L) [greencolltiling, right=of t11R] {$I^P$=\blueBox{1,64}\\$I$=0,0\\$j$=1};
\node(t12R) [greencolltiling, right=of t12L, xshift=4mm] {$I^P$=1,95\\$I$=0,31\\$j$=1};
\draw [dotted] (t12L) -- (t12R);
\node(t13L) [colltiling, right=of t12R] {$I^P$=1,96\\$I$=0,\redBox{32}\\$j$=1};
\node(t13R) [colltiling, right=of t13L, xshift=4mm] {$I^P$=1,127\\$I$=0,\redBox{63}\\$j$=1};
\draw [dotted] (t13L) -- (t13R);

\node(t20L) [colltiling, below=of t10L] {$I^P$=2,0\\$I$=0,\redBox{-64}\\$j$=2};
\node(t20R) [colltiling, right=of t20L, xshift=4mm] {$I^P$=2,31\\$I$=0,\redBox{-33}\\$j$=2};
\draw [dotted] (t20L) -- (t20R);
\node(t21L) [colltiling, right=of t20R] {$I^P$=2,32\\$I$=0,\redBox{-32}\\$j$=2};
\node(t21R) [colltiling, right=of t21L, xshift=4mm] {$I^P$=2,63\\$I$=0,\redBox{-1}\\$j$=2};
\draw [dotted] (t21L) -- (t21R);
\node(t22L) [yellowcolltiling, right=of t21R] {$I^P$=2,64\\$I$=0,0\\$j$=2};
\node(t22R) [yellowcolltiling, right=of t22L, xshift=4mm] {$I^P$=2,95\\$I$=0,31\\$j$=2};
\draw [dotted] (t22L) -- (t22R);
\node(t23L) [colltiling, right=of t22R] {$I^P$=2,96\\$I$=0,\redBox{32}\\$j$=2};
\node(t23R) [colltiling, right=of t23L, xshift=4mm] {$I^P$=2,127\\$I$=0,\redBox{63}\\$j$=2};
\draw [dotted] (t23L) -- (t23R);

\node(t30L) [colltiling, below=of t20L] {$I^P$=3,0\\$I$=0,\redBox{-64}\\$j$=3};
\node(t30R) [colltiling, right=of t30L, xshift=4mm] {$I^P$=3,31\\$I$=0,\redBox{-33}\\$j$=3};
\draw [dotted] (t30L) -- (t30R);
\node(t31L) [colltiling, right=of t30R] {$I^P$=3,32\\$I$=0,\redBox{-32}\\$j$=3};
\node(t31R) [colltiling, right=of t31L, xshift=4mm] {$I^P$=3,63\\$I$=0,\redBox{-1}\\$j$=3};
\draw [dotted] (t31L) -- (t31R);
\node(t32L) [greencolltiling, right=of t31R] {$I^P$=3,64\\$I$=0,0\\$j$=3};
\node(t32R) [greencolltiling, right=of t32L, xshift=4mm] {$I^P$=3,95\\$I$=0,31\\$j$=3};
\draw [dotted] (t32L) -- (t32R);
\node(t33L) [colltiling, right=of t32R] {$I^P$=3,96\\$I$=0,\redBox{32}\\$j$=3};
\node(t33R) [colltiling, right=of t33L, xshift=4mm] {$I^P$=3,127\\$I$=0,\redBox{63}\\$j$=3};
\draw [dotted] (t33L) -- (t33R);

\node(legend) [smallnode, text width=15cm, draw=none, above=of t01R] {\texttt{for j in cuda\_threads(0, 4, unit=cuda\_cta): with CudaWarps(2, 3):}};
\end{tikzpicture}
\caption{Example collective tiling: With the parent box being $B^P=(4, 128)$, we tile along the CTA dimension ($d = 0$), and specialize so only warp 2 of 4 is active. We have $T = (1, 128)$, $B = (1, 32)$ (one warp), and $O = (0, 64)$.
$I = (I^P_0~\%~1, I^P_1 - 64)$, $j = I^P_0$, and we deactivate threads that don't satisfy $0 \le I_1 < 32$ (shown in \redBox{pink/red}).
The \blueBox{thread pitch} is $T_0 D_1 = \mathit{blockDim}$.
}
\label{fig:cta_tiling}
\end{figure*}

\begin{figure*}[!b]
\sffamily
\begin{tikzpicture}[node distance=2mm]
\node(t00L) [yellowcolltiling] {$I^P$=\blueBox{0,0}\\$I$=0,0\\$j$=0};
\node(t00R) [yellowcolltiling, right=of t00L, xshift=4mm] {$I^P$=0,31\\$I$=0,31\\$j$=0};
\draw [dotted] (t00L) -- (t00R);
\node(t01L) [greencolltiling, right=of t00R] {$I^P$=\blueBox{0,32}\\$I$=0,0\\$j$=1};
\node(t01R) [greencolltiling, right=of t01L, xshift=4mm] {$I^P$=0,63\\$I$=0,31\\$j$=1};
\draw [dotted] (t01L) -- (t01R);
\node(t02L) [colltiling, right=of t01R] {$I^P$=0,64\\$I$=0,0\\\redBox{$j$=2}};
\node(t02R) [colltiling, right=of t02L, xshift=4mm] {$I^P$=0,95\\$I$=0,31\\\redBox{$j$=2}};
\draw [dotted] (t02L) -- (t02R);
\node(t03L) [colltiling, right=of t02R] {$I^P$=0,96\\$I$=0,0\\\redBox{$j$=3}};
\node(t03R) [colltiling, right=of t03L, xshift=4mm] {$I^P$=0,127\\$I$=0,31\\\redBox{$j$=3}};
\draw [dotted] (t03L) -- (t03R);

\node(t10L) [yellowcolltiling, below=of t00L] {$I^P$=1,0\\$I$=1,0\\$j$=0};
\node(t10R) [yellowcolltiling, right=of t10L, xshift=4mm] {$I^P$=1,31\\$I$=1,31\\$j$=0};
\draw [dotted] (t10L) -- (t10R);
\node(t11L) [greencolltiling, right=of t10R] {$I^P$=1,32\\$I$=1,0\\$j$=1};
\node(t11R) [greencolltiling, right=of t11L, xshift=4mm] {$I^P$=1,63\\$I$=1,31\\$j$=1};
\draw [dotted] (t11L) -- (t11R);
\node(t12L) [colltiling, right=of t11R] {$I^P$=1,64\\$I$=1,0\\\redBox{$j$=2}};
\node(t12R) [colltiling, right=of t12L, xshift=4mm] {$I^P$=1,95\\$I$=1,31\\\redBox{$j$=2}};
\draw [dotted] (t12L) -- (t12R);
\node(t13L) [colltiling, right=of t12R] {$I^P$=1,96\\$I$=1,0\\\redBox{$j$=3}};
\node(t13R) [colltiling, right=of t13L, xshift=4mm] {$I^P$=1,127\\$I$=1,31\\\redBox{$j$=3}};
\draw [dotted] (t13L) -- (t13R);

\node(t20L) [yellowcolltiling, below=of t10L] {$I^P$=2,0\\$I$=2,0\\$j$=0};
\node(t20R) [yellowcolltiling, right=of t20L, xshift=4mm] {$I^P$=2,31\\$I$=2,31\\$j$=0};
\draw [dotted] (t20L) -- (t20R);
\node(t21L) [greencolltiling, right=of t20R] {$I^P$=2,32\\$I$=2,0\\$j$=1};
\node(t21R) [greencolltiling, right=of t21L, xshift=4mm] {$I^P$=2,63\\$I$=2,31\\$j$=1};
\draw [dotted] (t21L) -- (t21R);
\node(t22L) [colltiling, right=of t21R] {$I^P$=2,64\\$I$=2,0\\\redBox{$j$=2}};
\node(t22R) [colltiling, right=of t22L, xshift=4mm] {$I^P$=2,95\\$I$=2,31\\\redBox{$j$=2}};
\draw [dotted] (t22L) -- (t22R);
\node(t23L) [colltiling, right=of t22R] {$I^P$=2,96\\$I$=2,0\\\redBox{$j$=3}};
\node(t23R) [colltiling, right=of t23L, xshift=4mm] {$I^P$=2,127\\$I$=2,31\\\redBox{$j$=3}};
\draw [dotted] (t23L) -- (t23R);

\node(t30L) [yellowcolltiling, below=of t20L] {$I^P$=3,0\\$I$=3,0\\$j$=0};
\node(t30R) [yellowcolltiling, right=of t30L, xshift=4mm] {$I^P$=3,31\\$I$=3,31\\$j$=0};
\draw [dotted] (t30L) -- (t30R);
\node(t31L) [greencolltiling, right=of t30R] {$I^P$=3,32\\$I$=3,0\\$j$=1};
\node(t31R) [greencolltiling, right=of t31L, xshift=4mm] {$I^P$=3,63\\$I$=3,31\\$j$=1};
\draw [dotted] (t31L) -- (t31R);
\node(t32L) [colltiling, right=of t31R] {$I^P$=3,64\\$I$=3,0\\\redBox{$j$=2}};
\node(t32R) [colltiling, right=of t32L, xshift=4mm] {$I^P$=3,95\\$I$=3,31\\\redBox{$j$=2}};
\draw [dotted] (t32L) -- (t32R);
\node(t33L) [colltiling, right=of t32R] {$I^P$=3,96\\$I$=3,0\\\redBox{$j$=3}};
\node(t33R) [colltiling, right=of t33L, xshift=4mm] {$I^P$=3,127\\$I$=3,31\\\redBox{$j$=3}};
\draw [dotted] (t33L) -- (t33R);

\node(legend) [smallnode, text width=15cm, draw=none, above=of t01R] {\texttt{for j in cuda\_threads(0, 2, unit=cuda\_warp):}};
\end{tikzpicture}
\caption{Example collective tiling: With the parent box being $B^P=(4, 128)$, we tile along the threads dimension ($d = 1$) in units of 32 threads (1 warp). Only 2 of 4 warps are used due to the loop bounds \lighttt{(0, 2)}.
We have $T = (4, 32)$, $B = (4, 32)$ (one warp per CTA), and $O = (0, 0)$.
$I = (I^P_0, I^P_1~\%~32)$, $j = \lfloor I^P_1 / 32 \rfloor$, and we deactivate threads that don't satisfy $j < c = 2$ (shown in \redBox{pink/red}).
The resulting collective unit is a group of 4 warps cooperating along the CTA-in-cluster dimension; \textit{note this is different from the raw \lighttt{cuda\_warp} unit specified}.
The \blueBox{thread pitch} is $T_1 = 32$.
}
\label{fig:warp_tiling}
\end{figure*}

\filbreak
\minorSub{Collective Tiling Domain Completion}

The domain and box are completed the same as a collective unit, except we don't have to handle the partial prepend step, as the domain is always complete.
The tile is completed in the same way as the box.
The offset and intra-box expr are completed differently.

\filbreak
\minorKey{Offset Coordinate Removal:} All removed offset coordinate values must be 0.

\filbreak
\minorKey{Offset Coordinate Splitting:} Recall $f$ is the splitting factor. An integer value $c$ splits into $(\frac{c}{f}, 0)$, and fails if $c$ is not a multiple of $f$.

\filbreak
\minorKey{Intra-box Expr Coordinate Removal}: All removed coordinate values must be 0.

\filbreak
\minorKey{Intra-box Expr Coordinate Splitting}: Each coordinate $e$ of the expression splits into $(\frac{e}{f}, e~\%f)$.

\FloatBarrier

\newpage
\myTitle{Distributed Memory}

In CUDA C++, often, the declared size of a matrix tile doesn't match the logical size of the matrix, because we distribute shards of the matrix tile to individual threads, and register allocation in CUDA C++ occurs at per-thread granularity.
For example, each thread may allocate a $8 \times 4$ tile, and the threads in the CTA may be arranged in a $16 \times 16$ grid, causing the full CTA matrix to be $128 \times 64$.

\filbreak
In Exo syntax, this example looks like

\input{coll_samples/simple.0.tex}

\filbreak
The number and behavior of the distributed dimensions gets deduced by Exo-GPU based on subsequent usage.\footnote{Future discussion: is this implicitness a good idea?}
We expect distributed indices to be plain variables, with no arithmetic done.
As part of code lowering, we strip the distributed dimensions so the generated CUDA C++ declares the size of only the distributed shard.

\input{coll_samples/simple_cxx.0.tex}

\filbreak
Similar patterns may occur with larger granularity, for example, allocating per-warpgroup shards for wgmma, or per-CTA shards for (cluster) distributed SMEM.
For Exo, this all falls under the umbrella of ``distributed memory''.

\filbreak
Each tensor allocation has two important \myKeyA{collective units}
\begin{itemize}
  \item The \myKeyA{alloc unit}: the collective unit for the alloc statement
  \filbreak
  \item The \myKeyA{native unit}: collective unit physically needed to allocate the memory type (defined by \lighttt{MemWin} sub-class, e.g. 1 thread for scalar registers, 1 CTA for SMEM, 1 warp for mma matrix tiles).
\end{itemize}
\filbreak
If these are not matched, we need to deduce 1 or more of the left-most dimensions as distributed dimensions.
The remaining dimensions describe the shape of the distributed shard allocated in each native thread collective.

\filbreak
The rules for our deduction should enforce that
\begin{enumerate}
  \item each distributed shard is accessed by the same thread collective each time\footnote{Exception: warp shuffle, multicast (not covered in this doc)}
  \filbreak
  \item each thread collective accesses the same distributed shard each time
  \filbreak
  \item each distributed shard is actually allocated by the correct (native) collective unit
\end{enumerate}

\filbreak
\mainSub{State Model}

Each \lighttt{cuda\_threads} iterator variable (previous example : \texttt{\greenBox{m}, \violetBox{n}}) has three relevant pieces of state:
\begin{itemize}
  \item Its \myKeyA{\texttt{CollIndexExpr}}, mapping \lighttt{threadIdx.x} or \lighttt{blockIdx.x} to the iteration variable value.
  \filbreak
  \item Its \myKeyA{tiling operator} $t_0 \mapsto t_1$, where $t_1$ is the number of threads in the \myKeyA{tile} of the collective tiling defined by the \lighttt{cuda\_threads} loop, and $t_0$ is the number of threads in the tile of the parent collective unit.
  \filbreak
  \item Its range \lighttt{[0, hi)} as defined by the for loop bounds.
\end{itemize}

\filbreak
We deduce from each usage a tuple of \lighttt{CollIndexExpr}; the length of this tuple is the number of distributed dimensions, and the values are the indexing variables' \lighttt{CollIndexExpr}.

\input{coll_samples/simple.1.tex}

\filbreak
\mainSub{Requirements}
\begin{itemize}
  \item Each usage of distributed memory must successfully deduce a tiling chain.
  \filbreak
  \item Each usage must deduce the an algebraically-identical\footnote{As opposed to syntactically identical} tuple of \lighttt{CollIndexExpr}.
  \filbreak
  \item The range of each index variable used must match the extents of the indexed dimension.
\end{itemize}

\filbreak
\mainSub{Deduction Algorithm}

The goal of the deduction is to find a valid \myKeyA{tiling chain}; this is a permutation of \lighttt{cuda\_threads} iteration variables such that
\begin{itemize}
  \item let $t_a$ be the number of threads in the tile of the collective tiling in scope for the tensor allocation;
  \item let $t_n$ be the number of threads in the tile (equivalently, box) of the native unit;
  \item the tiling operators of the variables forms a chain $t_a \mapsto ... \mapsto t_n$.
\end{itemize}

\filbreak
In the earlier example, $t_a = 256$, $t_n = 1$, and the tiling chain is \greenBox{$m: 256 \mapsto 16$}, \violetBox{$n: 16 \mapsto 1$}.
The collective tiling defined by the \lighttt{cuda\_threads} loop of the last variable in the tiling chain is the \myKeyA{leaf tiling}.
Recall the tile size includes inactive threads, which isn't relevant in this simple example, but will matter later.

\filbreak
If the native unit mismatches the collective unit, we do distributed memory analysis for each index expression.
Consume, from left-to-right, indices until some permutation of \textit{all} % ambiguous tiling chain
consumed variables forms a valid tiling chain.
Each consumed index must be a plain read of a \lighttt{cuda\_threads} iteration variable.
The remaining unconsumed indices correspond to non-distributed dimensions.

\filbreak
We save the result of the deduction as a tuple of \lighttt{CollIndexExpr} (the order of this tuple is the order of the consumed index variables, not the order of the tiling chain).

\filbreak
Failure conditions:
\begin{itemize}
  \item We don't find a valid tiling chain before we consume an invalid index expression or run out of indices.
  \filbreak
  \item The leaf collective tiling's collective unit doesn't match the native unit.\footnote{Tile thread count alone isn't enough to guard against this; we could have a mismatch due to incorrect shape, or due to misalignment.}
  \filbreak
  \item We consume two indices that share the same $t_0$ value, ignoring no-op tiling operators $t_0 \mapsto t_1$ where $t_0 = t_1$. (This enforces the \textit{all} emphasized above).
  \filbreak
  \item The deduced tuple of \lighttt{CollIndexExpr} mismatches that of another usage.
\end{itemize}

\filbreak
\mainSub{Negative Example: Invalid Index Expression}

\input{coll_samples/seq_fail.0.tex}

\filbreak
\mainSub{Negative Example: Mismatched \texttt{CollIndexExpr}}

\input{coll_samples/mismatched.0.tex}

\filbreak
\mainSub{Positive Example: Matched \texttt{CollIndexExpr}}

\input{coll_samples/matched.0.tex}

\filbreak
\mainSub{Positive Example: Warp Matrix Tile}

The \lighttt{Sm80\_RmemMatrixD} memory type has a native unit of \lighttt{cuda\_warp}; $t_n = 32$.

\input{coll_samples/warp_example.0.tex}

Note how the deduction in this case is based on a window expression.
The \lighttt{Sm80\_mma\_zero\_d\_tf32} function takes only one distributed shard, so all intervals (\texttt{:}) correspond to non-distributed dimensions.
This will not be the case for multicast and warp shuffle functions, which take multiple distributed shards.
To handle this, we will have to treat intervals as holes that are completed by introspecting the called \lighttt{instr}.
The full deduction is based on synthesizing the behavior of both the caller and callee.
This use case is a major motivation for why distributed memory as a concept exists at all (allowing explicit neighbor indexing), and why it's based on these more complicated tiling chain deductions, rather than simple syntactic checks.

\filbreak
\mainSub{Negative Example: Broken Chain}

\input{coll_samples/broken_chain.0.tex}

\filbreak
\mainSub{Positive Example: Broken Chain Fixes}

Minor changes to the broken chain example will compile successfully.
For example, we can replace the \texttt{for \yellowBox{b} in cuda\_threads(...)} loop with \texttt{with CudaWarps}.
This highlights the importance that the deduction relies on chaining tile sizes, not box sizes.

\input{coll_samples/chain.0.tex}

Another possible correction is to include \texttt{\yellowBox{b}} in the indexing expression.
This increases the number of distributed dimensions to 3, and consequently reduces the distributed shard size (from 2 to 1).

\input{coll_samples/chain.1.tex}

\filbreak
\mainSub{Negative Example: Repeated Index}

\input{coll_samples/repeated.0.tex}

\filbreak
\mainSub{Positive Example: Modified Repeated Index}

If we reorder \greenBox{m} and \violetBox{n} in the above example, we get a correct Exo-GPU program.

\input{coll_samples/repeated.1.tex}

Since the second usage of \greenBox{m} is \textit{not} deduced as distributed index, it appears as a literal index in the generated CUDA C++ code.
This is unusual, but valid, Exo GPU code:

\input{coll_samples/repeated_cxx.0.tex}

\filbreak
\minorSub{Multiple Tensors}

\input{coll_samples/many_tensors.0.tex}

\end{document}
