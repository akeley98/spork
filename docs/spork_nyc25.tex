% python3 code_to_tex.py nyc25.py nyc25_tex && xelatex </dev/null spork_nyc25.tex
\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{redstyle} = [draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowstyle} = [draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greenstyle} = [draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluestyle} = [draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetstyle} = [draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{nRedstyle} = [draw=nRedBoxFg, fill=nRedBoxBg, text=nRedBoxFg]
\tikzstyle{nGoldstyle} = [draw=nGoldBoxFg, fill=nGoldBoxBg, text=nGoldBoxFg]
\tikzstyle{nGreenstyle} = [draw=nGreenBoxFg, fill=nGreenBoxBg, text=nGreenBoxFg]
\tikzstyle{nBluestyle} = [draw=nBlueBoxFg, fill=nBlueBoxBg, text=nBlueBoxFg]
\tikzstyle{nPurplestyle} = [draw=nPurpleBoxFg, fill=nPurpleBoxBg, text=nPurpleBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\tikzstyle{producer} = [yellownode, text width=64mm, minimum width=64mm, minimum height=14mm]
\tikzstyle{consumer} = [greennode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{smallproducer} = [yellownode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{copylatency} = [violetnode, text width=84mm, minimum width=84mm, minimum height=8mm]
\tikzstyle{ring} = [violetnode, text width=16mm, minimum width=1mm, minimum height=14mm]
\newcommand{\consumerBox}[1]{{\color{greenBoxFg}\colorbox{greenBoxBg}{#1}}}
\newcommand{\producerBox}[1]{{\color{yellowBoxFg}\colorbox{yellowBoxBg}{#1}}}

\myBiggerTitle{Exo-GPU}

\textbf{\hfill \Large Safe, Imperative, User-schedulable Programming for Tensor Cores}

{\LARGE

\vfill

David Zhao Akeley

Yuka Ikarashi

Jonathan Ragan-Kelley

\hfill \myBiggerTitle{2025 MIT/Jane Street Symposium}}

%\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
\texttt{@proc} (procedure) decorator captures Python AST\\transpiled to C or CUDA C++.

\myKeyA{Imperative control flow:} most constructs map 1:1 to C.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.3.tex}
}

\vspace{3mm}
\hrule

{\LARGE
\texttt{seq}-loop $\mapsto$ \texttt{for (\textit{int var} = lo; \textit{var} < hi; ++\textit{var})}

\myKeyA{Sequential} loops here; contrast to \myKeyB{parallel} loops later.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE
\myKeyA{Static typing:}\\
annotate proc parameters, allocated variables.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
\myKeyA{Memory annotation:} \texttt{@}-sign associates memory type\\
for parameters \& declarations.
}

\newpage
\myBiggerTitle{Exo Rewrites}

{\LARGE
\myKeyA{User scheduling:} Python-embedded \texttt{proc} objects can be metaprogrammed using \myKeyA{behavior-preserving} rewrites.

Start with simple \texttt{proc}; rewrite into optimized \texttt{proc}.
\begin{itemize}
  \item e.g. optimize memory access patterns.
  \item target \myKeyA{CPU-based} accelerators supported by Exo today (e.g. GEMMINI, Intel AMX, ARM SME)
\end{itemize}

Goal: extend rewrite model \& underlying semantics to model \myKeyB{imperative GPU programming} in a disciplined way.

\yellowBox{CITE}

}

\newpage
\myBiggerTitle{GEMM: M/N Tiling}

{\large
\input{nyc25_tex/cpu.3.tex}
}

\vspace{3mm}
\hrule

{\LARGE
We want to \myKeyA{tile} the \greenBox{m}/\violetBox{n} loop nest.

}


\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Apply \texttt{divide\_loop} twice to tile the \greenBox{m} loop by 3.

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Exo rewrites all uses of \greenBox{\texttt{m}} in the loop body.

\texttt{m} $\mapsto$ \texttt{m2 * M1 + m1 * M0 + m0}

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Assume perfect tiling to simplify this talk (no tail case).

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (N)}

{\large
\input{nyc25_tex/n_divide_loop.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Apply the same 3-level tiling to the \violetBox{n} loop.

}

\newpage
\myBiggerTitle{GEMM: Reorder Loops}

{\large
\input{nyc25_tex/reorder_loops.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Reorder loops; Exo checked all rewrites for correctness.

}

\newpage
{\large
\textbf{\LARGE BEFORE}\\
\input{nyc25_tex/cpu.4.tex}
}

\vspace{1mm}
\hrule

{\large
\textbf{\LARGE AFTER}\\
\input{nyc25_tex/reorder_loops.1.tex}
}

\newpage
\myBiggerTitle{GPU: Blocks \& Threads}

{\LARGE

Goal: Map tiles to different levels of the GPU hierarchy

\begin{itemize}
\item Launch \myKeyA{grid} of CUDA threads from \myKeyA{CPU}.
\item Map work to CUDA threads with \myKeyB{parallel for} loops.
\end{itemize}

}
\vfill
\begin{tikzpicture}[node distance=2mm]

\node (grid) [normalnode, nRedstyle, text width=5cm, minimum width=5cm] {\textbf{grid}};
\node (cpu) [left=of grid, xshift=-2cm] {\textbf{CPU}};
\draw [arrow] (cpu) -- node[above] {\textbf{launch}} (grid);
\node (cuda) [above=of grid, yshift=+4mm] {\textbf{CUDA Constructs}};
\node (exo) [right=of cuda, xshift=25mm] {\textbf{Exo-GPU Constructs}};

\node (cta0) [normalnode, nGoldstyle, text width=20mm, minimum width=20mm, below=of grid, xshift=-15mm, yshift=-1cm] {\textbf{block}};
\node (cta1) [normalnode, nGoldstyle, text width=20mm, minimum width=20mm, below=of grid, xshift=+15mm, yshift=-1cm] {\textbf{block}};

\node (thread0) [normalnode, nGreenstyle, text width=16mm, minimum width=16mm, below=of cta0, yshift=-1cm, xshift=-32mm] {\textbf{thread}};
\node (thread1) [normalnode, nGreenstyle, text width=16mm, minimum width=16mm, right=of thread0] {\textbf{thread}};
\node (thread2) [normalnode, nGreenstyle, text width=16mm, minimum width=16mm, right=of thread1, xshift=22mm] {\textbf{thread}};

\draw [arrow] (grid.south) to[out=270, in=90] (cta0.north);
\draw [arrow] (grid.south) to[out=270, in=90] (cta1.north);

\draw [arrow] (cta0.south) to[out=250, in=90] (thread0.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread1.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread2.north);
\draw [dotted] (thread1.east) -- (thread2.west);

\node (blockDim) [yshift=-1cm] at ($(thread0.south)!0.5!(thread2.south)$) {\blueBox{blockDim}};
\draw [thick, bluestyle, fill=none] ($(thread0.south west) + (0.01, 0)$) to[out=270, in=90] (blockDim.north);
\draw [thick, bluestyle, fill=none] ($(thread2.south east) - (0.01, 0)$) to[out=270, in=90] (blockDim.north);
% Slight offset by 0.01 needed otherwise there is a mystery page break

\node (CudaDeviceFunction) [align=left, right=of grid, text width=100mm] {\large\texttt{\codecomment{\# CPU launches grid}\\with \nRedBox{\textbf{CudaDeviceFunction}}(\blueBox{blockDim=\textit{arg}}):\\~~\codecomment{\# Body lowered to CUDA C++}}\\};
\node (cudaTasks) [align=left, right=of cta1, text width=100mm] {\large\texttt{~~for \textit{iter} in \nGoldBox{\textbf{cuda\_tasks}}(\textit{lo}, \textit{hi}):}\\};
\node (cudaThreads) [align=left, right=of thread2, text width=110mm] {\large\texttt{~~~~for \textit{iter} in \nGreenBox{\textbf{cuda\_threads}}(\textit{lo}, \textit{hi}, unit=\textit{u}):}\\};
\end{tikzpicture}

\newpage
\myBiggerTitle{Sequential-first Model \yellowBox{emphasize key} \redBox{DENSE}}

{\LARGE
Thus far, % you have been adrift in the sheltered harbour of my patience
all rewrites were \myKeyA{checked for correctness}.
\begin{itemize}
  \item transitivity: all correct $\implies$ original/final \texttt{proc} equivalent.
\end{itemize}

Exo-GPU adds ``parallelize'' rewrites:
\begin{itemize}
  \item \myKeyA{Assumed correct} during scheduling.
  \item Only check the final \texttt{proc} for correct synchronization.
\end{itemize}

Rewrites treat \myKeyB{parallel for} loops as if they were \myKeyA{sequential}.
\begin{itemize}
  \item Corpus of Exo rewrites kept as-is for Exo-GPU.
  \item Only worry about parallelism at the end.
\end{itemize}
}

\newpage
\myBiggerTitle{GEMM: GPU and Memory \yellowBox{\small Remove GPU loops here}}

{\large
\input{nyc25_tex/simple_gpu.0.tex}
}

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\node (grid) [normalnode, nRedstyle, text width=5cm, minimum width=5cm] {\textbf{grid}};
\node (cpu) [left=of grid, xshift=-2cm] {\textbf{CPU}};
\draw [arrow] (cpu) -- node[above] {\textbf{launch}} (grid);
\node (text) [text width=9.5cm, align=left, anchor=east] at($(cpu.west)$) {\LARGE Move loop nest to CUDA \mbox{device}; use CUDA memory.};
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{GEMM: GPU task (block) loops}

{\large
\input{nyc25_tex/simple_gpu.1.tex}
}

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\node (grid) [normalnode, nRedstyle, text width=5cm, minimum width=5cm] {\textbf{grid}};
\node (cta0) [normalnode, nGoldstyle, text width=20mm, minimum width=20mm, below=of grid, xshift=-15mm, yshift=-4mm] {\textbf{block}};
\node (cta1) [normalnode, nGoldstyle, text width=20mm, minimum width=20mm, below=of grid, xshift=+15mm, yshift=-4mm] {\textbf{block}};
\draw [arrow] (grid.south) to[out=270, in=90] (cta0.north);
\draw [arrow] (grid.south) to[out=270, in=90] (cta1.north);

\node (text) [text width=9.5cm, align=left, anchor=east] at($(grid.west)!0.5!(cta0.west)$) {\LARGE Assign large \texttt{M1} $\times$ \texttt{N1} tiles to CUDA blocks.};
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.2.tex}
}

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\input{nyc25_mini_block_thread.tex}
\node (text) [text width=9.5cm, align=left, anchor=east, yshift=10mm] at($(thread0.west)$) {\LARGE Assign small \texttt{M0} $\times$ \texttt{N0} tiles to CUDA threads.};
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.3.tex}
}

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\input{nyc25_mini_block_thread.tex}
\node (text) [text width=9.5cm, align=left, anchor=east, yshift=10mm] at($(thread0.west)$) {\LARGE ``unit'' to be explained shortly.};
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{GEMM: GPU per-thread work \hfill \nBlueBox{GEMM v2}}

{\large
\input{nyc25_tex/simple_gpu.4.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Inner most loops stay as sequential.

Each thread loops over its own \texttt{M0} $\times$ \texttt{N0} iteration space.

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
\texttt{\textbf{cuda\_threads}} support non-1:1 threads/iterations mapping.
\begin{itemize}
  \item Assign multiple threads per parallel for loop ``iteration''.
\end{itemize}

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Configure with the \myKeyA{collective unit} (\texttt{unit} argument).
\begin{itemize}
  \item Static property of each scope in Exo-GPU.
\end{itemize}

One \myKeyA{thread collective} executes each iteration of the loop.
\begin{itemize}
  \item Shape defined by \myKeyA{collective unit}.
\end{itemize}

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

\begin{tikzpicture}[node distance=2mm]
\node (cta) [bluenode, minimum width=30mm, minimum height=40mm] {block: 256 threads};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m1 = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m1 = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m1 = 0}; threads [0, 15]};
\node (m15) [Mnode, below=of m2, yshift=-9mm] {\texttt{m1 = 15}; threads [240, 255]};
\draw [arrow] (cta) -- node[above] {\texttt{for \yellowBox{m1}}} (m2);
\draw [dotted] (m2) -- (m15);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m15n0) [Nnode, right=of m15, xshift=16mm] {240};
\node (m15n1) [Nnode, right=of m15n0] {241};
\node (m15n2) [Nnode, right=of m15n1] {242};
\node (m15n15) [Nnode, right=of m15n2, xshift=8mm] {255};
\draw [arrow] (m0) -- node[above] {\texttt{for \redBox{n1}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \redBox{n1}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \redBox{n1}}} (m2n0);
\draw [arrow] (m15) -- node[below] {\texttt{for \redBox{n1}}} (m15n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n1=0}} (m15n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n1=1}} (m15n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n1=2}} (m15n2);
\draw [dotted] (m2n15) --node[] {\texttt{n1=15}} (m15n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m15n2) -- (m15n15);

\end{tikzpicture}

\hrule

{\LARGE
Unlike other parallel loops, \texttt{\textbf{cuda\_threads}} cannot spawn more threads.
It just \myKeyA{subdivides} existing thread collectives.
}

\newpage
\myBiggerTitle{Summary: Thread Block Tiling \hfill \nBlueBox{GEMM v2}}

\begin{tikzpicture}[node distance=0mm]
\input{nyc25_grid_tile.tex}
\node (code) [text width=90mm, left=of C, align=left] {\large \input{nyc25_tex/loops.0.tex}\\};
\end{tikzpicture}

%\newline

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_grid_tile.tex}
\node (A0) [greenstyle, left=of cta0, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A1) [greenstyle, left=of cta4, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A2) [greenstyle, left=of cta8, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A3) [greenstyle, left=of cta12, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (B0) [violetstyle, above=of cta0, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B1) [violetstyle, above=of cta1, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B2) [violetstyle, above=of cta2, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B3) [violetstyle, above=of cta3, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (text) [left=of B0] {\myBiggerTitle{Block Matrix Product~~}};
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{Summary: Thread Tiling in Block \hfill \nBlueBox{GEMM v2}}

\begin{tikzpicture}[node distance=0mm]
\input{nyc25_block_tile.tex}
\node (code) [text width=90mm, left=of C, align=left] {\large \input{nyc25_tex/loops.1.tex}\\};
\end{tikzpicture}

{\LARGE \textit{Pedagogical: not the most efficient pattern.}}

% \newpage
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_block_tile.tex}
\node (A) [greenstyle, left=of t240, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (B) [violetstyle, above=of t15, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\draw [arrow] (A) to[in=90, out=90] (t240);
\draw [arrow] (A) to[in=90, out=90] (t241);
\draw [arrow] (A) to[in=90, out=90] (t242);
\draw [arrow] (A) to[in=90, out=90] (t255);
\draw [arrow] (B) to[in=180, out=180] (t15);
\draw [arrow] (B) to[in=180, out=180] (t31);
\draw [arrow] (B) to[in=180, out=180] (t63);
\draw [arrow] (B) to[in=180, out=180] (t255);
\node (text) [left=of B, text width=150mm, align=left] {\myBiggerTitle{Waste: Duplicate Reads in Block}};
\end{tikzpicture}

\newpage
\myBiggerTitle{Shared Memory: SMEM}

{\LARGE

\myKeyB{SMEM:} Per-thread-block ``scratchpad''

Save needed block of \greenBox{A}, \violetBox{B} in SMEM.
\begin{itemize}
  \item Break blocks into tiles to fit within SMEM.
\end{itemize}

Tile \blueBox{\texttt{k}} loop by \blueBox{\texttt{K0}} (\texttt{k $\mapsto$ k1 * K0 + k0}).

}

\vfill

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\input{nyc25_k0_tile.tex}
\end{tikzpicture}
\end{center}


\input{nyc25_k_tile_slides.tex}


\newpage
\myBiggerTitle{GEMM: Restructuring}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_no_sync_flow.tex}
\node (text) [below=of B] {\textbf{All steps are thread-block-cooperative.}};
\node (move_zero) [rednode, above=of zero, yshift=+5mm] {Factor out of loop};
\node (move_C) [rednode, above=of C, yshift=+5mm] {Factor out of loop};
\node (k1_info) [yellownode, above=of k1, yshift=+2mm] {Main loop};
\draw [arrow] (move_C) to[out=300, in=60] (C);
\draw [arrow] (move_zero) to[out=300, in=60] (zero);
\end{tikzpicture}
\end{center}

\vfill
\hrule
\vfill

{\LARGE
We need to move the ``zero accumulators'' and ``write out C'' steps out of the main loop nest.

}


\newpage
\myBiggerTitle{GEMM: Restructuring}

{\large
\input{nyc25_tex/simple_gpu.5.tex}
}

\vfill
\hrule
\vfill

{\LARGE
We need to move the ``zero accumulators'' and ``write out C'' steps out of the main loop nest.

}


\newpage
\myBiggerTitle{Loop Fission}

\begin{minipage}[t]{0.48\textwidth}\fixminipage
{\LARGE
\textbf{BEFORE}
}

{\large
\input{nyc25_tex/fission_def.0.tex}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}\fixminipage
{\LARGE
\textbf{AFTER}
}

{\large
\input{nyc25_tex/fission_def.1.tex}
}
\end{minipage}

\vspace{3mm}
\hrule

{\LARGE
Rewritten code follows.

}


% We have no room for captions so I just have to remember what to say here!

\newpage
{\large
\input{nyc25_tex/fission.0.tex}
}

\newpage
{\large
\input{nyc25_tex/fission.1.tex}
}

\newpage
{\large
\input{nyc25_tex/fission.2.tex}
}

\newpage
{\large
\input{nyc25_tex/fission.3.tex}
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/k1_before_smem.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
We will replace reads from \greenBox{A}, \violetBox{B} (in \myKeyA{GMEM}) with \myKeyB{SMEM}.

}
\vspace{-4mm}

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_no_sync_flow.tex}
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Stage $\texttt{M1} \times \texttt{K0}$ tile of \greenBox{\texttt{A}}, $\texttt{K0} \times \texttt{N1}$ tile of \violetBox{\texttt{B}} in shared memory.

\texttt{A\_smem}, \texttt{B\_smem} replace \texttt{A} and \texttt{B} in the loop body.

}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
The Exo rewrite generates loops to load the shared memory tile;
\myKeyA{sequential loops} by default -- we will fix this!

}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_in_order.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Divide \& parallelize the load-shared-memory loops.
}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary \hfill \nBlueBox{GEMM v3}}

{\large
\input{nyc25_tex/smem_in_order.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Add synchronization:\\
All threads in the thread block wait for each other.
}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary}

{\LARGE
Synchronization required: thread that loads a value to SMEM isn't the same as the one that uses it.

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\input{nyc25_sync_caption.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary}

{\LARGE
First sync: wait for SMEM tile to fill\\(RAW: read-after-write)

Second sync: don't overwrite until all SMEM reads are done\\(WAR: write-after-read)

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\input{nyc25_sync_caption.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{Modernize GPU GEMM}

{\LARGE
Next step: use accelerator instructions\\for the \myKeyA{GMEM}$\to$\myKeyB{SMEM} copies.

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\input{nyc25_sync_caption.tex}

\node (A_caption) [redstyle, above=of A, yshift=11mm, xshift=6mm] {Accelerate me!};
\node (B_caption) [redstyle, above=of B, yshift=11mm, xshift=6mm] {Accelerate me!};
\draw [redstyle, arrow, fill=none] (A) -- (A_caption);
\draw [redstyle, arrow, fill=none] (B) -- (B_caption);
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{Async Copies}

{\LARGE
Ampere (sm\_80) introduced \texttt{cp.async} instructions.

Copies directly from \myKeyA{GMEM} to \myKeyB{SMEM}, bypassing registers.

}
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_cp_async.tex}
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{The Catch}

{\LARGE
\texttt{cp.async} doesn't synchronize like most instructions.

e.g. \texttt{\_\_syncthreads();} (cross-thread-block sync) doesn't wait for \texttt{cp.async} to finish.

}
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_cp_async.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{Timelines}

{\LARGE
Categorize instrs by \myKeyA{interactions with synchronization}.

Exo-GPU \myKeyA{timelines}:
\begin{itemize}
  \item \textbf{\texttt{cpu\_in\_order}}: CPU instructions
  \item \textbf{\texttt{cuda\_in\_order}}: most CUDA instructions
  \item \textbf{\texttt{Sm80\_cp\_async}}: \texttt{cp.async} (special syncs needed)
  \item Others for H100 (not in this talk)
\end{itemize}

}

\newpage
\myBiggerTitle{Exo-GPU Synchronization}

{\LARGE
Parameterize Exo-GPU synchronization with timelines:

\begin{center}
  \texttt{Fence(\myKeyA{L1}, \myKeyB{L2})}
\end{center}

User specifies intended effect:

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\node (L1) [draw=black, align=center] {prior \myKeyA{L1}-timeline\\memory accesses};
\node (L2) [draw=black, align=center, right=of L1, xshift=24mm] {future \myKeyB{L2}-timeline\\memory accesses};
\draw [arrow] (L1) -- node[above] {\large \textbf{SYNC}} (L2);
\end{tikzpicture}
\end{center}

Compiler chooses exact instructions.

%% Otherwise imperative\\(executes in program order with surrounding instructions).

}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Current main loop:
only \textbf{\texttt{cuda\_in\_order}} instructions used,
so \textbf{\texttt{Fence}} is parameterized with \texttt{cuda\_in\_order}.
}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.3.tex}
}

\vspace{3mm}
\hrule

{\LARGE
TODO: replace these GMEM$\to$SMEM copies with cp.async.
}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.4.tex}
}

\vspace{3mm}
\hrule

{\LARGE
TODO: update timeline parameters for synchronization.
}

\newpage
\myBiggerTitle{GEMM: cp.async (Substitute Instruction)}

{\large
\input{nyc25_tex/cp_async.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Exo allows rewriting a block of code into a call to a\\\myKeyA{semantically-equivalent sub-procedure}.
\begin{itemize}
  \item Semantics \myKeyB{ignore concurrency}.
\end{itemize}

Replaced: \texttt{A\_smem[\codecomment{...}] = A[\codecomment{...}]} and \texttt{B\_smem[\codecomment{...}] = B[\codecomment{...}]}

}

\newpage
\myBiggerTitle{GEMM: cp.async (CudaAsync block)}

{\large
\input{nyc25_tex/cp_async.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Such instructions must be wrapped in a CudaAsync block.

}

\newpage
\myBiggerTitle{GEMM: cp.async (Synchronization) \hfill \nBlueBox{GEMM v4}}

{\large
\input{nyc25_tex/cp_async.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Update the timeline parameters for synchronization.

}

\newpage
\myBiggerTitle{More to do: Overlap Compute/Memory}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
% \input{nyc25_sync_caption.tex}

\node (cpAsync) [redstyle, yshift=-8mm] at ($(A)!0.5!(B)$) {\textbf{Now with \texttt{cp.async}!}};
\node (memory) [redstyle, yshift=-25mm] at ($(A.south west)!0.5!(B.south east)$) {Memory-heavy};
\node (compute) [redstyle, yshift=-25mm] at ($(accum.south)$) {Compute-heavy};
\draw [draw=redBoxFg, thick] (memory.north) to[out=90, in=270] (A.south west);
\draw [draw=redBoxFg, thick] (memory.north) to[out=90, in=270] (B.south east);
\draw [draw=redBoxFg, thick] (compute.north) to[out=90, in=270] (accum.south west);
\draw [draw=redBoxFg, thick] (compute.north) to[out=90, in=270] (accum.south east);

\end{tikzpicture}
\end{center}

%% {\LARGE
%% Syncs strictly separate memory-heavy and compute-heavy code portions.

%% }


\newpage
\myBiggerTitle{More to do: Overlap Compute/Memory}

{\LARGE
Each thread continues on after executing \texttt{cp.async} without waiting for the copy to \myKeyB{SMEM} to finish.

% We can put other work (compute) between issuing and waiting for the \texttt{cp.async} instruction.

}
\vspace{5mm}
\hrule
\vspace{5mm}
{
\LARGE
\input{nyc25_tex/cp_async_pseudocode.0.tex}
}


\newpage
\myBiggerTitle{More to do: Overlap Compute/Memory}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\input{nyc25_sync_caption.tex}
\node (cpAsync) [redstyle, yshift=-8mm] at ($(A)!0.5!(B)$) {\textbf{Now with \texttt{cp.async}!}};
\end{tikzpicture}
\end{center}
\hrule
{\LARGE
We need to \myKeyA{delay} the accumulate step to a future \blueBox{\texttt{k1}} iteration to make room for overlapping work!

}


\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]

\node (C0) [draw=black, minimum width=28mm, minimum height=24mm] {compute};
\node (C1) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C0) {compute};
\node (C2) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C1) {compute};
\node (C3) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C2) {compute};
%\node(bad) [left=of C0, xshift=-40mm] {\textbf{BAD}};

\node (A0) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C0) {Load A};
\node (B0) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C0) {Load B};

\node (A1) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C1) {Load A};
\node (B1) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C1) {Load B};

\node (A2) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C2) {Load A};
\node (B2) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C2) {Load B};

\node (A3) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C3) {Load A};
\node (B3) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C3) {Load B};

\draw [arrow] (A0.south east) -- (C0.west);
\draw [arrow] (C0.east) -- (A1.south west);
\draw [arrow] (A1.south east) -- (C1.west);
\draw [arrow] (C1.east) -- (A2.south west);
\draw [arrow] (A2.south east) -- (C2.west);
\draw [arrow] (C2.east) -- (A3.south west);
\draw [arrow] (A3.south east) -- (C3.west);

\node (k0) [yshift=-2mm, anchor=north] at($(B0.south east)!0.5!(C0.south west)$) {\blueBox{\texttt{k1 = 0}}};
\node (k1) [yshift=-2mm, anchor=north] at($(B1.south east)!0.5!(C1.south west)$) {\blueBox{\texttt{k1 = 1}}};
\node (k2) [yshift=-2mm, anchor=north] at($(B2.south east)!0.5!(C2.south west)$) {\blueBox{\texttt{k1 = 2}}};
\node (k3) [yshift=-2mm, anchor=north] at($(B3.south east)!0.5!(C3.south west)$) {\blueBox{\texttt{k1 = 3}}};

\end{tikzpicture}
}

\hrule

{\LARGE
Current state: load into SMEM and consume (compute) the same tile in one \blueBox{\texttt{k1}} iteration.
}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Loop skew: delay \myKeyA{consumption} (compute) of SMEM tile by one \blueBox{\texttt{k1}} iteration from SMEM load.

}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Need to transform SMEM into a \myKeyA{ring buffer}.

One slot may be being filled while another is consumed.

}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}
\input{nyc25_dag_war.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
WAR hazard: require syncs before re-using ring buffer slots.

}


\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}
\input{nyc25_dag_war.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Implement dependency graph edges with \myKeyA{split barriers}:\\Arrive/Await.
}

\newpage
\myBiggerTitle{Exo-GPU: Fence vs Arrive/Await}

{\LARGE
The \texttt{Fence} statement combines two concepts together:

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\node (L1) [draw=black, text width=40mm, minimum height=40mm, align=center] {\myKeyA{L1}-timeline memory access};
\node (fence) [yellowstyle, text width=10mm, minimum height=40mm, align=center, right=of L1, xshift=40mm] {};
\node (fence_caption) [below=of fence] {\textbf{Fence}};
\node (L2) [draw=black, text width=40mm, minimum height=40mm, align=center, right=of fence, xshift=40mm] {\myKeyB{L2}-timeline memory access};
\draw [arrow] (L1) -- (fence);
\draw [arrow] (fence) -- (L2);

\node (pre) [text width=80mm, align=left, anchor=south west] at($(L1.north west)$) {\textbf{Pre-Fence condition:} All threads' \myKeyA{L1}-timeline memory accesses have finished.};
\node (post) [text width=80mm, align=left, anchor=south east] at($(L2.north east)$) {\textbf{Post-Fence effect:}\\All threads' \myKeyB{L2}-timeline memory accesses may proceed.};
\end{tikzpicture}
\end{center}

}


\newpage
\myBiggerTitle{Exo-GPU: Fence vs Arrive/Await}

{\LARGE
Separate out the two with \texttt{Arrive} and \texttt{Await} statements.

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\node (L1) [draw=black, text width=40mm, minimum height=40mm, align=center] {\myKeyA{L1}-timeline memory access};
\node (arrive) [yellowstyle, text width=10mm, minimum height=40mm, align=center, right=of L1, xshift=9mm] {};
\node (arrive_caption) [below=of arrive] {\textbf{Arrive}};
\node (unrelated) [bluestyle, align=center, right=of arrive, xshift=4mm, yshift=-9mm, text width=34mm] { (unrelated\\work) };
\node (await) [yellowstyle, text width=10mm, minimum height=40mm, align=center, right=of unrelated, xshift=4mm, yshift=+9mm] {};
\node (await_caption) [below=of await] {\textbf{Await}};
\node (L2) [draw=black, text width=40mm, minimum height=40mm, align=center, right=of await, xshift=9mm] {\myKeyB{L2}-timeline memory access};
\draw [arrow] (L1) -- (arrive);
\draw [arrow] (await) -- (L2);
\draw [arrow] (arrive) to[out=45, in=135] (await);

\node (pre) [text width=80mm, align=left, anchor=south west] at($(L1.north west)$) {\textbf{Pre-Arrive condition:} All threads' \myKeyA{L1}-timeline memory accesses have finished.};
\node (post) [text width=80mm, align=left, anchor=south east] at($(L2.north east)$) {\textbf{Post-Await effect:}\\All threads' \myKeyB{L2}-timeline memory accesses may proceed.};
\end{tikzpicture}
\end{center}

}


\newpage
\myBiggerTitle{Exo-GPU: Fence vs Arrive/Await}

{\LARGE
\texttt{Arrive}/\texttt{Await} require a common \textbf{\texttt{barrier}}-typed variable.

\vspace{3mm}
\hrule

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\node (L1) [draw=black, text width=40mm, minimum height=40mm, align=center] {\myKeyA{L1}-timeline memory access};
\node (arrive) [yellowstyle, text width=10mm, minimum height=40mm, align=center, right=of L1, xshift=9mm] {};
\node (arrive_caption) [below=of arrive] {\textbf{Arrive}};
\node (unrelated) [bluestyle, align=center, right=of arrive, xshift=4mm, yshift=-9mm, text width=34mm] { (unrelated\\work) };
\node (await) [yellowstyle, text width=10mm, minimum height=40mm, align=center, right=of unrelated, xshift=4mm, yshift=+9mm] {};
\node (await_caption) [below=of await] {\textbf{Await}};
\node (L2) [draw=black, text width=40mm, minimum height=40mm, align=center, right=of await, xshift=9mm] {\myKeyB{L2}-timeline memory access};
\draw [arrow] (L1) -- (arrive);
\draw [arrow] (await) -- (L2);
\draw [arrow] (arrive) to[out=45, in=135] (await);

\node (pre) [align=left, anchor=south west, yshift=12mm] at($(L1.north west)$) {\texttt{Arrive(\myKeyA{L1}) >> \textit{barrier}}};
\node (post) [align=left, anchor=south east, yshift=12mm] at($(L2.north east)$) {\texttt{Await(\textit{barrier}, \myKeyB{L2})}};
\draw [arrow] (pre) -- (post);
\end{tikzpicture}
\end{center}

\vfill
\hfill \textbf{Example code in extra slides.}
\vfill

}


\newpage
\myBiggerTitle{Tensor Cores}

{\LARGE
Tensor core instructions = multiply-accumulate matrix tile.

}

{\large
\input{nyc25_tex/tensor_core_instr.0.tex}
}

{\LARGE
Exo can replace such blocks with calls to \myKeyA{semantically equivalent} tensor core instructions.

Async on Hopper (sm\_90a) \& later, so on a different \myKeyA{timeline}.
\begin{itemize}
  \item Still \texttt{cuda\_in\_order} for $\le$ sm\_80.
\end{itemize}
}

\newpage
\myBiggerTitle{Abstract Machine: Visibility Sets}

{\LARGE
Starting point for tackling synchronization checking.

\vspace{5mm}

\myKeyA{Visibility Set}: set of \myKeyA{Timeline Signatures}; pair of
\begin{itemize}
  \item ``Qualitative'' timeline
  \item ``Quantitative'' thread ID (unique in system)
\end{itemize}

\vspace{5mm}

These are associated for each memory read/mutate operation ever performed.

}


\newpage
\myBiggerTitle{Abstract Machine: Environment}

{\LARGE

Augment sequential semantics with a \myKeyA{synchronization environment}, parallel to the value environment.
\begin{itemize}
  \item History of reads/mutates done to each variable.
  \item Stores \myKeyA{visibility sets}.
\end{itemize}

\vspace{5mm}

Abstract machine is \myKeyA{fundamentally sequential}, but,
\begin{itemize}
  \item Tracks visibility of prior reads and mutates\\(``simulating'' parallel machine behavior).
  \item Visibility sets may grow with synchronization.
\end{itemize}

}


\newpage
\myBiggerTitle{Abstract Machine: Usage}

{\LARGE
Exo-GPU programs target this abstract machine.

\textbf{Correctness View:} check predicates on visibility sets.
\begin{itemize}
  \item Check each read is \myKeyA{guaranteed} to get the value given by the immediately-prior write in sequential order.
  \item Sequential-parallel equivalence.
\end{itemize}

\textbf{Optimization View:} the abstract machine trace \myKeyA{dictates} what instructions \& what threads are used for each operation.
\begin{itemize}
  \item Constrains the compiler.
  \item Generated CUDA internals aren't implementation details.
\end{itemize}

}


\newpage
\myBiggerTitle{Conclusion: Scheduling Chain of Equivalence}

{\LARGE
Step 1: apply scheduling operators
\begin{itemize}
  \item Analyzes \myKeyB{parallel} loops as if they were \myKeyA{sequential}.
\end{itemize}

Step 2: sync check final \texttt{proc}
\begin{itemize}
  \item Parallelization doesn't change the generated output relative to sequential version.
\end{itemize}
}

\begin{center}
{\large
\begin{tikzpicture}[node distance=8mm]
\sffamily
\node(proc0) [normalnode, text width=2cm, minimum width=2cm] {Original proc};
\node(procNS) [normalnode, right=of proc0, text width=4cm, minimum width=4cm] {Scheduled proc,\\interpreted\\\myKeyA{sequentially}};
\node(procNM) [normalnode, right=of procNS, text width=4cm, minimum width=4cm] {Scheduled proc,\\\myKeyB{parallel}\\interpretation};
\node(cuda) [normalnode, right=of procNM] {CUDA C++ header};

\draw [arrow] (proc0) to node(exo)[]{} (procNS);
\draw [arrow] (procNS) to node(SM)[]{} (procNM);
\draw [arrow] (procNM) to node(toCuda)[]{} (cuda);

\node(exo) [normalnode, yshift=-8mm, text width=4cm, minimum width=4cm, below=of exo, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Exo Rewrites};
\node(sync) [normalnode, yshift=-8mm, text width=5cm, below=of SM, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Sync Checking};
\node(spork) [normalnode, yshift=-8mm, text width=6cm, xshift=1cm, below=of toCuda, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg] {Code Generator};
\node(note) [above=of SM, redstyle, yshift=8mm, align=center] {Same proc, \textbf{different}\\\textbf{interpretation}};
\node(c) [smallnode, above=of cuda, xshift=-6mm] {C code};

\draw [line, draw=blueBoxFg] (exo) -- (proc0);
\draw [line, draw=blueBoxFg] (exo) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNS);
\draw [line, draw=blueBoxFg] (sync) -- (procNM);
\draw [line, draw=redBoxFg] (note) -- (procNS);
\draw [line, draw=redBoxFg] (note) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (procNM);
\draw [line, draw=blueBoxFg] (spork) -- (cuda);
\draw [arrow] (procNM.east) to (c.west);
\end{tikzpicture}
}
\end{center}

\newpage
\myBiggerTitleExtra{Extra Slides: Todo, Memory/Compute Overlap}

{\large
\input{nyc25_tex/ring_todo.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Currently: data movement at the top, compute at the bottom.
}

\newpage
\myBiggerTitleExtra{Extra Slides: Todo, Ring Buffer}

{\large
\input{nyc25_tex/ring_todo.1.tex}
}

\vspace{3mm}
\hrule
\vspace{-2mm}

{\LARGE
Move the \redBox{\texttt{smem}} declarations out of the \blueBox{\texttt{k1}} loop.\\
Add ring buffer dimension.
}

\newpage
\myBiggerTitleExtra{Extra Slides: Todo, Loop Skew}

{\large
\input{nyc25_tex/ring_todo.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Delay compute (bottom) relative to data movement (top).
}

\newpage
\myBiggerTitleExtra{Extra Slides: Todo, Arrive/Await}

{\large
\input{nyc25_tex/ring_todo.3.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Replace with \textbf{\texttt{Arrive}}, \textbf{\texttt{Await}}.
}

\newpage
\myBiggerTitleExtra{Extra Slides: Ring Buffer}

{\large
\input{nyc25_tex/ring.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Lift \texttt{A\_smem}, \texttt{B\_smem} out of the \texttt{k1} loop and ring buffer by 3.

}

\newpage
\myBiggerTitleExtra{Extra Slides: Loop Skew}

{\large
\input{nyc25_tex/ring.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE

Add an extra \blueBox{\texttt{k1}} iteration; delay the accum code by 1 iteration relative to the cp.async code.

}

\newpage
\myBiggerTitleExtra{Extra Slides: Split Barriers}

{\large
\input{nyc25_tex/split.0.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Declare \textbf{\texttt{barrier}}-typed variable.

}

\newpage
\myBiggerTitleExtra{Extra Slides: Split Barriers}

{\large
\input{nyc25_tex/split.1.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Resolve RAW Hazard (\textbf{\texttt{Sm80\_cp\_async $\to$ cuda\_in\_order}}).

}

\newpage
\myBiggerTitleExtra{Extra Slides: Split Barriers}

{\large
\input{nyc25_tex/split.2.tex}
}

\vspace{3mm}
\hrule

{\LARGE
Resolve WAR Hazard (\textbf{\texttt{cuda\_in\_order $\to$ Sm80\_cp\_async}}).

Required to re-use ring buffer slot.

}

\end{document}



%% \newpage
%% \myBiggerTitleExtra{Challenge: SIMT Parallelism}

%% {\LARGE

%% Talk about launching blocks of threads

%% Talk about mapping work to threads

%% Diagram: vector add eye candy? y[threadIdx] += x[threadIdx]

%% }

%% \newpage
%% \myBiggerTitleExtra{Challenge: Memory / Compute Overlap}

%% {\LARGE
%% SIMT = overlapping identical workloads.\\
%% We should also overlap heterogenous work: data \& compute.

%% }
%% \hrule
%% {
%% \Large
%% \begin{tikzpicture}[node distance=0mm]

%% \node (C0) [draw=black, minimum width=28mm, minimum height=24mm] {compute};
%% \node (C1) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C0) {compute};
%% \node (C2) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C1) {compute};
%% \node(bad) [left=of C0, xshift=-40mm] {\textbf{BAD}};

%% \node (A0) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C0) {Load A};
%% \node (B0) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C0) {Load B};

%% \node (A1) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C1) {Load A};
%% \node (B1) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C1) {Load B};

%% \node (A2) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C2) {Load A};
%% \node (B2) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C2) {Load B};

%% \draw [arrow] (A0.south east) -- (C0.west);
%% \draw [arrow] (C0.east) -- (A1.south west);
%% \draw [arrow] (A1.south east) -- (C1.west);
%% \draw [arrow] (C1.east) -- (A2.south west);
%% \draw [arrow] (A2.south east) -- (C2.west);

%% \end{tikzpicture}
%% }
%% \vspace{-1mm}
%% \hrule
%% {
%% \Large
%% \begin{tikzpicture}[node distance=0mm]
%% \input{nyc25_dag.tex}
%% \input{nyc25_dag_war.tex}
%% \node(good) [anchor=center] at(C0) {\textbf{GOOD}};
%% \end{tikzpicture}
%% }

%% \newpage
%% \myBiggerTitleExtra{Challenge: Efficient Synchronization}

%% {\LARGE

%% Try not to stall threads

%% Don't screw up

%% Nondeterministic \& subtle bugs possible

%% }
