% python3 code_to_tex.py nyc25.py nyc25_tex && xelatex </dev/null spork_nyc25.tex
\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\tikzstyle{producer} = [yellownode, text width=64mm, minimum width=64mm, minimum height=14mm]
\tikzstyle{consumer} = [greennode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{smallproducer} = [yellownode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{copylatency} = [violetnode, text width=84mm, minimum width=84mm, minimum height=8mm]
\tikzstyle{ring} = [violetnode, text width=16mm, minimum width=1mm, minimum height=14mm]
\newcommand{\consumerBox}[1]{{\color{greenBoxFg}\colorbox{greenBoxBg}{#1}}}
\newcommand{\producerBox}[1]{{\color{yellowBoxFg}\colorbox{yellowBoxBg}{#1}}}

\myBiggerTitle{Exo-GPU}

\textbf{\hfill \large Safe, Imperative, User-schedulable Programming for Tensor Cores}

{\LARGE

\vfill

David Zhao Akeley

Yuka Ikarashi

Jonathan Ragan-Kelley

\hfill \myBiggerTitle{2025 MIT/Jane Street Symposium}}

%\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage
\myBiggerTitle{Challenge: SIMT Parallelism}

{\LARGE

Talk about launching blocks of threads

Talk about mapping work to threads

Diagram: vector add eye candy? y[threadIdx] += x[threadIdx]

}

\newpage
\myBiggerTitle{Challenge: Memory / Compute Overlap}

{\LARGE

Talk about overlapping memory and compute workloads

Contrast with SIMT
\begin{itemize}
  \item SIMT = threads doing the same thing
  \item Now, threads are doing something different
\end{itemize}

Diagram:
\begin{itemize}
  \item BAD: chain of memory and compute tasks
  \item GOOD: scary looking DAG of memory/compute tasks
\end{itemize}

Non-trivial dependency graph

}

\newpage
\myBiggerTitle{Challenge: Efficient Synchronization}

{\LARGE

Try not to stall threads

Don't screw up

Nondeterministic \& subtle bugs possible

}


\newpage
\myBiggerTitle{Goals}

{\LARGE

Insert some quality BS here

% Imperative programming: for performance engineering, what-you-see-is-what-you-get.

}


\newpage
\myBiggerTitle{Exo: User-schedulable Language}

{\LARGE
Python-embedded imperative language

Rewrite rules: Python functions transform procedures
\begin{itemize}
  \item checked for functional equivalence
\end{itemize}

Instruction substitution: replace code blocks with instructions of equivalent functionality.

Exo exists today; extending it to model GPUs in a disciplined way.

}

\newpage
\myBiggerTitle{GEMM: Starting Code}

{\large
\input{nyc25_tex/cpu.0.tex}
}

{\LARGE
\texttt{@proc} (procedure) decorator captures Python AST\\transpiled to C or CUDA C++.

}

\newpage
\myBiggerTitle{GEMM: Starting Code}

{\large
\input{nyc25_tex/cpu.1.tex}
}

{\LARGE
Static typing:\\
annotate proc parameters, allocated variables.

}

\newpage
\myBiggerTitle{GEMM: Starting Code}

{\large
\input{nyc25_tex/cpu.2.tex}
}

{\LARGE
Memory annotation:\\
\texttt{@}-sign associates memory type for parameters \& declarations
}

\newpage
\myBiggerTitle{GEMM: Starting Code}

{\large
\input{nyc25_tex/cpu.3.tex}
}

{\LARGE
Most constructs map 1:1 to C.

\texttt{seq}-loop $\mapsto$ \texttt{for (int \textit{var} = lo; \textit{var} < hi; ++\textit{var})}

``sequential'' loop here; contrast to par loops later.

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.0.tex}
}

{\LARGE
Apply \texttt{divide\_loop} twice to tile the \greenBox{m} loop by 3.

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.1.tex}
}

{\LARGE
Exo rewrites all uses of \greenBox{\texttt{m}} in the loop body.

\texttt{m} $\mapsto$ \texttt{m2 * M1 + m1 * M0 + m0}

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.2.tex}
}

{\LARGE
Assume perfect tiling to simplify this talk (no tail case)

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (N)}

{\large
\input{nyc25_tex/n_divide_loop.0.tex}
}

{\LARGE
Apply the same 3-level tiling to the \violetBox{n} loop.

}

\newpage
\myBiggerTitle{GEMM: Reorder Loops \yellowBox{\small SO read off how the code works now ``so we loop over''}}

{\large
\input{nyc25_tex/reorder_loops.0.tex}
}

\newpage
\myBiggerTitle{GPU: Blocks \& Threads}

{\LARGE

Goal: Map tiles to different levels of the GPU hierarchy

\begin{itemize}
\item Demarcate CPU / CUDA code division
\item Change sequential loops to parallel
\item Use GPU memory
\end{itemize}

}

\begin{tikzpicture}[node distance=2mm]

\node (grid) [greennode, text width=6cm, minimum width=6cm] {grid};
\node (cpu) [left=of grid, xshift=-2cm] {\textbf{CPU}};
\draw [arrow] (cpu) -- node[above] {\textbf{launch}} (grid);
\node (cuda) [above=of grid, yshift=+4mm] {\textbf{CUDA Constructs}};
\node (exo) [right=of cuda, xshift=25mm] {\textbf{Exo-GPU Constructs}};

\node (cta0) [greennode, text width=20mm, minimum width=20mm, below=of grid, xshift=-2cm, yshift=-1cm] {block};
\node (cta1) [greennode, text width=20mm, minimum width=20mm, below=of grid, xshift=+2cm, yshift=-1cm] {block};

\node (thread0) [greennode, text width=16mm, minimum width=16mm, below=of cta0, yshift=-1cm, xshift=-32mm] {thread};
\node (thread1) [greennode, text width=16mm, minimum width=16mm, right=of thread0] {thread};
\node (thread2) [greennode, text width=16mm, minimum width=16mm, right=of thread1, xshift=32mm] {thread};

\draw [arrow] (grid.south) to[out=270, in=90] (cta0.north);
\draw [arrow] (grid.south) to[out=270, in=90] (cta1.north);

\draw [arrow] (cta0.south) to[out=270, in=90] (thread0.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread1.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread2.north);
\draw [dotted] (thread1.east) -- node[above] {\blueBox{blockDim}} (thread2.west);

\node (CudaDeviceFunction) [right=of grid, text width=90mm] {\texttt{\codecomment{\# CPU launches grid}\\with CudaDeviceFunction(\blueBox{blockDim=\textit{arg}}):\\~~\codecomment{\# Body lowered to CUDA C++}}};
\node (cudaTasks) [right=of cta1, text width=90mm] {\texttt{~~\codecomment{\# Parallel-for loop}\\~~\codecomment{\# generates per-block tasks}\\~~for \textit{iter} in cuda\_tasks(\textit{lo}, \textit{hi}):}};
\node (cudaThreads) [right=of thread2, text width=90mm] {\texttt{~~~~\codecomment{\# Somewhat more complicated}\\~~~~\codecomment{\# Assigns threads to iterations}\\~~~~for \textit{iter} in cuda\_threads(\textit{lo}, \textit{hi}, unit=\textit{u}):}};
\end{tikzpicture}

\newpage
\myBiggerTitle{GEMM: GPU and Memory \yellowBox{\small emph GPU/CPU code division, memory, remove GPU features here}}

{\large
\input{nyc25_tex/simple_gpu.0.tex}
}

{\LARGE
Move loops to CUDA device; switch to CUDA memory types

}

\newpage
\myBiggerTitle{GEMM: GPU task (block) loops}

{\large
\input{nyc25_tex/simple_gpu.1.tex}
}

{\LARGE
Assign large \texttt{M1} $\times$ \texttt{N1} tiles to CUDA blocks.

}

% \yellowBox{1 or 2 details one at a time, we need to frame mapping levels to loops...}

% \redBox{Add zoom-in code, no changes}

\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.2.tex}
}

{\LARGE
Assign small \texttt{M0} $\times$ \texttt{N0} tiles to CUDA threads.

\textit{NOTE: parallelism is not checked yet.}

\textit{Rewrites treat parallel loops as sequential (for now).}

}

\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.3.tex}
}

{\LARGE
``unit'' to be explained shortly.

}

\newpage
\myBiggerTitle{GEMM: GPU per-thread work}

{\large
\input{nyc25_tex/simple_gpu.4.tex}
}

{\LARGE
Inner most loops stay as sequential.

Each thread loops over \texttt{M0} $\times$ \texttt{N0} iteration space.

}

\newpage
\myBiggerTitle{GPU thread loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

{\LARGE
\texttt{unit} argument: \myKeyA{collective unit}.

Describes ``shape'' of grouping of threads (\myKeyA{thread collective}) assigned to execute each iteration of the loop.

Static property of each scope in Exo-GPU.

}

\newpage
\myBiggerTitle{GPU thread loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

{\LARGE
Unlike other parallel loops, \texttt{cuda\_threads} cannot spawn more threads.
It just subdivides existing thread collectives.
}

\begin{tikzpicture}[node distance=2mm]
\node (cta) [bluenode, minimum width=30mm, minimum height=40mm] {block: 256 threads};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m1 = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m1 = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m1 = 0}; threads [0, 15]};
\node (m15) [Mnode, below=of m2, yshift=-9mm] {\texttt{m1 = 15}; threads [240, 255]};
\draw [arrow] (cta) -- node[above] {\texttt{for \yellowBox{m1}}} (m2);
\draw [dotted] (m2) -- (m15);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m15n0) [Nnode, right=of m15, xshift=16mm] {240};
\node (m15n1) [Nnode, right=of m15n0] {241};
\node (m15n2) [Nnode, right=of m15n1] {242};
\node (m15n15) [Nnode, right=of m15n2, xshift=8mm] {255};
\draw [arrow] (m0) -- node[above] {\texttt{for \redBox{n1}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \redBox{n1}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \redBox{n1}}} (m2n0);
\draw [arrow] (m15) -- node[below] {\texttt{for \redBox{n1}}} (m15n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n1=0}} (m15n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n1=1}} (m15n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n1=2}} (m15n2);
\draw [dotted] (m2n15) --node[] {\texttt{n1=15}} (m15n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m15n2) -- (m15n15);

\end{tikzpicture}

\newpage
\myBiggerTitle{GEMM: Lift Alloc \& Expand Dim}

\yellowBox{Diagram of what we want to do}

{\large
\input{nyc25_tex/expand_dim.0.tex}
}

{\LARGE
Lift the allocation of \blueBox{\texttt{accum}} into block-scope\\
(just below \violetBox{\texttt{cuda\_tasks}} loops)
}

\newpage
\myBiggerTitle{GEMM: Lift Alloc \& Expand Dim}

{\large
\input{nyc25_tex/expand_dim.1.tex}
}

{\LARGE
Expand dim: each iteration uses its own index of \texttt{accum}
}

\newpage
\myBiggerTitle{GEMM: Loop Fission}

{\large
\input{nyc25_tex/expand_dim.2.tex}
}

{\LARGE
We will split the loop at the indicated locations, retaining the loop structure
}

\newpage

{\large
\input{nyc25_tex/fission.0.tex}
}
\newpage
\myBiggerTitle{Tile Re-Use}

TODO: there's no words on it!

\begin{tikzpicture}[node distance=0mm]
\node (cL) [normalnode, minimum width=50mm, minimum height=50mm] {};
\node (aL) [smallnode, minimum width=20mm, minimum height=50mm, left=of cL, xshift=-2mm] {$A$};
\node (aRow) [greennode, minimum width=18mm, minimum height=6mm, above=of aL, yshift=-18mm, text width=0mm] {};
\node (bL) [smallnode, minimum width=50mm, minimum height=20mm, above=of cL, yshift=2mm] {$B$};
\draw (aRow.east) to[out=0, in=270] ($(bL.south) - (2.0, 0)$);
\draw (aRow.east) to[out=0, in=270] ($(bL.south) - (1.2, 0)$);
\draw (aRow.east) to[out=0, in=270] ($(bL.south) - (0.4, 0)$);
\draw (aRow.east) to[out=0, in=270] ($(bL.south) + (0.4, 0)$);
\draw (aRow.east) to[out=0, in=270] ($(bL.south) + (1.2, 0)$);
\draw (aRow.east) to[out=0, in=270] ($(bL.south) + (2.0, 0)$);
\node (ML) [right=of aL, xshift=2mm] {\greenBox{\texttt{M1}}};
\node (NL) [below=of bL, yshift=-2mm] {\violetBox{\texttt{N1}}};
\node (aKL) [above=of aL] {\blueBox{\texttt{K0}}};
\node (bKL) [left=of bL] {\blueBox{\texttt{K0}}};
\node (Lcaption) [below=of cL, xshift=-11mm] {Each row of the $A$ tile is used \violetBox{\texttt{N1}} times.};

\node (cR) [normalnode, minimum width=50mm, minimum height=50mm, right=of cL, xshift=42mm] {};
\node (aR) [smallnode, minimum width=20mm, minimum height=50mm, left=of cR, xshift=-2mm] {$A$};
\node (bR) [smallnode, minimum width=50mm, minimum height=20mm, above=of cR, yshift=2mm] {$B$};
\node (bCol) [violetnode, minimum width=6mm, minimum height=18mm, left=of bR, xshift=18mm, text width=0mm] {};
\draw (bCol.south) to[out=270, in=0] ($(aR.east) - (0, 2.0)$);
\draw (bCol.south) to[out=270, in=0] ($(aR.east) - (0, 1.2)$);
\draw (bCol.south) to[out=270, in=0] ($(aR.east) - (0, 0.4)$);
\draw (bCol.south) to[out=270, in=0] ($(aR.east) + (0, 0.4)$);
\draw (bCol.south) to[out=270, in=0] ($(aR.east) + (0, 1.2)$);
\draw (bCol.south) to[out=270, in=0] ($(aR.east) + (0, 2.0)$);
\node (MR) [right=of aR, xshift=2mm] {\greenBox{\texttt{M1}}};
\node (NR) [below=of bR, yshift=-2mm] {\violetBox{\texttt{N1}}};
\node (aKR) [above=of aR] {\blueBox{\texttt{K0}}};
\node (bKR) [left=of bR] {\blueBox{\texttt{K0}}};
\node (Rcaption) [below=of cR, xshift=-11mm] {Each column of the $B$ tile is used \greenBox{\texttt{M1}} times.};
\end{tikzpicture}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.0.tex}
}

{\LARGE
Split k loop, and reorder sequential \blueBox{k1} loop outwards
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.1.tex}
}

{\LARGE
Stage $\texttt{M1} \times \texttt{K0}$ tile of \greenBox{\texttt{A}}, $\texttt{K0} \times \texttt{N1}$ tile of \violetBox{\texttt{B}} in shared memory.

\texttt{A\_smem}, \texttt{B\_smem} replace \texttt{A} and \texttt{B} in the loop body.
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.2.tex}
}

{\LARGE
The compiler generates loops to load the shared memory tile
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_in_order.0.tex}
}

{\LARGE
Divide \& Parallelize the load-shared-memory loops
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_in_order.1.tex}
}

{\LARGE
Add synchronization (\yellowBox{\texttt{cuda\_in\_order}} to be explained)
}

\newpage
\myBiggerTitle{TODO}

{\LARGE
Summarize the previous code again

Talk about strict separation of memory \& compute work

Talk about async instr weirdness

Talk about timelines

Talk about cp.async 16 bytes

}


\newpage
\myBiggerTitle{Semi-Declarative Synchronization}

{\LARGE

Still executed inline with other imperative code
\begin{itemize}
  \item not a task graph representation, etc.
\end{itemize}

}


\newpage
\myBiggerTitle{GEMM: cp.async}

{\large
\input{nyc25_tex/cp_async_no_instr.0.tex}
}

{\LARGE

Modify the load-shared-memory loop nest

(add an extra level with 4 iterations)

}

\newpage
\myBiggerTitle{GEMM: cp.async (Substitute Instruction)}

{\large
\input{nyc25_tex/cp_async.0.tex}
}

{\LARGE

Replace the \texttt{kTmp}, \texttt{nTmp} loops with cp.async instructions.

Exo knows the instr loads 4 contiguous f32 values.

}

\newpage
\myBiggerTitle{GEMM: cp.async (CudaAsync block)}

{\large
\input{nyc25_tex/cp_async.1.tex}
}

{\LARGE

Such instructions must appear in a CudaAsync block.

}

\newpage
\myBiggerTitle{GEMM: cp.async (Synchronization)}

{\large
\input{nyc25_tex/cp_async.2.tex}
}

{\LARGE

Change the timeline parameters in the synchronization statements

}

\newpage
\myBiggerTitle{GEMM: Ring Buffer}

{\large
\input{nyc25_tex/ring.0.tex}
}

{\LARGE

Lift \texttt{A\_smem}, \texttt{B\_smem} out of the \texttt{k1} loop and ring buffer by 3.

}

\newpage
\myBiggerTitle{GEMM: Loop Skew}

{\large
\input{nyc25_tex/ring.1.tex}
}

{\LARGE

Add an extra \blueBox{\texttt{k1}} iteration; delay the accum code by 1 iteration relative to the cp.async code.

}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.0.tex}
}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.1.tex}
}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.2.tex}
}

\newpage
\myBiggerTitle{Tensor Cores}

{\LARGE

Tensor Cores = More timelines + more complicated instruction substitution

}

\newpage
\myBiggerTitle{Abstract Machine}

{\LARGE

Talk about visibility sets
\begin{itemize}
  \item ``qualitative'' timeline + ``quantitative'' thread ID
  \item grows with synchronization
\end{itemize}

Talk about two-stage correctness
\begin{itemize}
  \item Stage 1: rewrite operations
  \item Stage 2: parallelism
\end{itemize}

Talk about abstract machine as tool for checking correctness

Talk about abstract machine constraining the compiler's outputted CUDA

}

\end{document}
