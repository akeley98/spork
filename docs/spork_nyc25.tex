% python3 code_to_tex.py nyc25.py nyc25_tex && xelatex </dev/null spork_nyc25.tex
\input{slides_common.tex}

\begin{document}

\tikzstyle{smallnode} = [rectangle, minimum width=1.25cm, minimum height=1cm, text centered, text width=1.25cm, draw=black, fill=white]
\tikzstyle{smallishnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=white]
\tikzstyle{normalnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{widenode} = [rectangle, minimum width=62mm, minimum height=8mm, text centered, text width=62mm, draw=black, fill=white]
\tikzstyle{bignode} = [rectangle, minimum width=3.5cm, minimum height=2cm, text centered, text width=3cm, draw=black, fill=white]
\tikzstyle{smemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorB, fill=white]
\tikzstyle{gmemnode} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=keyColorA, fill=white]
\tikzstyle{smallishsmemnode} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm, draw=keyColorB, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{rednode} = [normalnode, draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellownode} = [normalnode, draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greennode} = [normalnode, draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluenode} = [normalnode, draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetnode} = [normalnode, draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{redstyle} = [draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowstyle} = [draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greenstyle} = [draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluestyle} = [draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetstyle} = [draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]

\tikzstyle{Mnode} = [greennode, text width=55mm, minimum width=55mm, minimum height=7mm]
\tikzstyle{Nnode} = [violetnode, text width=7mm, minimum width=7mm, minimum height=7mm]

\tikzstyle{producer} = [yellownode, text width=64mm, minimum width=64mm, minimum height=14mm]
\tikzstyle{consumer} = [greennode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{smallproducer} = [yellownode, text width=20mm, minimum width=20mm, minimum height=14mm]
\tikzstyle{copylatency} = [violetnode, text width=84mm, minimum width=84mm, minimum height=8mm]
\tikzstyle{ring} = [violetnode, text width=16mm, minimum width=1mm, minimum height=14mm]
\newcommand{\consumerBox}[1]{{\color{greenBoxFg}\colorbox{greenBoxBg}{#1}}}
\newcommand{\producerBox}[1]{{\color{yellowBoxFg}\colorbox{yellowBoxBg}{#1}}}

\myBiggerTitle{Exo-GPU}

\textbf{\hfill \large Safe, Imperative, User-schedulable Programming for Tensor Cores}

{\LARGE

\vfill

David Zhao Akeley

Yuka Ikarashi

Jonathan Ragan-Kelley

\hfill \myBiggerTitle{2025 MIT/Jane Street Symposium}}

%\includegraphics[width=\linewidth]{usda_spork.jpg}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.0.tex}
}

{\LARGE
\texttt{@proc} (procedure) decorator captures Python AST\\transpiled to C or CUDA C++.

\myKeyA{Imperative control flow:} most constructs map 1:1 to C.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.3.tex}
}

{\LARGE
\texttt{seq}-loop $\mapsto$ \texttt{for (\textit{int var} = lo; \textit{var} < hi; ++\textit{var})}

\myKeyA{Sequential} loops here; contrast to \myKeyB{parallel} loops later.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.1.tex}
}

{\LARGE
\myKeyA{Static typing:}\\
annotate proc parameters, allocated variables.

}

\newpage
\myBiggerTitle{GEMM: Starting out with Exo}

{\large
\input{nyc25_tex/cpu.2.tex}
}

{\LARGE
\myKeyA{Memory annotation:} \texttt{@}-sign associates memory type\\
for parameters \& declarations.
}

\newpage
\myBiggerTitle{Exo Rewrites}

{\LARGE
\myKeyA{User scheduling:} Python-embedded \texttt{proc} objects can be metaprogrammed using \myKeyA{behavior-preserving} rewrites.

Start with simple \texttt{proc}; rewrite into optimized \texttt{proc}.
\begin{itemize}
  \item e.g. optimize memory access patterns.
  \item target \myKeyA{CPU-based} accelerators supported by Exo today (e.g. Intel AMX)
\end{itemize}


Goal: extend rewrite model \& underlying semantics to model \myKeyB{imperative GPU programming} in a disciplined way.

}

\newpage
\myBiggerTitle{GEMM: M/N Tiling}

{\large
\input{nyc25_tex/cpu.3.tex}
}

{\LARGE
We want to \myKeyA{tile} the \greenBox{m}/\violetBox{n} loop nest.

}


\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.0.tex}
}

{\LARGE
Apply \texttt{divide\_loop} twice to tile the \greenBox{m} loop by 3.

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.1.tex}
}

{\LARGE
Exo rewrites all uses of \greenBox{\texttt{m}} in the loop body.

\texttt{m} $\mapsto$ \texttt{m2 * M1 + m1 * M0 + m0}

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (M)}

{\large
\input{nyc25_tex/m_divide_loop.2.tex}
}

{\LARGE
Assume perfect tiling to simplify this talk (no tail case).

}

\newpage
\myBiggerTitle{GEMM: Divide Loop (N)}

{\large
\input{nyc25_tex/n_divide_loop.0.tex}
}

{\LARGE
Apply the same 3-level tiling to the \violetBox{n} loop.

}

\newpage
\myBiggerTitle{GEMM: Reorder Loops}

{\large
\input{nyc25_tex/reorder_loops.0.tex}
}

{\LARGE
Reorder loops; Exo checked all rewrites for correctness.

}

\newpage
\myBiggerTitle{GEMM: Loop Nest Before \& After}

{\large
\input{nyc25_tex/cpu.4.tex}
}

\vspace{1mm}
\hrule

{\large
\input{nyc25_tex/reorder_loops.1.tex}
}

\newpage
\myBiggerTitle{GPU: Blocks \& Threads}

{\LARGE

Goal: Map tiles to different levels of the GPU hierarchy

\begin{itemize}
\item Launch \myKeyA{grid} of CUDA threads from \myKeyA{CPU}.
\item Map work to CUDA threads with \myKeyB{parallel for} loops.
\end{itemize}

}
\vfill
\begin{tikzpicture}[node distance=2mm]

\node (grid) [bluenode, text width=5cm, minimum width=5cm] {grid};
\node (cpu) [left=of grid, xshift=-2cm] {\textbf{CPU}};
\draw [arrow] (cpu) -- node[above] {\textbf{launch}} (grid);
\node (cuda) [above=of grid, yshift=+4mm] {\textbf{CUDA Constructs}};
\node (exo) [right=of cuda, xshift=25mm] {\textbf{Exo-GPU Constructs}};

\node (cta0) [rednode, text width=20mm, minimum width=20mm, below=of grid, xshift=-15mm, yshift=-1cm] {block};
\node (cta1) [rednode, text width=20mm, minimum width=20mm, below=of grid, xshift=+15mm, yshift=-1cm] {block};

\node (thread0) [greennode, text width=16mm, minimum width=16mm, below=of cta0, yshift=-1cm, xshift=-32mm] {thread};
\node (thread1) [greennode, text width=16mm, minimum width=16mm, right=of thread0] {thread};
\node (thread2) [greennode, text width=16mm, minimum width=16mm, right=of thread1, xshift=22mm] {thread};

\draw [arrow] (grid.south) to[out=270, in=90] (cta0.north);
\draw [arrow] (grid.south) to[out=270, in=90] (cta1.north);

\draw [arrow] (cta0.south) to[out=250, in=90] (thread0.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread1.north);
\draw [arrow] (cta0.south) to[out=270, in=90] (thread2.north);
\draw [dotted] (thread1.east) -- (thread2.west);

\node (blockDim) [yshift=-1cm] at ($(thread0.south)!0.5!(thread2.south)$) {\blueBox{blockDim}};
\draw [thick, bluestyle, fill=none] (thread0.south) to[out=270, in=90] (blockDim.north);
\draw [thick, bluestyle, fill=none] (thread2.south) to[out=270, in=90] (blockDim.north);

\node (CudaDeviceFunction) [align=left, right=of grid, text width=100mm] {\large\texttt{\codecomment{\# CPU launches grid}\\with CudaDeviceFunction(\blueBox{blockDim=\textit{arg}}):\\~~\codecomment{\# Body lowered to CUDA C++}}\\};
\node (cudaTasks) [align=left, right=of cta1, text width=100mm] {\large\texttt{~~for \textit{\redBox{iter}} in cuda\_tasks(\textit{lo}, \textit{hi}):}\\};
\node (cudaThreads) [align=left, right=of thread2, text width=110mm] {\large\texttt{~~~~for \textit{\greenBox{iter}} in cuda\_threads(\textit{lo}, \textit{hi}, unit=\textit{u}):}\\};
\end{tikzpicture}

\newpage
\myBiggerTitle{Sequential-first Model}

{\LARGE
Thus far, % you have been adrift in the sheltered harbour of my patience
all rewrites were \myKeyA{checked for correctness}.
\begin{itemize}
  \item transitivity: all correct $\implies$ original/final \texttt{proc} equivalent.
\end{itemize}

Exo-GPU adds ``parallelize'' rewrites:
\begin{itemize}
  \item \myKeyA{Assumed correct} during scheduling.
  \item Only check the final \texttt{proc} for correct synchronization.
\end{itemize}

Rewrites treat \myKeyB{parallel for} loops as if they were \myKeyA{sequential}.
\begin{itemize}
  \item Corpus of Exo rewrites kept as-is for Exo-GPU.
  \item Only worry about parallelism at the end.
\end{itemize}
}

\newpage
\myBiggerTitle{GEMM: GPU and Memory \yellowBox{\small Remove GPU loops here}}

{\large
\input{nyc25_tex/simple_gpu.0.tex}
}

{\LARGE
Move loop nest to CUDA device.

Switch to CUDA memory types.

}

\newpage
\myBiggerTitle{GEMM: GPU task (block) loops}

{\large
\input{nyc25_tex/simple_gpu.1.tex}
}

{\LARGE
Assign large \texttt{M1} $\times$ \texttt{N1} tiles to CUDA blocks.

}

% \yellowBox{1 or 2 details one at a time, we need to frame mapping levels to loops...}

% \redBox{Add zoom-in code, no changes}

\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.2.tex}
}

{\LARGE
Assign small \texttt{M0} $\times$ \texttt{N0} tiles to CUDA threads.

}

\newpage
\myBiggerTitle{GEMM: GPU thread loops}

{\large
\input{nyc25_tex/simple_gpu.3.tex}
}

{\LARGE
``unit'' to be explained shortly.

}

\newpage
\myBiggerTitle{GEMM: GPU per-thread work}

{\large
\input{nyc25_tex/simple_gpu.4.tex}
}

{\LARGE
Inner most loops stay as sequential.

Each thread loops over \texttt{M0} $\times$ \texttt{N0} iteration space.

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

{\LARGE
\texttt{\textbf{cuda\_threads}} support non-1:1 threads/iterations mapping.
\begin{itemize}
  \item Assign multiple threads per parallel for loop ``iteration''.
\end{itemize}

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

{\LARGE

Configure with the \myKeyA{collective unit} (\texttt{unit} argument).
\begin{itemize}
  \item Static property of each scope in Exo-GPU.
\end{itemize}

One \myKeyA{thread collective} executes each iteration of the loop.
\begin{itemize}
  \item shape defined by \myKeyA{collective unit}.
\end{itemize}

}

\newpage
\myBiggerTitle{GPU Thread Loops}

{\large
\input{nyc25_tex/cuda_threads.0.tex}
}

{\LARGE
Unlike other parallel loops, \texttt{\textbf{cuda\_threads}} cannot spawn more threads.
It just \myKeyA{subdivides} existing thread collectives.
}

\begin{tikzpicture}[node distance=2mm]
\node (cta) [bluenode, minimum width=30mm, minimum height=40mm] {block: 256 threads};

\node (m2) [Mnode, right=of cta, xshift=16mm] {\texttt{m1 = 2}; threads [32, 47]};
\node (m1) [Mnode, above=of m2] {\texttt{m1 = 1}; threads [16, 31]};
\node (m0) [Mnode, above=of m1] {\texttt{m1 = 0}; threads [0, 15]};
\node (m15) [Mnode, below=of m2, yshift=-9mm] {\texttt{m1 = 15}; threads [240, 255]};
\draw [arrow] (cta) -- node[above] {\texttt{for \yellowBox{m1}}} (m2);
\draw [dotted] (m2) -- (m15);
\node (m0n0) [Nnode, right=of m0, xshift=16mm] {0};
\node (m0n1) [Nnode, right=of m0n0] {1};
\node (m0n2) [Nnode, right=of m0n1] {2};
\node (m0n15) [Nnode, right=of m0n2, xshift=8mm] {15};
\node (m1n0) [Nnode, below=of m0n0] {16};
\node (m1n1) [Nnode, below=of m0n1] {17};
\node (m1n2) [Nnode, below=of m0n2] {18};
\node (m1n15) [Nnode, below=of m0n15] {31};
\node (m2n0) [Nnode, below=of m1n0] {32};
\node (m2n1) [Nnode, below=of m1n1] {33};
\node (m2n2) [Nnode, below=of m1n2] {34};
\node (m2n15) [Nnode, below=of m1n15] {47};
\node (m15n0) [Nnode, right=of m15, xshift=16mm] {240};
\node (m15n1) [Nnode, right=of m15n0] {241};
\node (m15n2) [Nnode, right=of m15n1] {242};
\node (m15n15) [Nnode, right=of m15n2, xshift=8mm] {255};
\draw [arrow] (m0) -- node[above] {\texttt{for \redBox{n1}}} (m0n0);
\draw [arrow] (m1) -- node[above] {\texttt{for \redBox{n1}}} (m1n0);
\draw [arrow] (m2) -- node[above] {\texttt{for \redBox{n1}}} (m2n0);
\draw [arrow] (m15) -- node[below] {\texttt{for \redBox{n1}}} (m15n0);
\draw [dotted] (m2n0) -- node[] {\texttt{n1=0}} (m15n0);
\draw [dotted] (m2n1) -- node[] {\texttt{n1=1}} (m15n1);
\draw [dotted] (m2n2) -- node[] {\texttt{n1=2}} (m15n2);
\draw [dotted] (m2n15) --node[] {\texttt{n1=15}} (m15n15);
\draw [dotted] (m0n2) -- (m0n15);
\draw [dotted] (m1n2) -- (m1n15);
\draw [dotted] (m2n2) -- (m2n15);
\draw [dotted] (m15n2) -- (m15n15);

\end{tikzpicture}

\newpage
\myBiggerTitle{Summary: Thread Block Tiling}

\begin{tikzpicture}[node distance=0mm]
\input{nyc25_grid_tile.tex}
\node (code) [text width=90mm, left=of C, align=left] {\large \input{nyc25_tex/loops.0.tex}\\};
\end{tikzpicture}

%\newline

\begin{center}
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_grid_tile.tex}
\node (A0) [greenstyle, left=of cta0, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A1) [greenstyle, left=of cta4, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A2) [greenstyle, left=of cta8, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (A3) [greenstyle, left=of cta12, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (B0) [violetstyle, above=of cta0, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B1) [violetstyle, above=of cta1, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B2) [violetstyle, above=of cta2, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (B3) [violetstyle, above=of cta3, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\node (text) [left=of B0] {\myBiggerTitle{Block Matrix Product~~}};
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{Summary: Thread Tiling Within Block}

\begin{tikzpicture}[node distance=0mm]
\input{nyc25_block_tile.tex}
\node (code) [text width=90mm, left=of C, align=left] {\large \input{nyc25_tex/loops.1.tex}\\};
\end{tikzpicture}

{\LARGE \textit{Pedagogical: not the most efficient pattern.}}

% \newpage
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_block_tile.tex}
\node (A) [greenstyle, left=of t240, minimum width=60mm, minimum height=20mm, xshift=-6mm] {\Huge A[...]};
\node (B) [violetstyle, above=of t15, minimum width=20mm, minimum height=24mm, yshift=6mm] {\Huge B[...]};
\draw [arrow] (A) to[in=90, out=90] (t240);
\draw [arrow] (A) to[in=90, out=90] (t241);
\draw [arrow] (A) to[in=90, out=90] (t242);
\draw [arrow] (A) to[in=90, out=90] (t255);
\draw [arrow] (B) to[in=180, out=180] (t15);
\draw [arrow] (B) to[in=180, out=180] (t31);
\draw [arrow] (B) to[in=180, out=180] (t63);
\draw [arrow] (B) to[in=180, out=180] (t255);
\node (text) [left=of B, text width=150mm, align=left] {\myBiggerTitle{Waste: Duplicate Reads in Block}};
\end{tikzpicture}

\newpage
\myBiggerTitle{Shared Memory: SMEM}

{\LARGE

\myKeyA{SMEM:} Per-thread-block ``scratchpad''

Save needed block of \greenBox{A}, \violetBox{B} in SMEM.
\begin{itemize}
  \item Break blocks into tiles to fit within SMEM.
\end{itemize}

Tile \blueBox{\texttt{k}} loop by \blueBox{\texttt{K0}} (\texttt{k $\mapsto$ k1 * K0 + k0}).

}

\vfill

\begin{center}
\begin{tikzpicture}[node distance=2mm]
\input{nyc25_k0_tile.tex}
\end{tikzpicture}
\end{center}


\input{nyc25_k_tile_slides.tex}


\newpage
\myBiggerTitle{GEMM: Restructuring}

{\large
\input{nyc25_tex/simple_gpu.5.tex}
}

{\LARGE
TODO: divide \texttt{k} loop \& reorder outside \texttt{\textbf{cuda\_threads}} loops.

}


\newpage
\myBiggerTitle{GEMM: Restructuring}

{\large
\input{nyc25_tex/simple_gpu.6.tex}
}

{\LARGE
TODO: factor out of \blueBox{\texttt{k1}} loop: \texttt{accum = 0}, write to \texttt{C}.

}


\newpage
\myBiggerTitle{GEMM: Restructuring}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_no_sync_flow.tex}
\node (text) [below=of B] {\textbf{All steps are thread-block-cooperative.}};
\node (move_zero) [yellownode, above=of zero, yshift=+5mm] {Factor out of loop};
\node (move_C) [yellownode, above=of C, yshift=+5mm] {Factor out of loop};
\node (k1_info) [yellownode, above=of k1, yshift=+2mm] {Main loop};
\draw [arrow] (move_C) to[out=300, in=60] (C);
\draw [arrow] (move_zero) to[out=300, in=60] (zero);
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{GEMM: Lift Alloc \& Expand Dim}

{\large
\input{nyc25_tex/simple_gpu.7.tex}
}

{\LARGE
Scope of \texttt{accum} variable is too small.

Need to lift it out to thread-block scope.

}

\newpage
\myBiggerTitle{GEMM: Lift Alloc \& Expand Dim}

{\large
\input{nyc25_tex/expand_dim.0.tex}
}

{\LARGE
Lift the allocation of \redBox{\texttt{accum}} into thread-block-scope\\
(just below \violetBox{\texttt{cuda\_tasks}} loops)
}

\newpage
\myBiggerTitle{GEMM: Lift Alloc \& Expand Dim}

{\large
\input{nyc25_tex/expand_dim.1.tex}
}

{\LARGE
Expand dim: each iteration uses its own index of \texttt{accum}.
}

\newpage
\myBiggerTitle{GEMM: Loop Fission}

{\large
\input{nyc25_tex/expand_dim.2.tex}
}

{\LARGE
We will split the loop at the indicated locations, retaining the loop structure.
}

\newpage

{\large
\input{nyc25_tex/fission.0.tex}
}

\newpage
\myBiggerTitle{GEMM: K Loop Tiling}

{\large
\input{nyc25_tex/fission.1.tex}
}

{\LARGE
We'll focus on the accumulate loops for the rest of the talk.

}

\newpage
\myBiggerTitle{GEMM: K Loop Tiling}

{\large
\input{nyc25_tex/smem_broken.0.tex}
}

%% {\LARGE
%% Split k loop, and reorder sequential \blueBox{k1} loop outwards
%% }

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_no_sync_flow.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/k1_before_smem.0.tex}
}

{\LARGE
Replace direct reads from \greenBox{A}, \violetBox{B} (in GMEM) with SMEM.

}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.1.tex}
}

{\LARGE
Stage $\texttt{M1} \times \texttt{K0}$ tile of \greenBox{\texttt{A}}, $\texttt{K0} \times \texttt{N1}$ tile of \violetBox{\texttt{B}} in shared memory.

\texttt{A\_smem}, \texttt{B\_smem} replace \texttt{A} and \texttt{B} in the loop body.

}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_broken.2.tex}
}

{\LARGE
The compiler generates loops to load the shared memory tile.

\texttt{seq} loops by default -- we will fix this!

}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_in_order.0.tex}
}

{\LARGE
Divide \& Parallelize the load-shared-memory loops.
}

\newpage
\myBiggerTitle{GEMM: Stage in SMEM}

{\large
\input{nyc25_tex/smem_in_order.1.tex}
}

{\LARGE
Add synchronization:\\
All threads in the thread block wait for each other.
}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary}

{\LARGE
Synchronization required:\\
Thread that loaded a value to SMEM isn't the same as the one that uses it.

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary}

{\LARGE
First sync: wait for SMEM tile to fill\\(RAW: read-after-write)

Second sync: don't overwrite until all SMEM reads are done\\(WAR: write-after-read)

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\end{tikzpicture}
\end{center}

\newpage
\myBiggerTitle{``Classic'' GPU GEMM Summary}

{\LARGE
Use accelerator instructions for the \textbf{GMEM}$\to$\textbf{SMEM} copies.

}

\vfill
\hrule
\vfill

\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\node (A_caption) [redstyle, above=of A, yshift=8mm, xshift=6mm] {Accelerate me!};
\node (B_caption) [redstyle, above=of B, yshift=8mm, xshift=6mm] {Accelerate me!};
\draw [redstyle, arrow, fill=none] (A) -- (A_caption);
\draw [redstyle, arrow, fill=none] (B) -- (B_caption);
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{Async Copies}

{\LARGE
Ampere (sm\_80) introduced async copy instructions\\
(GMEM $\to$ SMEM)

Execution continues on without waiting for the copy to finish.

\yellowBox{Less background here}

}

{\Large
\input{nyc25_tex/cp_async_pseudocode.0.tex}
}

{\LARGE
\myKeyA{Overlapping:} issue non-dependent \myKeyA{compute} work while copies are in-flight.
}

\newpage
\myBiggerTitle{Timelines}

{\LARGE
Most synchronization instructions \myKeyA{don't wait} for cp.async.

\myKeyA{Timelines:} categorize instrs by \myKeyA{synchronization needed}.

\begin{itemize}
  \item \textbf{\texttt{cuda\_in\_order}}: most CUDA instructions
  \item \textbf{\texttt{Sm80\_cp\_async}}: \texttt{cp.async} (special syncs needed)
  \item Others for H100 (not in this talk)
\end{itemize}

}

\newpage
\myBiggerTitle{Exo-GPU Synchronization}

{\LARGE
Parameterize Exo-GPU synchronization with timelines:

\begin{center}
  \texttt{Fence(\myKeyA{L1}, \myKeyB{L2})}
\end{center}

\textbf{Semi-declarative}; user specifies intended effect:

\begin{center}
prior \myKeyA{L1}-timeline instrs $\xrightarrow{\textbf{SYNC}}$ future \myKeyB{L2}-timeline instrs
\end{center}

Compiler chooses exact instructions.

Otherwise imperative\\(executes in program order with surrounding instructions)

}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.2.tex}
}

{\LARGE
Current main loop:
only \textbf{\texttt{cuda\_in\_order}} instructions used,
so \textbf{\texttt{Fence}} is parameterized with \texttt{cuda\_in\_order}.
}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.3.tex}
}

{\LARGE
TODO: replace these GMEM$\to$SMEM copies with cp.async.
}

\newpage
\myBiggerTitle{GEMM: Prepare for cp.async}

{\large
\input{nyc25_tex/smem_in_order.4.tex}
}

{\LARGE
TODO: update timeline parameters for synchronization.
}

\newpage
\myBiggerTitle{GEMM: cp.async (Substitute Instruction)}

{\large
\input{nyc25_tex/cp_async.0.tex}
}

{\LARGE

Replace \texttt{A\_smem[\codecomment{...}] = A[\codecomment{...}]} and \texttt{B\_smem[\codecomment{...}] = B[\codecomment{...}]}\\with cp.async instructions.

Exo allows the rewrite, because the substituted functionality, \textit{ignoring concurrency}, is equivalent.

}

\newpage
\myBiggerTitle{GEMM: cp.async (CudaAsync block)}

{\large
\input{nyc25_tex/cp_async.1.tex}
}

{\LARGE

Such instructions must be wrapped in a CudaAsync block.

}

\newpage
\myBiggerTitle{GEMM: cp.async (Synchronization)}

{\large
\input{nyc25_tex/cp_async.2.tex}
}

{\LARGE

Update the timeline parameters for synchronization.

}

\newpage
\myBiggerTitle{More to do: Overlap Compute/Memory}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\end{tikzpicture}
\end{center}


\newpage
\myBiggerTitle{More to do: Overlap Compute/Memory}
\begin{center}
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_sync_flow.tex}
\node (async) [redstyle, right=of A, xshift=-20mm, yshift=-12mm, align=center, text width=40mm, minimum height=25mm] {\textbf{NOW WITH cp.async!}};
\end{tikzpicture}
\end{center}
\hrule
{\LARGE
We need to \myKeyA{delay} the accumulate step to a future iteration to make room for overlapping work!

}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Loop skew: delay \myKeyA{consumption} (compute) of SMEM data by one \blueBox{\texttt{k1}} iteration from SMEM load.

}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Need to transform SMEM into a \myKeyA{ring buffer}.

One slot may be being filled while another is consumed.

}

\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}
\input{nyc25_dag_war.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
WAR hazard: require syncs before re-using ring buffer slots.

}


\newpage
\myBiggerTitle{Producer/Consumer Dependencies}

{
\Large
\begin{tikzpicture}[node distance=0mm]
\input{nyc25_dag.tex}
\input{nyc25_dag_k.tex}
\input{nyc25_dag_ring.tex}
\input{nyc25_dag_war.tex}

\end{tikzpicture}
}

\hrule

{\LARGE
Implement dependency graph edges with \myKeyA{split barriers}:\\Arrive/Await.
}

\newpage
\myBiggerTitle{Exo-GPU: Arrive/Await}

{\LARGE
Special variable type: \texttt{\textit{var}: \textbf{barrier}}.

Usage: \texttt{Arrive(\myKeyA{L1}, 1) >> \textit{var}} \hfill \texttt{Await(\textit{var}, \myKeyB{L2}, \blueBox{\textsf{delay}})}

where \texttt{\myKeyA{L1}, \myKeyB{L2}} are timelines as in \texttt{Fence}.

\hspace{5mm}

Threads can't pass the \texttt{Await} until all threads have passed the matched \texttt{Arrive} (matching controled by \blueBox{delay}).

\hspace{5mm}

Put non-dependent work between the Arrive and Await!

}

\newpage
\myBiggerTitle{GEMM: Todo}

{\large
\input{nyc25_tex/ring_todo.0.tex}
}

{\LARGE
Currently: data movement at the top, compute at the bottom.
}

\newpage
\myBiggerTitle{GEMM: Todo, Ring Buffer}

{\large
\input{nyc25_tex/ring_todo.1.tex}
}

{\LARGE
Move the \redBox{\texttt{smem}} declarations out of the \blueBox{\texttt{k1}} loop.\\
Add ring buffer dimension.
}

\newpage
\myBiggerTitle{GEMM: Todo, Loop Skew}

{\large
\input{nyc25_tex/ring_todo.2.tex}
}

{\LARGE
Delay compute (bottom) relative to data movement (top).
}

\newpage
\myBiggerTitle{GEMM: Todo, Arrive/Await}

{\large
\input{nyc25_tex/ring_todo.3.tex}
}

{\LARGE
Replace with \textbf{\texttt{Arrive}}, \textbf{\texttt{Await}}.
}

\newpage
\myBiggerTitle{GEMM: Ring Buffer}

{\large
\input{nyc25_tex/ring.0.tex}
}

{\LARGE

Lift \texttt{A\_smem}, \texttt{B\_smem} out of the \texttt{k1} loop and ring buffer by 3.

}

\newpage
\myBiggerTitle{GEMM: Loop Skew}

{\large
\input{nyc25_tex/ring.1.tex}
}

{\LARGE

Add an extra \blueBox{\texttt{k1}} iteration; delay the accum code by 1 iteration relative to the cp.async code.

}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.0.tex}
}

{\LARGE
Declare \textbf{\texttt{barrier}}-typed variable.

}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.1.tex}
}

{\LARGE
Resolve RAW Hazard (\textbf{\texttt{Sm80\_cp\_async $\to$ cuda\_in\_order}}).

}

\newpage
\myBiggerTitle{GEMM: Split Barriers}

{\large
\input{nyc25_tex/split.2.tex}
}

{\LARGE
Resolve WAR Hazard (\textbf{\texttt{cuda\_in\_order $\to$ Sm80\_cp\_async}}).

Required to re-use ring buffer slot.

}

\newpage
\myBiggerTitle{Tensor Cores}

{\LARGE

Tensor Cores = More timelines + more complicated instruction substitution

}

\newpage
\myBiggerTitle{Abstract Machine}

{\LARGE

Talk about visibility sets
\begin{itemize}
  \item ``qualitative'' timeline + ``quantitative'' thread ID
  \item grows with synchronization
\end{itemize}

Talk about two-stage correctness
\begin{itemize}
  \item Stage 1: rewrite operations
  \item Stage 2: parallelism
\end{itemize}

Talk about abstract machine as tool for checking correctness

Talk about abstract machine constraining the compiler's outputted CUDA

}

\end{document}



%% \newpage
%% \myBiggerTitle{Challenge: SIMT Parallelism}

%% {\LARGE

%% Talk about launching blocks of threads

%% Talk about mapping work to threads

%% Diagram: vector add eye candy? y[threadIdx] += x[threadIdx]

%% }

%% \newpage
%% \myBiggerTitle{Challenge: Memory / Compute Overlap}

%% {\LARGE
%% SIMT = overlapping identical workloads.\\
%% We should also overlap heterogenous work: data \& compute.

%% }
%% \hrule
%% {
%% \Large
%% \begin{tikzpicture}[node distance=0mm]

%% \node (C0) [draw=black, minimum width=28mm, minimum height=24mm] {compute};
%% \node (C1) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C0) {compute};
%% \node (C2) [draw=black, minimum width=28mm, minimum height=24mm, anchor=center, xshift=72mm] at(C1) {compute};
%% \node(bad) [left=of C0, xshift=-40mm] {\textbf{BAD}};

%% \node (A0) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C0) {Load A};
%% \node (B0) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C0) {Load B};

%% \node (A1) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C1) {Load A};
%% \node (B1) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C1) {Load B};

%% \node (A2) [greenstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=6mm] at(C2) {Load A};
%% \node (B2) [violetstyle, minimum width=28mm, minimum height=12mm, anchor=center, xshift=-36mm, yshift=-6mm] at(C2) {Load B};

%% \draw [arrow] (A0.south east) -- (C0.west);
%% \draw [arrow] (C0.east) -- (A1.south west);
%% \draw [arrow] (A1.south east) -- (C1.west);
%% \draw [arrow] (C1.east) -- (A2.south west);
%% \draw [arrow] (A2.south east) -- (C2.west);

%% \end{tikzpicture}
%% }
%% \vspace{-1mm}
%% \hrule
%% {
%% \Large
%% \begin{tikzpicture}[node distance=0mm]
%% \input{nyc25_dag.tex}
%% \input{nyc25_dag_war.tex}
%% \node(good) [anchor=center] at(C0) {\textbf{GOOD}};
%% \end{tikzpicture}
%% }

%% \newpage
%% \myBiggerTitle{Challenge: Efficient Synchronization}

%% {\LARGE

%% Try not to stall threads

%% Don't screw up

%% Nondeterministic \& subtle bugs possible

%% }
