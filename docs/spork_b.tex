% exocc b_samples.py && python3 code_to_tex.py b_samples.py b_samples && xelatex spork_b.tex </dev/null
\input{whitepaper_common.tex}

\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{redstyle} = [draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowstyle} = [draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greenstyle} = [draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluestyle} = [draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetstyle} = [draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{nRedstyle} = [draw=nRedBoxFg, fill=nRedBoxBg, text=nRedBoxFg]
\tikzstyle{nGoldstyle} = [draw=nGoldBoxFg, fill=nGoldBoxBg, text=nGoldBoxFg]
\tikzstyle{nGreenstyle} = [draw=nGreenBoxFg, fill=nGreenBoxBg, text=nGreenBoxFg]
\tikzstyle{nBluestyle} = [draw=nBlueBoxFg, fill=nBlueBoxBg, text=nBlueBoxFg]
\tikzstyle{nPurplestyle} = [draw=nPurpleBoxFg, fill=nPurpleBoxBg, text=nPurpleBoxFg]

\tikzstyle{CollTypeExampleStyle} = [rectangle, draw=black, text centered, text width=23mm]
\tikzstyle{CollTilingExampleStyle} = [rectangle, draw=black, text centered, text width=12.5mm]

\begin{document}

% deriveCollTiling
% Instructions, use deriveCollTiling for memory
% out-of-order non-convergent abstract machine optimization

\myTitle{Exo-GPU (Spork) Guide}

\myChapterLink{sec:Overview}{Overview}

\myChapterLink{sec:CudaDeviceFunction}{Cuda Device Function \& Warp Specialization}

Launching kernels, distributing work to clusters with \lighttt{cuda\_tasks} loops, \lighttt{CudaWarps} blocks.

\myChapterLink{sec:CollType}{Collective Units \& Collective Types}

Modeling subdivisions of threads in the cluster, e.g. threads, warps, warpgroups, CTAs, CTA pairs.

\myChapterLink{sec:CollTiling}{Collective Tiling}

Distributing work to threads-in-cluster with \lighttt{cuda\_threads} loops and \lighttt{CudaWarps} blocks.

\myChapterLink{sec:DistributedMemory}{Distributed Memory}

Sharding tensor and barrier allocations onto different groups of threads.

\myChapterLink{sec:SyncUsage}{Synchronization Usage}

\lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}, home barrier, guarding, async instructions, timelines.

\myChapterLink{sec:SyncSemantics} {Synchronization Semantics}

Abstract machine semantics, how to read sync-check (\lighttt{camspork}) errors.

\myChapterLink{sec:Instr}{Instructions \& Timelines}

Instruction classes, timelines, atomic instructions, special window e.g., \lighttt{CUtensorMap}.

\myChapterLink{sec:Glossary}{Glossary \& Reference}

\section{Overview}
\label{sec:Overview}


This document assumes knowledge of CPU-only Exo.
We describe the concepts of the GPU extension.
All Exo code continues to be CPU code by default (we say that such code is at \myKeyA{CPU-scope}).
To move code to the GPU, wrap it inside a \lighttt{with CudaDeviceFunction} block (Section~\ref{sec:CudaDeviceFunction}); this defines the \lighttt{clusterDim} (number of CTAs per cluster, def~\ref{sec:gCluster}) and \lighttt{blockDim} (number of threads per CTA, def~\ref{sec:gCta}), and may also define \myKeyA{warp variables} (def~\ref{sec:gWarpVariable}) giving names to subcollections of warps (def~\ref{sec:gWarp}) for the purposes of warp specialization.
The statements within are at \myKeyA{CUDA scope} and are converted to CUDA C++ code.

We generalize loops in Exo to sequential and parallel for loops, distinguished by their \myKeyA{loop mode} (def~\ref{sec:gLoopMode}).
The body of the \lighttt{CudaDeviceFunction} block must contain only a single statement: a nest of one or more \lighttt{cuda\_tasks} loops.
The body of the inner-most \lighttt{cuda\_tasks} loop is a \myKeyA{device task}; one instance of the device task is assigned to one CUDA cluster for execution (see example~\ref{sec:gCudaDeviceFunction}).
Within the device task, each statement instance (def~\ref{sec:gStatementInstance}) is executed by a set of threads within the cluster; this is a \myKeyA{thread collective}.

A \lighttt{cuda\_threads(0, $c_\text{hi}$, unit=\_)} loop may appear anywhere inside a device task.
This statement divides its executing thread collective into $c_\text{hi}$-many disjoint subsets (possibly with some inactive threads left over), and assigns one to execute each iteration of the loop.
The \lighttt{unit} parameter is a \myKeyA{collective unit} (def~\ref{sec:gCollUnit}), from which a \myKeyA{collective type} $\delta$ is unpacked (Section~\ref{sec:CollType}).
The collective type describes a number and arrangement of threads (e.g. single thread, warp, CTA, CTA pair).
In the common case, the thread collectives assigned to each iteration are described by this unpacked $\delta$.

This design encodes uniform execution as a mostly syntactic property of the language.
When the thread collectives that execute instances of a statement are described by a collective type $\delta$, the compiler deduces that the statement is at $\delta$-scope (e.g. warp-scope, CTA-scope, as in the top of Figure~\ref{fig:OverviewThreads}).
The underlying static analysis for this is based on \myKeyA{local thread indices} (def~\ref{sec:gLocalThreadIndex}), which number the threads within a cluster lexicographically based on CTA index, then thread index.
The threads within a thread collective need not have a contiguous range of local thread indices (e.g., ``all even numbered CTAs in a cluster'' is a valid thread collective).

Based on this static analysis, we define a \myKeyA{collective tiling} (Section~\ref{sec:CollTiling}) and thread mapping function annotating each CUDA-scope statement, we define \myKeyA{distributed memory} (Section~\ref{sec:DistributedMemory}), which maps shards of an array onto different thread collectives for storage, and we define correct synchronization (Section~\ref{sec:SyncSemantics}), which we currently can check for a selection of concrete problem sizes.

\subsection{Distributed Memory Overview}

Each \lighttt{cuda\_threads} loop iterator indirectly defines a \myKeyA{thread pitch} (def~\ref{sec:gThreadPitch}).
This is the difference, in local thread indices (def~\ref{sec:gLocalThreadIndex}), between thread collectives assigned to consecutive loop iterations.
Distributed memory is deduced from the indexing pattern of an array, and attempts to map shards of the array into thread collectives described by the collective type specified by the memory type.
This flexibility allows memory to be sharded at different levels of the CUDA thread hierarchy (e.g. per-CTA shared memory shards, per-thread register shards).
The deduction assigns a thread pitch to each distributed dimension; elements that are adjacent on a dimension are resident in thread collectives whose local thread indices differ by that dimension's thread pitch.
We define requirements for distributed memory deduction (Section~\ref{sec:DistributedMemory}); in particular, the index used to index a distributed dimension must be a plain read of an iterator with the same thread pitch as that dimension (Figure~\ref{fig:OverviewThreads}, bottom).

\begin{figure}[t]
\codehrule
\input{b_samples/OverviewThreads.0.tex}
\caption{Threads \& Distributed Memory Example Code}
\label{fig:OverviewThreads}
\codehrule
\end{figure}

\subsection{Synchronization Overview}

Exo-GPU introduces synchronization statements: \lighttt{Fence}, \lighttt{Arrive}, and \lighttt{Await}, as well as the ability to allocate barrier variables, which control pairing between \lighttt{Arrive} and \lighttt{Await} statements.
Like other Exo statements, threads execute synchronization statements in program order.

We view each memory access (read or write) as being performed by a certain thread collective and \myKeyA{qualitative timeline} (\textsf{QualTL}, \ref{sec:gQualTL}).
We need this latter attribute to be able to reason about async instructions; the qualitative timeline of a memory access varies depending on what instruction performs the access.

A \lighttt{Fence} statement instance synchronizes the threads within the thread collective that executes it, e.g. a \lighttt{Fence} at warp-scope corresponds to a \lighttt{\_\_syncwarp}-like construct, and a \lighttt{Fence} at CTA-scope corresponds to a \lighttt{\_\_syncthreads}-like construct (Figure~\ref{fig:OverviewSyncExample}).
``Paired'' instances of an \lighttt{Arrive} and an \lighttt{Await} statement implement a split-barrier construct.
We use this syntax:

\hphantom{spacing}
\texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}
\hfill
\texttt{Arrive($\tau_s^\mathrm{pre}$) >}\texttt{> $e$}
\hfill
\texttt{Await($e, \tau_s^\mathrm{post}, n$)}
\hphantom{spacing}

where $\tau_s^\mathrm{pre}$ and $\tau_s^\mathrm{post}$ are \myKeyA{sync timelines} (\textsf{SyncTL}, \ref{sec:gSyncTL}), which filter the set of qualitative timelines of memory accesses that are synchronized, and $e$ and $n$ are a barrier expression (def~\ref{sec:gBarrierExpr}) and an integer, which together control pairing of executed \lighttt{Arrive} and \lighttt{Await} instances.

The barrier variable itself is allocated with the syntax ``\texttt{<name>: barrier[$n^*$] @ $\pi_z$}'', where $n^*$ defines the array size of the barrier variable and $\pi_z$ defines the completion mechanism (\lighttt{CudaMbarrier}, \lighttt{CudaCommitGroup}, or \lighttt{CudaClusterSync}).
The array is subject to distributed memory analysis.

The goal of synchronization checking is to validate \myKeyA{sequential-parallel equivalence}.
The semantics of an Exo program (which defines the numerical outputs) continue to be defined sequentially; we can, in a sense, view the new Exo-GPU constructs as ``annotations'' on sequential code.
In particular, the semantics of parallel loops are identical to sequential loops, and async instruction calls are interpreted as if they were non-async instructions.
We want to convince ourselves that the generated parallel CUDA program generates the outputs that sequential semantics specify it will output.

Although this is not the formulation formally used by the abstract machine used for synchronization checking (Section~\ref{sec:SyncSemantics}), it's useful to reason about correctness in terms of dependency edges between statement instances.
Take a sequential trace of an Exo-GPU program, decomposed into reads and writes to array elements, and instances of \lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}.
For each pair of reads/writes to the same array element (other than two reads) to be safe, there must be a path from the earlier read/write to the later read/write via dependency edges.
Given the statement to the left of the $\to$ appears earlier in the sequential trace than the statement to the right, and there is a thread in common to their executing thread collectives, we have the following dependency edges.
Italicized values are defined in Section~\ref{sec:SyncUsage}.

\begin{itemize}
  \item \texttt{Read|Write $\to$ Read|Write}\\when the prior memory access is not \emph{out-of-order}, and the prior memory access uses a qualitative timeline that is in the \emph{extended qualitative timeline set} of the subsequent memory access.
  \item \texttt{Read|Mutate $\to$ Fence($\tau_s^\text{pre}$, \_)|Arrive($\tau_s^\text{pre}$) >}\texttt{> \_}\\when the qualitative timeline of the memory access is in the \textit{full timeline set} of $\tau_s^\text{pre}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Read}\\when the qualitative timeline of the read is in the \textit{full timeline set} of $\tau_s^\text{post}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Write}\\when the qualitative timeline of the read is in the \textit{temporal timeline set} of $\tau_s^\text{post}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Fence($\tau_s^\text{pre}$, \_)|Arrive($\tau_s^\text{pre}$) >}\texttt{> \_}\\when there is a qualitative timeline in common to the \textit{full timeline sets} of $\tau_s^\text{pre}$ and $\tau_s^\text{post}$.
\end{itemize}

Finally, we have special handling for atomics, there are dependency edges from instances of \lighttt{Arrive(\_) >}\lighttt{> $b$} and \lighttt{Await($b$, \_, \_)} defined by arrive/await pairing, and dependency edges from reads/writes to \lighttt{Await} instances directly for instructions that take a barrier directly (currently, only TMA-to-SMEM instructions).

\begin{figure}[h!]
\codehrule
\input{b_samples/OverviewSyncExample.0.tex}
\caption{Examples of Synchronization Statements}
\label{fig:OverviewSyncExample}
\codehrule
\end{figure}




\FloatBarrier
\newpage
\section{Cuda Device Function \& Warp Specialization}
\label{sec:CudaDeviceFunction}

Wrap code with a \lighttt{with CudaDeviceFunction(...):} statement to transform it to CUDA.
The body of the \lighttt{CudaDeviceFunction} statement must consist of exactly one statement: a nest of one or more \lighttt{cuda\_tasks} loops.
The body of the inner-most \lighttt{cuda\_tasks} loop is a \myKeyA{device task}; each is assigned to a CUDA cluster for execution.
We implement a persistent-kernel design, so multiple tasks may be co-located on the same cluster.
The shape of the \lighttt{cuda\_tasks} iteration space must be a cuboid, i.e., the loop bounds of one \lighttt{cuda\_tasks} loop must not be dependent on another \lighttt{cuda\_tasks} loop.

The \lighttt{CudaDeviceFunction} object is a Python object, containing attributes
\begin{itemize}
  \item \lighttt{clusterDim} (default 1), number of CTAs per cluster.
  \item \lighttt{blocks\_per\_sm} (default 1), number of CTAs concurrently executing per hardware SM.
  \item \lighttt{blockDim}, number of threads per CTA.
  \item \lighttt{warp\_config}, list of \lighttt{CudaWarpConfig} objects.
\end{itemize}
Exactly one of \lighttt{blockDim} or \lighttt{warp\_config} must be given.
The latter is intended for kernels with warp specialization, where we partition the warps in the CTA into named groups of warps, possibly with a different number of registers each.
Each \lighttt{CudaWarpConfig} defines a \myKeyA{warp variable}, and has attributes
\begin{itemize}
  \item \lighttt{name: str}, the name of the warp variable.
  \item \lighttt{count: int}, number of warps.
  \item \lighttt{setmaxnreg\_dec: Optional[int]}, registers per thread; regs allocated by \lighttt{setmaxnreg.dec}.
  \item \lighttt{setmaxnreg\_inc: Optional[int]}, registers per thread; regs allocated by \lighttt{setmaxnreg.inc}.
\end{itemize}
The \lighttt{blockDim} of the CTA is implicitly 32 times the sum of the number of warps defined.
Within the device task, a \lighttt{with CudaWarps(name=<str>)} statement may be used to restrict the body of the statement to only execute on the subset of warps named (Figure~\ref{fig:CudaDeviceFunction0}).

\begin{figure}[h]
\codehrule
\input{b_samples/CudaDeviceFunction.0.tex}
\caption{Kernel launch with warp specialization}
\label{fig:CudaDeviceFunction0}
\codehrule
\end{figure}

\FloatBarrier
\newpage
\section{Collective Units \& Collective Types}
\label{sec:CollType}

We use collective types $\delta$ to describe a quantity and arrangement of threads within a cluster, such as ``single thread'', ``warp'', ``CTA'', ``one warp from a pair of CTAs''.
These are unpacked from a collective unit $\tau_u$ defined in the frontend language (def~\ref{sec:gCollUnit}).
A collective type consists of two equal-length tuples: a domain and a box.
The dimension $M$ of the collective type is the length of these tuples.
The \myKeyA{domain} ($\delta.D_0...\delta.D_{M-1}$): $\mathbb{N}_{\ge2}^M$ describes an organization of the threads in a cluster into an $M$-dimensional grid.
The \myKeyA{box} ($\delta.B_0...\delta.B_{M-1}$): $\mathbb{N}_\top^M$ describes the number of threads on each dimension to select (the special value $\top$ indicates ``no requirement'').

We first define a linear ordering of threads in a cluster, then extend to multidimensional coordinates.
The local thread index of a thread is \lighttt{cluster\_ctarank * blockDim.x + threadIdx.x}
i.e., the threads in a cluster are numbered in (\lighttt{cluster\_ctarank, threadIdx.x})-lexicographical order (Exo-GPU parallelizes on the x dimension only).

For a given domain, we derive the \myKeyA{dimension thread pitch} $\delta.P_i$:
\begin{align*}
    \delta.P_i = \prod_{k=i+1}^{M-1} \delta.D_k
\end{align*}
and we define the mapping $\mathsf{toLocal}(D, c)$ (def~\ref{sec:gToLocal}), which converts a domain $D$ and coordinates $c$ to a local thread index; the coordinates $[0, \delta.D_0-1]_\mathbb{N} \times ... \times [0, \delta.D_{M-1}-1]_\mathbb{N}$ get mapped to local thread indices in lexicographical order.
The product of the domain coordinates $\delta.D_0 \times ... \times \delta.D_{M-1}$ must be equal to the number of threads in the cluster (\lighttt{clusterDim.x * blockDim.x}).

\subsection{Collective Types \& Thread Collectives}
\label{sec:CollTypeThreadCollective}

A thread collective is described by a collective type $\delta$ when all threads are in the same cluster, and, with $\mu: \mathcal{P}(\mathbb{N})$ being the set of local thread indices of the threads, there exist sets $C_0 ... C_{M-1}: \mathcal{P}(\mathbb{N})$ such that
\begin{itemize}
  \item $C_i \subseteq [0, \delta.D_i - 1]$
  \item $\delta.B_i \ne \top \implies \exists x.\; C_i = [x, x + \delta.B_i - 1]_\mathbb{N}$.
  \item $\mu = \{ \mathsf{toLocal}(\delta.D, c) \mid c \in C_0 \times ... \times C_{M-1}\}$
\end{itemize}
If we have no $\top$ box coordinates, this is specifying that the thread collective forms a cuboid of dimensions $\delta.B_0 \times ... \times \delta.B_{M-1}$ when its threads are arranged in the grid implied by $\delta.D$; therefore, the number of threads in the thread collective is the product of the box coordinates (Figure~\ref{fig:CollTypeExample}).

\begin{figure}[h!]
\sffamily
\hspace{-12mm}
\begin{tikzpicture}[node distance=0mm]
\node(t000a) [CollTypeExampleStyle] {(0, 0)\\0 $ \notin \mu$\\0, 0, 0, 0};
\node(t000b) [CollTypeExampleStyle, right=of t000a, xshift=6mm] {(0, 127)\\127 $ \notin \mu$\\0, 0, 0, 127};
\draw [dotted] (t000a) -- (t000b);
\node(t001a) [CollTypeExampleStyle, right=of t000b, xshift=2mm, yshift=-3mm] {(0, 128)\\128 $ \notin \mu$\\\blueBox{0}, 0, 1, 0};
\node(t001b) [CollTypeExampleStyle, right=of t001a, xshift=6mm] {(0, 255)\\255 $ \notin \mu$\\0, 0, 1, 127};
\draw [dotted] (t001a) -- (t001b);
\node(t002a) [CollTypeExampleStyle, right=of t001b, xshift=2mm, yshift=-3mm] {(0, 256)\\256 $ \notin \mu$\\0, 0, \violetBox{2}, \redBox{0}};
\node(t002b) [CollTypeExampleStyle, right=of t002a, xshift=6mm] {(0, 383)\\383 $ \notin \mu$\\0, 0, \violetBox{2}, \redBox{127}};
\draw [dotted] (t002a) -- (t002b);
\node(t010a) [CollTypeExampleStyle, below=of t000a, yshift=-2mm] {(1, 0)\\384 $ \notin \mu$\\0, 1, 0, 0};
\node(t010b) [CollTypeExampleStyle, right=of t010a, xshift=6mm] {(1, 127)\\511 $ \notin \mu$\\0, 1, 0, 127};
\draw [dotted] (t010a) -- (t010b);
\node(t011a) [CollTypeExampleStyle, right=of t010b, xshift=2mm, yshift=-3mm] {(1, 128)\\512 $ \notin \mu$\\0, 1, 1, 0};
\node(t011b) [CollTypeExampleStyle, right=of t011a, xshift=6mm] {(1, 255)\\639 $ \notin \mu$\\0, 1, 1, 127};
\draw [dotted] (t011a) -- (t011b);
\node(t012a) [CollTypeExampleStyle, right=of t011b, xshift=2mm, yshift=-3mm] {(1, 256)\\640 $ \notin \mu$\\0, 1, 2, 0};
\node(t012b) [CollTypeExampleStyle, right=of t012a, xshift=6mm] {(1, 383)\\767 $ \notin \mu$\\0, 1, 2, 127};
\draw [dotted] (t012a) -- (t012b);

\node(t100a) [CollTypeExampleStyle, below=of t010a, yshift=-4mm, xshift=-4mm] {(2, 0)\\768 $ \notin \mu$\\\blueBox{1}, \yellowBox{0}, 0, 0};
\node(t100b) [CollTypeExampleStyle, right=of t100a, xshift=6mm] {(2, 127)\\895 $ \notin \mu$\\1, 0, 0, 127};
\draw [dotted] (t100a) -- (t100b);
\node(t101a) [CollTypeExampleStyle, right=of t100b, xshift=2mm, yshift=-3mm] {(2, 128)\\896 $ \notin \mu$\\\blueBox{1}, 0, 1, 0};
\node(t101b) [CollTypeExampleStyle, right=of t101a, xshift=6mm] {(2, 255)\\1023 $ \notin \mu$\\1, 0, 1, 127};
\draw [dotted] (t101a) -- (t101b);
\node(t102a) [CollTypeExampleStyle, greenstyle, right=of t101b, xshift=2mm, yshift=-3mm] {(2, 256)\\1024 $ \in \mu$\\1, 0, 2, 0};
\node(t102b) [CollTypeExampleStyle, greenstyle, right=of t102a, xshift=6mm] {(2, 383)\\1151 $ \in \mu$\\1, 0, 2, 127};
\draw [dotted] (t102a) -- (t102b);
\node(t110a) [CollTypeExampleStyle, below=of t100a, yshift=-2mm] {(3, 0)\\1152 $ \notin \mu$\\\blueBox{1}, \yellowBox{1}, 0, 0};
\node(t110b) [CollTypeExampleStyle, right=of t110a, xshift=6mm] {(3, 127)\\1279 $ \notin \mu$\\1, 1, 0, 127};
\draw [dotted] (t110a) -- (t110b);
\node(t111a) [CollTypeExampleStyle, right=of t110b, xshift=2mm, yshift=-3mm] {(3, 128)\\1280 $ \notin \mu$\\1, 1, 1, 0};
\node(t111b) [CollTypeExampleStyle, right=of t111a, xshift=6mm] {(3, 255)\\1407 $ \notin \mu$\\1, 1, 1, 127};
\draw [dotted] (t111a) -- (t111b);
\node(t112a) [CollTypeExampleStyle, greenstyle, right=of t111b, xshift=2mm, yshift=-3mm] {(3, 256)\\1408 $ \in \mu$\\1, 1, 2, 0};
\node(t112b) [CollTypeExampleStyle, greenstyle, right=of t112a, xshift=6mm] {(3, 383)\\1535 $ \in \mu$\\1, 1, 2, 127};
\draw [dotted] (t112a) -- (t112b);

\node(t200a) [CollTypeExampleStyle, below=of t110a, yshift=-4mm, xshift=-4mm] {(4, 0)\\1536 $ \notin \mu$\\2, 0, 0, 0};
\node(t200b) [CollTypeExampleStyle, right=of t200a, xshift=6mm] {(4, 127)\\1663 $ \notin \mu$\\2, 0, 0, 127};
\draw [dotted] (t200a) -- (t200b);
\node(t201a) [CollTypeExampleStyle, right=of t200b, xshift=2mm, yshift=-3mm] {(4, 128)\\1664 $ \notin \mu$\\\blueBox{2}, \yellowBox{0}, 1, 0};
\node(t201b) [CollTypeExampleStyle, right=of t201a, xshift=6mm] {(4, 255)\\1791 $ \notin \mu$\\2, 0, 1, 127};
\draw [dotted] (t201a) -- (t201b);
\node(t202a) [CollTypeExampleStyle, right=of t201b, xshift=2mm, yshift=-3mm] {(4, 256)\\1792 $ \notin \mu$\\2, 0, 2, \redBox{0}};
\node(t202b) [CollTypeExampleStyle, right=of t202a, xshift=6mm] {(4, 383)\\1919 $ \notin \mu$\\2, 0, 2, \redBox{127}};
\draw [dotted] (t202a) -- (t202b);
\node(t210a) [CollTypeExampleStyle, below=of t200a, yshift=-2mm] {(5, 0)\\1920 $ \notin \mu$\\2, 1, \violetBox{0}, 0};
\node(t210b) [CollTypeExampleStyle, right=of t210a, xshift=6mm] {(5, 127)\\2047 $ \notin \mu$\\2, 1, 0, 127};
\draw [dotted] (t210a) -- (t210b);
\node(t211a) [CollTypeExampleStyle, right=of t210b, xshift=2mm, yshift=-3mm] {(5, 128)\\2048 $ \notin \mu$\\2, \yellowBox{1}, \violetBox{1}, 0};
\node(t211b) [CollTypeExampleStyle, right=of t211a, xshift=6mm] {(5, 255)\\2175 $ \notin \mu$\\2, 1, 1, 127};
\draw [dotted] (t211a) -- (t211b);
\node(t212a) [CollTypeExampleStyle, right=of t211b, xshift=2mm, yshift=-3mm] {(5, 256)\\2176 $ \notin \mu$\\2, 1, \violetBox{2}, 0};
\node(t212b) [CollTypeExampleStyle, right=of t212a, xshift=6mm] {(5, 383)\\2303 $ \notin \mu$\\2, 1, 2, 127};
\draw [dotted] (t212a) -- (t212b);

\node(t300a) [CollTypeExampleStyle, below=of t210a, yshift=-4mm, xshift=-4mm] {(6, 0)\\2304 $ \notin \mu$\\3, 0, 0, 0};
\node(t300b) [CollTypeExampleStyle, right=of t300a, xshift=6mm] {(6, 127)\\2431 $ \notin \mu$\\3, 0, 0, 127};
\draw [dotted] (t300a) -- (t300b);
\node(t301a) [CollTypeExampleStyle, right=of t300b, xshift=2mm, yshift=-3mm] {(6, 128)\\2432 $ \notin \mu$\\\blueBox{3}, 0, 1, 0};
\node(t301b) [CollTypeExampleStyle, right=of t301a, xshift=6mm] {(6, 255)\\2559 $ \notin \mu$\\3, 0, 1, 127};
\draw [dotted] (t301a) -- (t301b);
\node(t302a) [CollTypeExampleStyle, right=of t301b, xshift=2mm, yshift=-3mm] {(6, 256)\\2560 $ \notin \mu$\\3, 0, 2, 0};
\node(t302b) [CollTypeExampleStyle, right=of t302a, xshift=6mm] {(6, 383)\\2687 $ \notin \mu$\\3, 0, 2, 127};
\draw [dotted] (t302a) -- (t302b);
\node(t310a) [CollTypeExampleStyle, below=of t300a, yshift=-2mm] {(7, 0)\\2688 $ \notin \mu$\\3, 1, 0, 0};
\node(t310b) [CollTypeExampleStyle, right=of t310a, xshift=6mm] {(7, 127)\\2815 $ \notin \mu$\\3, 1, 0, 127};
\draw [dotted] (t310a) -- (t310b);
\node(t311a) [CollTypeExampleStyle, right=of t310b, xshift=2mm, yshift=-3mm] {(7, 128)\\2816 $ \notin \mu$\\3, 1, 1, 0};
\node(t311b) [CollTypeExampleStyle, right=of t311a, xshift=6mm] {(7, 255)\\2943 $ \notin \mu$\\3, 1, 1, 127};
\draw [dotted] (t311a) -- (t311b);
\node(t312a) [CollTypeExampleStyle, right=of t311b, xshift=2mm, yshift=-3mm] {(7, 256)\\2944 $ \notin \mu$\\3, 1, 2, 0};
\node(t312b) [CollTypeExampleStyle, right=of t312a, xshift=6mm] {(7, 383)\\3071 $ \notin \mu$\\3, 1, 2, 127};
\draw [dotted] (t312a) -- (t312b);

\draw[line, bluestyle] (t100a.north west) -- ($(t100a.north west) - (14mm, 0mm)$);
\draw[line, bluestyle] (t110a.north west) -- ($(t110a.north west) - (14mm, 0mm)$);
\draw[line, bluestyle] ($(t110a.north west) - (14mm, 0mm)$) -- ($(t100a.north west) - (14mm, 0mm)$);
\node(dim0) [anchor=south west] at($(t100a.north west) - (14mm, 0mm)$) {\blueBox{dim 0: $[1, 1]_\mathbb{N}, \delta.B_0=1$}};

\draw[line, yellowstyle] (t100a.south west) -- ($(t100a.south west) - (12mm, 0mm)$);
\draw[line, yellowstyle] (t110a.south west) -- ($(t110a.south west) - (12mm, 0mm)$);
\draw[line, yellowstyle] ($(t110a.south west) - (12mm, 0mm)$) -- ($(t100a.south west) - (12mm, 0mm)$);
\node(dim1) [anchor=north west] at($(t110a.south west) - (12mm, 0mm)$) {\yellowBox{dim 1: $[0, 1]_\mathbb{N}, \delta.B_1=2$}};

\draw[line, violetstyle] (t002a.north west) -- ($(t002a.north west) + (0mm, 15mm)$);
\draw[line, violetstyle] (t002b.north west) -- ($(t002b.north west) + (0mm, 15mm)$);
\draw[line, violetstyle] ($(t002a.north west) + (0mm, 15mm)$) -- ($(t002b.north west) + (0mm, 15mm)$);
\node(dim2) [anchor=south] at($(t002a.north west)!0.5!(t002b.north west)+(0mm, 15mm)$) {\violetBox{dim 2: $[2, 2]_\mathbb{N}, \delta.B_2=1$}};

\draw[line, redstyle] (t002a.north east) -- ($(t002a.north east) + (0mm, 4mm)$);
\draw[line, redstyle] (t002b.north east) -- ($(t002b.north east) + (0mm, 4mm)$);
\draw[line, redstyle] ($(t002a.north east) + (0mm, 4mm)$) -- ($(t002b.north east) + (0mm, 4mm)$);
\node(dim3) [anchor=south east] at($(t002b.north east)+(0mm, 4mm)$) {\redBox{dim 3: $[0, 127]_\mathbb{N}, \delta.B_3=128$}};

\draw[thick, <->, bluestyle] ($(t001a.north west) - (3mm, 0mm)$) -- ($(t311a.south west) - (3mm, 0mm)$);
\node(D0) [anchor=center] at($(t001a.north west)!0.5!(t311a.south west) - (3mm, 0mm)$) {\blueBox{$\delta.D_0=4$}};

\draw[thick, <->, yellowstyle] ($(t201a.north east)+(3mm, 0mm)$) -- ($(t211a.south east)+(3mm, 0mm)$);
\node(D1) [anchor=center] at($(t201a.north east)!0.5!(t211a.south east)+(3mm, 0mm)$) {\yellowBox{$\delta.D_1=2$}};

\draw[thick, <->, violetstyle] ($(t210a.south west)-(0mm, 2mm)$) -- ($(t212b.south east)-(0mm, 2mm)$);
\node(D2) [anchor=center] at($(t210a.south west)!0.6!(t212b.south east)-(0mm, 2mm)$) {\violetBox{$\delta.D_2=3$}};

\draw[thick, <->, redstyle] ($(t202a.south west) + (2mm, -1.3mm)$) -- ($(t202b.south east) + (-2mm, -1.3mm)$);
\node(D3) [anchor=center] at($(t202a.south west)!0.5!(t202b.south east) + (0mm, -1.3mm)$) {\redBox{$\delta.D_3=128$}};

\node(legend) [rectangle, draw=black, text centered, text width=5cm, above=of t000a, xshift=40mm, yshift=2mm] {(cluster\_ctarank, threadIdx.x)\\local thread index $\in \mu$\\coordinates};
\end{tikzpicture}
\caption{Example of a thread collective being described by the collective type $\delta$ with $\delta.B = (1, 2, 1, 128)$ and $\delta.D = (4, 2, 3, 128)$, which is a collective type in aligned form (def~\ref{sec:gAlignedForm}). \lighttt{clusterDim = 8} and \lighttt{blockDim = 384}. $\mu = \{ \mathsf{toLocal}(\delta.D, c) \mid c \in [1, 1]_\mathbb{N} \times [0, 1]_\mathbb{N} \times [2, 2]_\mathbb{N} \times [0, 127]_\mathbb{N}\}$ (def~\ref{sec:gToLocal}). This is one warpgroup per CTA of a CTA pair.}
\label{fig:CollTypeExample}
\end{figure}

\subsection{Collective Type Reshape}
\label{sec:CollTypeReshape}

Reshaping a collective type means to apply a series of dimension split operations to yield a new collective type.
We split the $k^{th}$ dimension of a collective type by a factor $f \in \mathbb{N}$ by
\begin{itemize}
  \item Inserting the coordinate pair $(\delta.D_k / f, f)$ in place of $\delta.D_k$.
  \item Inserting the coordinate pair $(\delta.B_k / f, f)$ in place of $\delta.B_k$, if $\delta.B_k \ne \top$ and $\delta.B_k \ge f$.
  \item Inserting the coordinate pair $(1, \delta.B_k)$ in place of $\delta.B_k$, if $\delta.B_k \ne \top$ and $\delta.B_k < f$.
  \item Inserting the coordinate pair $(\top, \top)$ in place of $\delta.B_k$, if $\delta.B_k = \top$.
\end{itemize}
e.g. if $\delta.D = (4, 384)$ and $\delta.B = (1, 128)$, then splitting dimension 1 by $32$ gives $\delta.D = (4, 12, 32)$ and $\delta.B = (1, 4, 32)$.

\subsection{Collective Unit to Collective Type}
\label{sec:CollUnit}

Collective units are also parameterized by a pair of $M$-tuples (domain and box), with coordinates being integer expressions of \lighttt{blockDim} and \lighttt{clusterDim}, or $\top$ in the case of the box.
This is represented in the frontend language as a Python \lighttt{CollUnit} object (the code consistently abbreviates ``collective'' as ``coll''), with $\top$ represented by \lighttt{None}.
We describe this further in def~\ref{sec:gCollUnit}.

\FloatBarrier
\newpage
\useMainSub
\section{Collective Tiling}
\label{sec:CollTiling}

The \lighttt{cuda\_tasks} loops assign instances of device tasks (def~\ref{sec:gDeviceTask}) to different clusters on the system, and the user has no control (yet) over the mapping between device tasks and clusters.
On the other hand, the \lighttt{cuda\_threads} loop, which assigns work to threads within a cluster, provides the user with tight control over this work mapping.

Each statement at CUDA scope (def~\ref{sec:gCudaScope}) has a deduced \myKeyA{collective tiling} attribute.
The collective tiling describes an arrangement of the threads in the cluster into a multidimensional space, in the same manner as a collective type's domain (def~\ref{sec:gDomain}), and groups \lighttt{cuda\_threads} loop iterators by the dimension they operate on.
The statement's collective tiling may be converted to
\begin{itemize}
  \item an \myKeyA{output collective type}.
    The thread collective assigned to execute an instance of this statement will always be described (Section~\ref{sec:CollTypeThreadCollective}) by this collective type.
  \item a \myKeyA{thread mapping function} of type $\Sigma_c \to \mathcal{P}(\mathbb{N})$; this converts the control environment $\sigma_c:\Sigma_c$ (i.e. $\sigma_c: \mathbb{Y} \to \mathbb{Z}$; the per-control-variable values) to the local thread indices (def~\ref{sec:gLocalThreadIndex}) of the thread collective assigned to execute an instance of this statement.
  NB this is using syntax from the PLDI submission.
  \item a per-\lighttt{cuda\_threads} iterator \myKeyA{thread pitch}: $\mathbb{N}$ (def~\ref{sec:gThreadPitch}).
  \item a per-\lighttt{cuda\_threads} iterator \myKeyA{tiled dimension index}: $\mathbb{N}_\bot$.
\end{itemize}
The \myKeyA{domain} and \myKeyA{box} of a collective tiling $\omega$ are that of its output collective type, and for brevity, we denote it as $\omega.D$ and $\omega.B$ respectively.

\subsection{Collective Tiling State}
\label{sec:CollTilingState}

A collective tiling $\omega: \Omega$ (of some dimensionality $M$) is an $M$-tuple of \myKeyA{collective dimension descriptors} $(\omega_0 ... \omega_{M-1})$.
Each descriptor $\omega_i$ consists of
\begin{itemize}
  \item $\omega_i.n: \mathbb{N}$, dimension extent
  \item $\omega_i.\textit{ops}: \mathcal{O}^*$, tuple of \myKeyA{collective dimension operators}
\end{itemize}
where $\mathcal{O}$ is $\mathbb{Y} \times \mathbb{N}^3$, with each attribute denoted as
\begin{itemize}
  \item $\omega_i.\textit{ops}_j.\textit{iter}: \mathbb{Y}$ (name of a control variable; no two ops in $\omega$ can have the same \textit{iter})
  \item $\omega_i.\textit{ops}_j.\textit{offset}: \mathbb{N}$
  \item $\omega_i.\textit{ops}_j.\textit{box}: \mathbb{N}$
  \item $\omega_i.\textit{ops}_j.\textit{tileCount}: \mathbb{N}$
\end{itemize}
The \myKeyA{output collective type} of $\omega$ is a collective type $\delta = (\omega.D, \omega.B)$ defined by
\begin{itemize}
  \item $\omega.D_i = \omega_i.n$
  \item $\omega.B_i$ is $\omega_i.n$ if $\omega_i.\textit{ops}$ is empty, otherwise it is the $\textit{box}$ of the last op.
  \item (recall dimension thread pitch values $\omega.P_i$ are implicit, as in def~\ref{sec:gThreadPitch}).
\end{itemize}
The \myKeyA{thread mapping function} selects, for each collective dimension descriptor $\omega_i$, an interval of size $\omega.B_i$ based on an affine transform of the values of the iterators associated with that dimension.
These intervals together select a sub-grid of the $M$-dimensional space of threads-in-cluster.

We derive the thread mapping from $\omega$ based on the function
\begin{align*}
    & \textsf{collMap}: \Omega \to \Sigma_c \to \mathcal{P}(\mathbb{N}) \\
    & (\omega_0, ..., \omega_{M-1}) \mapsto \sigma_c \mapsto \{ \textsf{toLocal}(\omega.D, c) \mid
      c \in [x_0, x_0 + \omega.B_0 - 1]_\mathbb{N} \times ... \times
            [x_{M-1}, x_{M-1} + \omega.B_{M-1} - 1]_\mathbb{N} \} \\
    & \text{ with } x_i = \sum_{\textit{op} \in \omega_i.\textit{ops}} \textit{op}.\textit{offset} + \sigma_c(\textit{op}.\textit{iter}) \textit{op}.\textit{box}
\end{align*}
where \textsf{toLocal} is def~\ref{sec:gToLocal} and the actual thread mapping function is $\textsf{collMap}(\omega)$ (partial evaluation).

For a given $y: \mathbb{Y}$, let $\textit{op}_y = \omega_i.\textit{ops}_j$ such that $y = \textit{op}_y.\textit{iter}$, if it exists.
The \myKeyA{thread pitch} and \myKeyA{tiled dimension index} of $y: \mathbb{Y}$ as defined by $\omega$ is
\begin{itemize}
  \item 0 and $i$, if $\textit{op}_y$ exists and $\textit{op}_y.\textit{tileCount} \le 1$
  \item $(\omega.P_i)(\textit{op}_y.\textit{box})$ and $i$, if $\textit{op}_y$ exists and $\textit{op}_y.\textit{tileCount} > 1$
  \item 0 and $\bot$ if $\textit{op}_y$ does not exist.
\end{itemize}

\subsection{Collective Tiling Reshape \& Domain Completion}
\label{sec:CollTilingReshape}

Similar to collective types (Section~\ref{sec:CollTypeReshape}), collective tilings may be reshaped by splitting dimensions.
We split the $k^{th}$ dimension of $\omega$ by a factor $f: \mathbb{N}$ by replacing the collective dimension descriptor $\omega_k$ with a pair $(\omega_\text{hi}, \omega_\text{lo})$ defined by
\begin{itemize}
  \item $\omega_\text{hi}.n = \omega_k.n / f$
  \item $\omega_\text{hi}.\textit{ops}$ is all $\textit{op} \in \omega_k.\textit{ops}$ with $\textit{op}.\textit{box} \ge f$, modified by dividing both $\textit{op}.\textit{box}$ and $\textit{op}.\textit{offset}$ by $f$. This fails if any division gives a non-integer.
  \item $\omega_\text{lo}.n = f$
  \item $\omega_\text{lo}.\textit{ops}$ is all $\textit{op} \in \omega_k.\textit{ops}$ with $\textit{op}.\textit{box} < f$, unmodified.
\end{itemize}

This mirrors Section~\ref{sec:CollTypeReshape}, in that this results in the single domain coordinate $\omega.D_k$ being replaced with $\omega.D_k / f$ and $f$.
We use the function $\textsf{domainCompletion}: \Omega \times \Delta \to \Omega \times \Delta$ (def~\ref{sec:gDomainCompletion}) to reshape a collective tiling and a collective type so that they have the same domain.

\subsection{Collective Type Matching}
\label{sec:CollTilingCollTypeMatching}

Given a collective type $\delta_0$ and collective tiling $\omega_0$, we can query if $\omega_0$ \myKeyA{matches} $\delta_0$.
Let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
Then the query returns true if domain completion succeeded and $\omega.B = \delta.B$.
This certifies that statements with collective tiling $\omega_0$ are at $\delta_0$-scope (def~\ref{sec:gCollType}).

\subsection{Derived Collective Tilings}
\label{sec:DerivedCollTiling}

The collective tiling of each statement is the same as that of its parent, except that \lighttt{with CudaDeviceFunction}, \lighttt{for cuda\_threads}, and \lighttt{with CudaWarps} assign a new collective tiling for their children.
The latter two are defined based on the \textsf{deriveCollTiling} function (TODO define); here we just summarize the collective tilings.

\subsection{CudaDeviceFunction}
\label{sec:CollTilingCudaDeviceFunction}

If \lighttt{clusterDim = 1}, then $\omega$ is one-dimensional, with $\omega_0.n = \lighttt{blockDim}$.
Otherwise, $\omega$ is two-dimensional, with $\omega_0.n = \lighttt{clusterDim}$ and $\omega_1.n = \lighttt{blockDim}$.
In both cases, $\omega_*.\textit{ops}$ is empty.

\subsection{cuda\_threads Loops}
\label{sec:CollTilingCudaThreads}

A \lighttt{cuda\_threads} loop must be in the form \lighttt{for $y$ in cuda\_threads(0, $c_\text{hi}$, unit=$\tau_u$)}, with $c_\text{hi}$ a positive constant integer.
Let $\omega_\text{raw}$ be the collective tiling of the loop statement, and let $\delta_\text{raw}$ be the collective type unpacked from $\tau_u$ without alignment and without 1-padding (def~\ref{sec:gCollUnit}).
Let $(\omega, \delta) = \textsf{domainCompletion}(\omega_\text{raw}, \delta_\text{raw})$ (def~\ref{sec:gDomainCompletion}) so that $\omega$ and $\delta$ have the same domain.

The tiled dimension index $k$ is the value such that $\delta.B_k \notin \{ \top, \omega.B_k, \omega.D_k \}$.
If no such $k$ exists, then the collective tiling of the child statements is $\omega$, and $c_\text{hi}$ must be 1 (trivial tiling).

If multiple such $k$ exist, then the loop is ill-formed (ambiguous tiling).

If $k$ exists uniquely, then we must have $c_\text{hi} \delta.B_k \le \omega.B_k$, otherwise the loop is ill-formed (not enough threads).
In this case, the collective tiling $\omega'$ of the child statements is like $\omega$, but with a new $\textit{op}: \mathcal{O}$ appended to $\omega'_k.\textit{ops}$, with
\begin{itemize}
  \item $\textit{op}.\textit{iter} = y$
  \item $\textit{op}.\textit{offset} = 0$
  \item $\textit{op}.\textit{box} = \delta.B_k$
  \item $\textit{op}.\textit{tileCount} = c_\text{hi}$
\end{itemize}
so that the output collective type of $\omega'$ is the same as that of $\omega$, except that $\omega'.B_k = \delta.B_k$.
If all other box coordinates of $\omega'$ already match those of $\delta$, then the output collective type of $\omega'$ is $\delta$.
This is commonly the case, but we designed this to allow for mismatches on dimensions other than $k$ to make it easier to place CTA-in-cluster loops inside thread-in-CTA loops or \lighttt{with CudaWarps} blocks.

\subsection{CudaWarps}
\label{sec:CollTilingCudaWarps}

% Pants-on-fire simplified explanation for the end user.
We describe this somewhat informally in terms of the \lighttt{cuda\_threads} loop behavior.
A \lighttt{with CudaWarps(lo, hi, name=...)} statement has the following defaults:
\begin{itemize}
  \item \lighttt{lo = 0} if not given.
  \item \lighttt{hi}, if not given, is the number of warps of the warp variable named.
  \item \lighttt{name} is \lighttt{""} if this is a top-level case (see below), or the same as the parent \lighttt{with CudaWarps} statement if this is a nested case.
\end{itemize}

\lighttt{with CudaWarps} statements that appear in CUDA device functions with at least two warp variables and with no other \lighttt{with CudaWarps} as a direct or indirect parent are a top-level case.
A top-level case must appear in \lighttt{cuda\_agnostic\_intact\_cta}-scope (def~\ref{sec:gCollUnit}).
The \lighttt{true\_lo} and \lighttt{true\_hi} of the statement are \lighttt{lo + p} and \lighttt{hi + p}, \lighttt{p} being the prefix of the warp variable (def~\ref{sec:gWarpVariable}).
All other cases are nested cases, which have \lighttt{true\_lo} and \lighttt{true\_hi} being the same as \lighttt{lo} and \lighttt{hi}.

The \lighttt{with CudaWarps} statement defines the collective tiling $\omega'$ of its child statements in a similar manner as \lighttt{for \_ in cuda\_threads(0, true\_hi, unit=cuda\_warp)}, except that the threads that would have executed iterations \lighttt{true\_lo} through \lighttt{true\_hi - 1} instead cooperate to execute the statement body.
The \lighttt{with CudaWarps} statement defines a dummy iterator variable $y$ and adds a new collective dimension operator to $\omega'$ for $y$ (except in case of a trivial tiling).

\subsection{Collective Tiling Figure}
\label{sec:CollTilingFigure}

We will illustrate the collective tiling that annotates the inner-most statement of the following example proc.
The illustration is on a separate page.

\filbreak
\input{b_samples/for_CollTiling_figure.0.tex}

where we note that
\begin{itemize}
  \item The \texttt{\violetBox{n\_cta}} loop has a collective unit (def~\ref{sec:gCollUnit}) of \lighttt{4 * cuda\_cta\_in\_cluster\_strided(2)}, indicating that each iteration is executed cooperatively by a thread collective comprising 4 CTAs, with \lighttt{cluster\_ctarank} of the CTAs in the thread collective increasing by 2's.
  \item All \lighttt{unit} parameters are documented (def~\ref{sec:gCollUnit}).
  \item The \lighttt{CudaWarps} statement has the effect of deactivating the 0th warpgroup (def~\ref{sec:gWarpgroup}) of a CTA.
    The Exo-GPU compiler generates a hidden \lighttt{CudaWarps\_consumer\_None\_None} variable associated with this statement.
\end{itemize}

\filbreak
{
\sffamily
\begin{tikzpicture}[node distance=0mm]
\input{b_CollTiling_autogen.tex}

\node(keyText) [anchor=south west, yshift=60mm, xshift=-5mm] at(cta1.north west) {\textbf{KEY:}};
\node(keyThread) [CollTilingExampleStyle, anchor=west] at(keyText.east) {tid\\$c_0$\\$c_1$\\$c_2$};
\node(keyTid) [anchor=west, text width=40mm] at(keyThread.east) {where ``tid'' is the local thread index given by toLocal($\omega.D, (c_0, c_1, c_2)$) (def~\ref{sec:gLocalThreadIndex},~def~\ref{sec:gToLocal}).};
\node(keyCta) [anchor=north west, yellowstyle, yshift=-2mm] at(keyThread.south west) {\texttt{cluster\_ctarank} (def~\ref{sec:gCluster})$\rightarrow$};

\node(ops_2_2_value) [anchor=south east, bluestyle, text width=80mm, yshift=+35mm] at(t_0_0_383.north east)  {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=128, \textit{iter}=\texttt{t}$};
\node(ops_2_1_value) [anchor=south east, bluestyle, text width=80mm, yshift=+1mm] at(ops_2_2_value.north east)  {\rmfamily $\textit{offset}=0$, $\textit{box}=128$, $\textit{tileCount}=2, \textit{iter}=\texttt{wg}$};
\node(ops_2_0_value) [anchor=south east, bluestyle, text width=80mm, yshift=+1mm] at(ops_2_1_value.north east)  {\rmfamily $\textit{offset}=128$, $\textit{box}=256$, $\textit{tileCount}=1$,\\$\textit{iter}=\texttt{CudaWarps\_consumer\_None\_None}$};
\node(ops_1_0_value) [anchor=south east, violetstyle, text width=80mm, yshift=+1mm] at(ops_2_0_value.north east) {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=2$, $\textit{iter}=\texttt{n\_cta}$};
\node(ops_0_0_value) [anchor=south east, greenstyle, text width=80mm, yshift=+1mm] at(ops_1_0_value.north east) {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=4$, $\textit{iter}=\texttt{m\_cta}$};

\node(ops_2_2_label) [anchor=east] at(ops_2_2_value.west) {\rmfamily $\omega_2.\textit{ops}_2$};
\node(ops_2_1_label) [anchor=east] at(ops_2_1_value.west) {\rmfamily $\omega_2.\textit{ops}_1$};
\node(ops_2_0_label) [anchor=east] at(ops_2_0_value.west) {\rmfamily $\omega_2.\textit{ops}_0$};
\node(ops_1_0_label) [anchor=east] at(ops_1_0_value.west) {\rmfamily $\omega_1.\textit{ops}_0$};
\node(ops_0_0_label) [anchor=east] at(ops_0_0_value.west) {\rmfamily $\omega_0.\textit{ops}_0$};

\node(dim2) [anchor=north west, xshift=-50mm] at(ops_2_0_value.north west) {\textbf{Dim 2:} $\omega_2.n = 384$};
\node(dim1) [anchor=north west, xshift=-50mm] at(ops_1_0_value.north west) {\textbf{Dim 1:} $\omega_1.n = 2$};
\node(dim0) [anchor=north west, xshift=-50mm] at(ops_0_0_value.north west) {\textbf{Dim 0:} $\omega_0.n = 4$};
\node(CollTiling) [draw=black, anchor=east, text width=20mm] at(dim1.west) {CollTiling\\$\omega: \Omega$ state};

\node(domain) [anchor=north east, yshift=-2mm, text width=50mm] at(dim2.south east) {The domain $\omega.D = (4, 2, 384)$ is given by ($\omega_0.n$, $\omega_1.n$, $\omega_2.n$) (def~\ref{sec:gDomain}).};

\end{tikzpicture}
}

\FloatBarrier
\newpage
\section{Distributed Memory}
\label{sec:DistributedMemory}

Both barrier and data allocations in CUDA scope are subject to distributed memory analysis, although the rules are somewhat different for the two.
For barrier allocations, all dimensions are distributed.
For data allocations, the memory type specifies a collective unit (the \myKeyA{native unit}) from which a collective type $\delta_0$ is unpacked with alignment and 1-padding (def~\ref{sec:gCollUnit}); this must not be agnostic (def~\ref{sec:gAgnostic}).
Exo-GPU then deduces a certain number of distributed dimensions (say, $R$) so that each slice $x[c_0,...,c_{R-1},:,...,:]$ is allocated on a different thread collective described by $\delta_0$ (Section~\ref{sec:CollTypeThreadCollective}).
The distributed dimensions are always to the left of non-distributed dimensions.
During codegen, Exo-GPU erases indicies and array extents corresponding to distributed dimensions.
The codegen functions for memory and instructions only see indices and extents corresponding to non-distributed dimensions.

\subsection{Collective Indexing Pairs}
\label{sec:CollIndexingPairs}

Distributed memory analysis for a variable is based on the collective tiling $\omega_0^\text{alloc}$ (Section~\ref{sec:CollTiling}) of the variable allocation statement, and the set of \myKeyA{collective indexing pairs} $\Omega \times \mathsf{Expr}^*$ collected from all statements and expressions that index the variable:
\begin{itemize}
  \item For read expressions ``$x[e^*]$'' not in the context of an instruction call, the collective indexing pair is $(\omega, e^*)$, where $\omega$ is the collective tiling of the statement containing the read expression.
  \item For write statements ``$x[e^*] \texttt{= \_}$'' and reduce statements ``$x[e^*] \texttt{+= \_}$'', the collective indexing pair is $(\omega, e^*)$, where $\omega$ is the collective tiling of the statement.
  \item TODO sync statement
  \item TODO describe instructions.
\end{itemize}

\subsection{Thread Pitch Requirement}
\label{sec:DistributedMemoryThreadPitch}

For each collective indexing pair, we deduce a thread pitch tuple.
The deduced number of distributed dimensions ($R$) is the length of this tuple.
All collective indexing pairs must lead to the same deduced tuple.

We will soon define what a \myKeyA{required iterator} is separately for data and barriers.
A \myKeyA{required index expression} is a plain read of a required iterator.
A \myKeyA{permitted index expression} is a plain read of a required iterator, or of a \lighttt{cuda\_threads} iterator with 0 thread pitch, as defined by $\omega$ (Section~\ref{sec:CollTilingState}).

\mainKey{Data:} A collective index pair $(\omega_0, e^*)$ is first updated with domain completion (def~\ref{sec:gDomainCompletion}) so that the memory type's collective type has the same domain as the collective tiling.
Let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.

Dimension $i$ is an \myKeyA{intact dimension} if $\delta.B_i = 1$.
An iterator is a \myKeyA{required iterator} if all these conditions apply:
\begin{itemize}
  \item appears in $\omega$
  \item does not appear in $\omega_0^\text{alloc}$
  \item its tiled dimension index is that of an intact dimension
  \item its thread pitch is not 0
\end{itemize}
with ``tiled dimension index'' and ``thread pitch'' as defined by $\omega$ (Section~\ref{sec:CollTilingState}).
The number of distributed dimensions $R$ is the lowest possible value $R$ such that $e_0, ..., e_{R-1}$ consists only of permitted index expressions and all required index expressions appear exactly once (fail if this is not possible).
The thread pitch tuple is $(g(e_0), ..., g(e_{R-1}))$ where $g(e)$ gives the thread pitch (as defined by $\omega$) of the iterator indexed by the permitted index expression $e$.

\mainKey{Barrier:} A collective index pair $(\omega, e^*)$ is processed without domain completion.
An iterator is a \myKeyA{required iterator} if all these conditions apply:
\begin{itemize}
  \item appears in $\omega$
  \item does not appear in $\omega_0^\text{alloc}$
  \item its thread pitch is not 0
\end{itemize}
with ``thread pitch'' as defined by $\omega$ (Section~\ref{sec:CollTilingState}).
The number of distributed dimensions $R$ is the number of index expressions $e^*$, which must consist only of permitted index expressions, and all required index expressions must appear exactly once (fail if this is not the case).
The thread pitch tuple is $(g(e_0), ..., g(e_{R-1}))$ where $g(e)$ gives the thread pitch (as defined by $\omega$) of the iterator indexed by the permitted index expression $e$.

\subsection{Base Threads Requirement}
\label{sec:DistributedMemoryBaseThreads}

The thread pitch tuple requirement will diagnose many cases of inconsistent sharding, but will not detect ``offset-only'' mismatches, such as $x[0], x[1], x[2]...$ being accessed by threads 0, 1, 2, ... in one usage and threads 64, 65, 66, ... in another usage (as could occur with warp specialization).
We further need to enforce that over the lifetime of a barrier array element $z[c^*]$, the same thread collective is used for all \lighttt{Arrive} statements (similarly for all \lighttt{Await} statements).

We will define the \myKeyA{base offset} and \myKeyA{base box} of a collective tiling separately for data and barrier variables.
For data variables and non-mbarrier barrier variables, all collective indexing pairs must have equivalent base offsets and base boxes.
For mbarrier-mechanism barrier variables\footnote{This case is distinguished with \lighttt{different\_arrive\_await\_threads=True}}, all collective indexing pairs taken from \lighttt{Arrive} statements must have equivalent base offsets and base boxes; the same requirement applies separately to all \lighttt{Await} statements.

\mainKey{Barrier:} Given an $M$-dimensional collective tiling $\omega$, the \myKeyA{base offset} is defined by the tuple $(O_0, ..., O_{M-1})$ where $O_i = \sum_{j} \omega_i.\textit{ops}_j.\textit{offset}$ (Section~\ref{sec:CollTilingState}).
The \myKeyA{base box} is $\omega.B$.

\mainKey{Data:} Given a collective tiling $\omega_0$ and the collective type $\delta_0$ unpacked from the native unit, let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
The \myKeyA{base offset} $O$ and the \myKeyA{base box} $\textit{BB}$ are both $M$ tuples, $M$ being the dimensionality of $\omega$.
The $i^{th}$ dimension is a \myKeyA{intact dimension} if $\delta.B_i = \delta.D_i$.
For intact dimensions $i$, $O_i = 0$ and $\textit{BB}_i = \delta.D_i$.
For other dimensions, $O_i = \sum_{j} \omega_i.\textit{ops}_j.\textit{offset}$ (Section~\ref{sec:CollTilingState}) and $\textit{BB}_i = \omega.B_i$.

\mainKey{Base Offset Equivalence:} Compute the linear offset of $\omega$ as $\sum_{i} O_i \omega.P_i$, $O$ denoting the base offset derived from $\omega$ and $\omega.P_i$ the dimension thread pitch (def~\ref{sec:gThreadPitch}).
Base offset equivalence means that the linear offset computed for two collective tilings are the same.

\mainKey{Base Box Equivalence:} Given collective tilings $\omega_1$ and $\omega_2$ with base boxes $\textit{BB}_1$ and $\textit{BB}_2$ respectively, the two collective tilings have equivalent base boxes when $\delta_1 = \delta_2$ given $(\delta_1, \delta_2) = \mathsf{domainCompletionTypeOnly}((\omega_1.D, \textit{BB}_1), (\omega_2.D, \textit{BB}_2))$ (def~\ref{sec:gDomainCompletionTypeOnly}).

\subsection{Box Size Requirements}
\label{sec:DistributedMemoryBoxSize}

These additional requirements for data allocations only ensure the storage for each shard is truly allocated by a thread collective described by $\delta_0$.

\mainKey{Alloc Box Requirement:} Let $(\omega^\text{alloc}, \delta) = \mathsf{domainCompletion}(\omega_0^\text{alloc}, \delta_0)$.
The $i^{th}$ dimension is an \myKeyA{intact dimension} if $\delta.B_i = \delta.D_i$.
For each intact dimension $i$, we must have $\omega^\text{alloc}.B_i = \delta.B_i$.

\mainKey{Usage Box Requirement:} For each collective indexing pair $(\omega_0, e^*)$, let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
The $i^{th}$ dimension is a \myKeyA{subdivided dimension} if $\delta.B_i = 1$.
Given $\delta_0$ is in aligned form (def~\ref{sec:gAlignedForm}) and not agnostic (def~\ref{sec:gAgnostic}), this is mutually exclusive with being an intact dimension.
For each subdivided dimension $i$, we must have $\omega.B_i = 1$.

\FloatBarrier
\newpage
\section{Synchronization Usage}
\label{sec:SyncUsage}

\subsection{Fundamentals}
\label{sec:SyncUsageFundamentals}

\subsection{Intended Usage Patterns -- Access Before Synchronization}
\label{sec:AccessBeforeSync}

\subsection{Intended Usage Patterns -- Access After Synchronization}
\label{sec:AccessAfterSync}

\subsection{Intended Usage Patterns -- Arrive-Await Pairing}
\label{sec:ArriveAwaitPairing}

\subsection{Intended Usage Patterns -- Multiple Memory Accesses}
\label{sec:MultipleMemoryAccesses}

\subsection{Cluster Sync Usage}
\label{sec:ClusterSyncUsage}

\subsection{Commit Group Usage}
\label{sec:CommitGroupUsage}

\subsection{Mbarrier Usage}
\label{sec:MbarrierUsage}

\subsection{Barrier Multicast}
\label{sec:BarrierMulticast}

% TODO thread pitch multiple of blockDim, multicast convergence

\subsection{Barrier Guarding Requirement}
\label{sec:BarrierGuarding}

\subsection{Solitary Barrier Requirement}
\label{sec:SolitaryBarrier}


\FloatBarrier
\newpage
\section{Synchronization Semantics}
\label{sec:SyncSemantics}

The synchronization semantics are defined by translating the Exo-GPU program into an abstract machine program, and interpreting the abstract machine program with respect to abstract machine semantics.
The program is synchronized correctly if no error condition is reached.
In the current implementation (with camspork), this can only be done for a given concrete problem size specified by \lighttt{exo.Procedure.sync\_check}.

Both existing Exo value semantics and new abstract machine semantics are defined sequentially.
The primary difference is that in the value semantics, a data array element holds a numerical value, while in abstract machine semantics, each data array element encodes the history of reads and mutates performed to it, with each access encoded as a \myKeyA{visibility record} (Section~\ref{sec:VisRecordState}).
Specifically, the state of an interpreted abstract machine program consists of
\begin{itemize}
  \item A synchronization environment $\rho$ (rho), Section~\ref{sec:SyncEnv}.
  \item A current task ID $\iota: \mathbb{I}$ (iota), Section~\ref{sec:SyncSemanticsThreadMapping}
  \item A control environment $\sigma: \Sigma$ (sigma), with $\Sigma \Coloneqq \mathbb{Y} \to \mathbb{Z}$ (def~\ref{sec:gControlEnv}).
\end{itemize}

When a \myKeyA{non-sync-exempt} (def~\ref{sec:gSyncExempt}) array element is read or mutated, a new visibility record (\textsf{VisRecord}) gets added to the synchronization environment for that element (Section~\ref{sec:VisRecordCreation}).
Each interpreted \lighttt{Fence} (Section~\ref{sec:FenceSemantics}), \lighttt{Arrive} (Section~\ref{sec:ArriveSemantics}), and \lighttt{Await} statement (Section~\ref{sec:AwaitSemantics}) conditionally modifies all visibility records in the \emph{entire environment}, via lifting (Section~\ref{sec:Lifting}) and augment (Section~\ref{sec:Augment}).
\lighttt{Arrive} and \lighttt{Await} statements may also modify the arrive and await count.
Finally, the abstract machine specified correctness checks for each read to (Section~\ref{sec:ChecksOnRead}), mutate of (Section~\ref{sec:ChecksOnMutate}), or free of (Section~\ref{sec:ChecksOnFree}) an array element; these checks are a function of the synchronization environment state for said array element.

For this informal documentation, we will skip the translation step and summarize the effects of relevant Exo-GPU statements upon the above state directly.

\subsection{Control Environment}
\label{sec:ControlEnv}

Function $\sigma: \mathbb{Y} \to \mathbb{Z}$, mapping control variable names (def~\ref{sec:gMathbbY}) to integer values.
This is common to both Exo value semantics, and Exo-GPU abstract machine semantics.

\subsection{Thread Mapping}
\label{sec:SyncSemanticsThreadMapping}

The thread mapping function for a given statement $s$ is denoted, in the PLDI submission, as $g_{s^\#}: \Sigma \to \mathbb{I} \to \mathcal{P}(\mathbb{G})$ where
\begin{itemize}
  \item $s^\#$ denotes $s$ translated to abstract machine IR, which this documentation skips over.
  \item $\Sigma$ is the set of control environments (Section~\ref{sec:ControlEnv}).
  \item $\mathbb{I}$ is the set of task IDs; each executed instance of a device task (def~\ref{sec:gDeviceTask}) defines a new current task ID.
  This abstracts over cluster IDs (def~\ref{sec:gCluster}), since the mapping between tasks and clusters is not statically encoded.
  \item $\mathbb{G} \Coloneqq \mathbb{I} \times \mathbb{N}$ is the set of global thread IDs (def~\ref{sec:gGlobalThreadID}); each is a pair of task ID and local thread index (def~\ref{sec:gLocalThreadIndex}).
  In the context of the abstract machine, a set of $\mathbb{G}$ is a \myKeyA{thread collective} (def~\ref{sec:gThreadCollective}).
\end{itemize}

For each \myKeyA{CPU scope} statement (def~\ref{sec:gCpuScope}), the thread mapping function always gives $\mathbb{G}$.

For each \myKeyA{CUDA scope} statement (def~\ref{sec:gCudaScope}), the thread mapping function is
\begin{equation*}
    g_{s^\#}(\sigma, \iota) = \{ (\iota, n) \mid n \in \textsf{collMap}(\omega_s, \sigma) \}
\end{equation*}
where $\omega_s$ is the collective tiling (Section~\ref{sec:CollTiling}) that collective analysis (def~\ref{sec:gCollAnalysis}) annotated the statement $s$ with, and $\mathsf{collMap}$ is as defined in Section~\ref{sec:CollTilingState}.

\mainKey{Implementation Note:} This is not at all how camspork works (i.e. we don't actually materialize the thread mapping function).
For the real implementation, we still encode the loop mode (def~\ref{sec:gLoopMode}) for each loop and store the current thread collective as a thread cuboid (def~\ref{sec:gThreadCuboid}), which is modified by parallel loops.


\subsection{Synchronization Environment (SyncEnv)}
\label{sec:SyncEnv}

A synchronization environment $\rho$ is a function of type
\begin{equation*}
    \rho: \mathbb{X} \cup \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathsf{VisRecord}) \times \mathcal{P}(\mathsf{VisRecord}) \times \mathbb{N}^2
\end{equation*}
which maps an index into a data variable $x: \mathbb{X}$ (def~\ref{sec:gMathbbX}) or a barrier variable $z: \mathbb{B}$ (def~\ref{sec:gMathbbB}) to a 3-tuple of
\begin{itemize}
  \item set of read visibility records, accessed with $.r$
  \item set of mutate visibility records, accessed with $.m$
  \item pair of arrive count and await count, accessed together as $.b$ and separately as $.b.a$ and $.b.w$ respectively.
\end{itemize}

% Referenced in glossary for barrier env
\mainKey{Implementation Note:} the arrive and await count attributes are separated out into a separate barrier environment in camspork, i.e., the synchronization environment is implemented as $\mathbb{X} \cup \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathsf{VisRecord}) \times \mathcal{P}(\mathsf{VisRecord})$ only, and the barrier environment is $\mathbb{B} \to \mathbb{Z}^* \to \mathbb{N}^2$.

\subsection{VisRecord State}
\label{sec:VisRecordState}

Each visibility record $r$ is a pair of type $\mathcal{P}(\mathsf{TlSig}) \times \mathcal{P}(\mathbb{P})$, with the two elements being
\begin{itemize}
  \item The \myKeyA{visibility set} (def~\ref{sec:gVisSet}), denoted $r.s$, which is a set of timeline signatures $\mathsf{TlSig} \Coloneqq \mathbb{G} \times \mathsf{QualTL} \times \mathsf{VF}$; these being a global thread ID (def~\ref{sec:gGlobalThreadID}), a qualitative timeline (def~\ref{sec:gQualTL}), and a \myKeyA{visibility flag} (def~\ref{sec:gVisFlag}), which is one of $\mathsf{VF_{atom}}$ (atomic-only), $\mathsf{VF_{temp}}$ (temporal), $\mathsf{VF_{full}}$, or $\mathsf{VF_{issue}}$.
  The visibility set is conditionally augmented (Section~\ref{sec:Augment}) with more timeline signatures as a result of \lighttt{Fence} and \lighttt{Await} statements.
  \item The \myKeyA{pending await} set (def~\ref{sec:gPendingAwait}), denoted $r.p$, which are tuples of type $\mathbb{B} \times \mathbb{Z}^* \times \mathbb{Z}$, this being a combination of an indexed barrier variable (def~\ref{sec:gMathbbB}) and an arrive-count.
  These mediate the interaction between paired \lighttt{Arrive} statement instances (Section~\ref{sec:ArriveSemantics}) and \lighttt{Await} statement instances (Section~\ref{sec:AwaitSemantics}).
\end{itemize}

This is a different formulation than the PLDI paper; see the reference section (def~\ref{sec:gVisSet}, def~\ref{sec:gPendingAwait}) for more information.

The sync-check error messages format a visibility record's visibility set and pending await set separately.
An integer is used to represent the task ID, counting up starting from 0 for each device task instance (def~\ref{sec:gDeviceTask}) in each CUDA device function launch.

The visibility set is encoded as the union of zero or more ``timeline signature intervals'', each formatted as the lines \\
``\texttt{threads: [task\_index = $\iota_0$ [$c_0^*$],}\\
\texttt{task\_index = $\iota_1$ [$c_1^*$]], inclusive, formatted w/ domain [$D$]}''\\
followed by lines of the form\\
``\texttt{$q_0$ -> $\mathsf{vf}_0^*$}\\
\hphantom{``}\texttt{$q_1$ -> $\mathsf{vf}_1^*$}\\
...''\\
where each $q_i$ is a qualitative timeline (def~\ref{sec:gQualTL}) and each $\mathsf{vf}_i^*$ is a set of visibility flags (def~\ref{sec:gVisFlag}).
The timeline signature interval is the set of all $((\iota, n), q, \mathsf{vf})$ such that
\begin{itemize}
  \item $(\iota_0, \mathsf{toLocal}(D, c_0^*)) \le (\iota, n) \le (\iota_1, \mathsf{toLocal}(D, c_1^*))$, sorted lexicographically (def~\ref{sec:gToLocal}), and
  \item $0 \le n < \prod_m D_m$, and,
  \item $q = q_i$ for some $q_i$, with $\mathsf{vf} \in \mathsf{vf}_i^*$
\end{itemize}
Each pending await $(z, n^*, a)$ in the pending await set is formatted as \\
``\texttt{pending await: $z$[$n^*$] arrive\_count=$a$}''.

\subsection{VisRecord Creation}
\label{sec:VisRecordCreation}

Each read, mutate, \lighttt{Arrive}, or \lighttt{Await} statement that references an array element $x[n^*]$ or $z[n^*]$ in non-sync-exempt (def~\ref{sec:gSyncExempt}) memory causes a new set of visibility records $r^*$ (to be defined) to be associated with $\rho(x, n^*)$ or $\rho(z, n^*)$.
For brevity we will omit the $z$ case in the following description.

A read, \lighttt{Arrive}, or \lighttt{Await} access to an array element $x[n^*]$ causes $r^*$ to be added to the read visibility records set for that array element, i.e. $\rho$ is updated (def~\ref{sec:gUpdateNotation}) as
\begin{equation*}
    \rho[x \to n^* \to (\rho(x, n^*).r \cup r^*, \rho(x, n^*).m, \rho(x, n^*).b)]
\end{equation*}
This is done subsequent to the to-be-defined checks in Section~\ref{sec:ChecksOnRead}.

A mutate access to an array element $x[n^*]$ causes the read visibility records set to be cleared, the mutate visibility records set to be cleared if the access is not atomic (Section~\ref{sec:AtomicInstr}), then $r^*$ is added to the mutate visibility records set, i.e. $\rho$ is updated as
\begin{align*}
    & \rho[x \to n^* \to (\{\}, r^*, \rho(x, n^*).b)] & \text{if not atomic} \\
    & \rho[x \to n^* \to (\{\}, r^* \cup \rho(x, n^*).m, \rho(x, n^*).b)] & \text{if atomic}
\end{align*}
This is done subsequent to the to-be-defined checks in Section~\ref{sec:ChecksOnMutate}.

The new visibility records $r^*$ are initialized based on
\begin{itemize}
  \item the thread collective $g_{s^\#}(\iota, \sigma)$ (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item the convergence of the memory access (Section~\ref{sec:InstrConvergentAccess})
  \item the out-of-order flag (TODO)
  \item the initial qualitative timeline $q$ (Section~\ref{sec:InstrTL}) of the access.
  \item the set of barrier array elements referenced by the trailing barrier expression (Section~\ref{sec:InstrTrailingBarrierExpr}); this is empty for non-instr accesses or instrs without a trailing barrier expression.
\end{itemize}

Define $\mathsf{NewVisRecord}(g^*)$ as having a visibility set that is the union of
\begin{itemize}
    \item $g^* \times \{ q \} \times \{ \mathsf{VF_{issue}} \}$
    \item $g^* \times \{ q \} \times \{ \mathsf{VF_{atom}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}} \}$, only added if the access is \emph{not} out-of-order (TODO)
    \item $\mathbb{G} \times \{ q_a \} \times \{ \mathsf{VF_{atom}} \}$, only added if the access defines an atomic qualitative timeline $q_a$ (Section~\ref{sec:AtomicInstr}).
\end{itemize}
and a pending await set that is based off the arrive count (Section~\ref{sec:SyncEnv}) of each referenced barrier array element:
\begin{equation*}
    \{ (z, n^*, \rho(z, n^*).b.a) \mid \text{ for each $z[n^*]$ referenced (def~\ref{sec:gBarrierExpr}) in the trailing barrier expression} \}
\end{equation*}

If the access is convergent (Section~\ref{sec:InstrConvergentAccess}), $r^*$ contains a single visibility record:
\begin{equation*}
    r^* = \{ \mathsf{NewVisRecord}(g_{s^\#}(\iota, \sigma)) \}
\end{equation*}
Otherwise, $r^*$ contains one visibility record for each thread involved in the statement instance:
\begin{equation*}
    r^* = \{ \mathsf{NewVisRecord}(\{g\}) \mid g \in g_{s^\#}(\iota, \sigma) \}
\end{equation*}
However, for performance reasons, if the non-convergent access is out-of-order, the set of threads may be expanded during sync-check due to non-convergent out-of-order abstract machine optimization (def~\ref{sec:gOooOpt}).

Note that for single-threaded accesses, the convergence makes no difference to the result (setting aside said optimization).

\subsection{Lifting}
\label{sec:Lifting}

The semantics for synchronization statements entail mapping over each visibility record (Section~\ref{sec:VisRecordState}) in the entire synchronization environment (Section~\ref{sec:SyncEnv}).
This is done by applying the function $\mathsf{lift}$ to the synchronization environment, which takes $\lambda: \mathsf{VisRecord} \to \mathsf{VisRecord}$.
$\rho' = \mathsf{lift}(\rho, \lambda)$ is defined by
\begin{equation*}
    \rho'(x, n^*) = \{ \lambda(r) \mid r \in \rho(x, n^*).r \}, \{ \lambda(r) \mid r \in \rho(x, n^*).m \}, \rho(x, n^*).b
\end{equation*}

\subsection{VisRecord Synchronizes-with (Witness)}
\label{sec:Witness}

Given a first synchronization timeline $\tau_s^\mathrm{pre}$ (def~\ref{sec:gSyncTL}) and a thread collective $g^*$ mapped to execute (Section~\ref{sec:SyncSemanticsThreadMapping}) a certain \lighttt{Fence} or \lighttt{Arrive} statement instance, the statement instance ``witnesses'', or ``synchronizes-with'' a visibility record (Section~\ref{sec:VisRecordState}) when its visibility set contains a timeline signature (def~\ref{sec:gTlSig}) of the form $(g, q, \mathsf{VF_{issue}})$ with $g \in g^*$ and $q \in \tau_s^\mathrm{pre}.\mathrm{full}$.

The witness function $\mathcal{W}: \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisRecord} \to \mathsf{Bool}$ is defined as
\begin{equation*}
  \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) = \exists (g, q, v) \in r.s \text{ such that } g \in g^* \land q \in \tau_s^\mathrm{pre}.\mathrm{full} \land v \in \{ \mathsf{VF_{issue}}, \mathsf{VF_{full}} \}
\end{equation*}
The transitivity flag from the PLDI submission is removed. TODO is $\mathsf{VF_{full}}$ needed here?

\subsection{VisRecord Augment}
\label{sec:Augment}

Given a second synchronization timeline $\tau_s^\mathrm{post}$ (def~\ref{sec:gSyncTL}) and a thread collective $g^*$ mapped to execute (Section~\ref{sec:SyncSemanticsThreadMapping}) a certain \lighttt{Fence} or \lighttt{Await} statement instance, the statement instance ``augments'' a visibility record $r$ (Section~\ref{sec:VisRecordState}) by adding the following sets of timeline signatures (def~\ref{sec:gTlSig}) to the visibility set $r.s$:
\begin{align*}
    & \mathcal{A}_\mathrm{full} \Coloneqq g^* \times \tau_s^\mathrm{post}.\mathrm{full} \times \{ \mathsf{VF_{atom}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}}, \mathsf{VF_{issue}} \} \\
    & \mathcal{A}_\mathrm{temp} \Coloneqq g^* \times \tau_s^\mathrm{post}.\mathrm{temp} \times \{ \mathsf{VF_{temp}} \}
\end{align*}
The augment function $\mathcal{A}: \mathsf{VisRecord} \to \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisRecord}$ is defined based on the above as follows:
\begin{equation*}
    \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) = (r.s\cup \mathcal{A}_\mathrm{full} \cup \mathcal{A}_\mathrm{temp}, r.p)
\end{equation*}

\subsection{Fence Semantics}
\label{sec:FenceSemantics}

A \lighttt{Fence} statement augments (Section~\ref{sec:Augment}) all visibility records (Section~\ref{sec:VisRecordState}) that it witnesses (Section~\ref{sec:Witness}).
Specifically, for a given statement \texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}, with $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping}), define $\lambda_\mathrm{Fence}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Fence}(r) = \begin{cases}
        \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) & \text{ if } \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment is $\mathsf{lift}(\rho, \lambda_\mathrm{Fence})$ (Section~\ref{sec:Lifting}).

\subsection{Arrive Semantics}
\label{sec:ArriveSemantics}

A \lighttt{Arrive} statement adds pending await(s) (def~\ref{sec:gPendingAwait}) to all visibility records (Section~\ref{sec:VisRecordState}) that it witnesses (Section~\ref{sec:Witness}), and increments the arrive count of a barrier array element.

For a given statement \texttt{Arrive($\tau_s^\mathrm{post}$) >> $e^*$}, with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $z[n_h^*]$ being the home barrier of $e^*$ (def~\ref{sec:gHomeBarrier})
  \item $a$ being the arrive count of the home barrier (i.e. $\rho(z, n_h^*).b.a$)
\end{itemize}
define $\lambda_\mathrm{Arrive}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Arrive}(r) = \begin{cases}
        (r.s, r.p \cup \{ (z, n^*, a) \text{ for each $z[n^*]$ referenced by any barrier expr.} \})
        & \text{ if } \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment $\rho''$ is defined from $\rho$ (Section~\ref{sec:SyncEnv}) by
\begin{align*}
  & \rho' = \mathsf{lift}(\rho, \lambda_\mathrm{Arrive}) & \text{(Section~\ref{sec:Lifting})} \\
  & \rho'' = \rho'[z \to n_h^* \to (r.r, r.m, r.b.a + 1, r.b.w)] \text{ with } r = \rho'(z, n_h^*) & \text{(def~\ref{sec:gUpdateNotation})}
\end{align*}
The way we use and update only the arrive count of the home barrier (as a stand-in for the state of all barriers) is the reason for the barrier multicast convergence requirement (Section~\ref{sec:BarrierMulticast}).

\subsection{Await Semantics}
\label{sec:AwaitSemantics}

An \lighttt{Await} statement augments (Section~\ref{sec:Augment}) all visibility records (Section~\ref{sec:VisRecordState}) that contain the needed pending await (def~\ref{sec:gPendingAwait}), and updates the await count of the barrier array element referenced by the statement's barrier expression.

For a given statement \texttt{Await}($e, \tau_s^\mathrm{post}, n$) with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $z[n^*]$ being the barrier array element referenced by $e$ (def~\ref{sec:gBarrierExpr})
  \item $a$ being the arrive count of the referenced barrier array element (i.e. $\rho(z, n^*).b.a$)
  \item $w$ being the await count of the referenced barrier array element (i.e. $\rho(z, n^*).b.w$)
\end{itemize}
the behavior differs based on whether $n \ge 0$ (arrive-indexed case) or $n < 0$ (await-indexed case).

If $n \ge 0$ (arrive-indexed case), then define
\begin{align*}
    & a_\mathrm{max} = a - n - 1 \\
    & w' = \mathrm{max}(w, a_\mathrm{max} + 1)
\end{align*}
If $n < 0$ (await-indexed case), then define
\begin{align*}
    & \mathrm{lag} = -1 - n \\
    & a_\mathrm{max} = w - \mathrm{lag} \\
    & w' = w + 1
\end{align*}
Define $\lambda_\mathrm{Await}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Await}(r) = \begin{cases}
        \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) & \text{ if } \exists (z', n'^*, a') \in r.p \text{ with } z = z' \land n^* = n'^* \land a' \le a_\mathrm{max} \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment $\rho''$ is defined from $\rho$ (Section~\ref{sec:SyncEnv}) by
\begin{align*}
  & \rho' = \mathsf{lift}(\rho, \lambda_\mathrm{Await}) & \text{(Section~\ref{sec:Lifting})} \\
  & \rho'' = \rho'[z \to n^* \to (r.r, r.m, r.b.a, w')] \text{ with } r = \rho'(z, n^*) & \text{(def~\ref{sec:gUpdateNotation})}
\end{align*}

\mainKey{Implementation Note:} camspork removes old pending awaits to avoid infinite state growth.

\subsection{Check VisRecord Helper}
\label{sec:CheckVisRecordHelper}

For the following checks, we define the helper \\
$\mathsf{CheckVisRecord}: \mathsf{Bool} \to \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisFlag} \to \mathsf{VisRecord} \to \mathsf{Bool}$ as
\begin{equation*}
    \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{vf}, r) = \begin{cases}
        \exists g \in g^*, \exists q \in q^*, (g, q, \mathsf{vf}) \in r.s & \text{ if $t$ is true} \\
        \forall g \in g^*, \exists q \in q^*, (g, q, \mathsf{vf}) \in r.s & \text{ if $t$ is false} \\
    \end{cases}
\end{equation*}
In this context $t$ is the \myKeyA{convergence flag}; note it only changes the qualifier $\forall$ or $\exists$ for $g \in g^*$, which has no effect when $g^*$ consists only of a single global thread ID (def~\ref{sec:gGlobalThreadID}).

\subsection{Checks on Read, Arrive, Await}
\label{sec:ChecksOnRead}

Read-like accesses only impose requirements on the mutate visibility record set.

For each array element $x[n^*]$ referenced by a read within a statement, an \lighttt{Arrive} statement, or an \lighttt{Await} statement,
\begin{itemize}
  \item let $g^* = g_{s^\#}(\iota, \sigma)$ be the thread collective assigned to execute the statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item let $q^*$ be the \myKeyA{extended timeline set} (Section~\ref{sec:InstrTL})
  \item let $t$ be the convergence flag (Section~\ref{sec:InstrConvergentAccess})
\end{itemize}
Require that
\begin{equation*}
    \forall r \in \rho(x, n^*).m, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{VF_{full}}, r)
\end{equation*}
If the above is not true, the abstract machine enters the error condition.
Otherwise, this step causes no state changes, but the abstract machine continues on to the changes specified in Section~\ref{sec:VisRecordCreation}.

\subsection{Checks on Mutate}
\label{sec:ChecksOnMutate}

Mutate accesses impose requirements on both the read and mutate visibility record sets.

For each array element $x[n^*]$ that is mutated as part of a write, reduce, or instr call statement,
\begin{itemize}
  \item let $g^* = g_{s^\#}(\iota, \sigma)$ be the thread collective assigned to execute the statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item let $q^*$ be the \myKeyA{extended timeline set} (Section~\ref{sec:InstrTL})
  \item let $t$ be the convergence flag (Section~\ref{sec:InstrConvergentAccess})
  \item let $\mathsf{vf}: \mathsf{VF}$ (def~\ref{sec:gVisFlag}) be $\mathsf{VF_{temp}}$ if the access is non-atomic write-only, $\mathsf{VF_{full}}$ if the access is non-atomic read-write, and $\mathsf{VF_{atom}}$ if the access is atomic (Section~\ref{sec:AtomicInstr}).
\end{itemize}
Require that
\begin{align*}
    & \forall r \in \rho(x, n^*).r, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{VF_{temp}}, r) \\
    & \forall r \in \rho(x, n^*).m, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{vf}, r)
\end{align*}
If the above is not true, the abstract machine enters the error condition.
Otherwise, this step causes no state changes, but the abstract machine continues on to the changes specified in Section~\ref{sec:VisRecordCreation}.

\subsection{Checks on SMEM Free}
\label{sec:ChecksOnFree}

Variables stored in shared memory must currently be allocated and freed at \lighttt{cuda\_cluster} scope (def~\ref{sec:gCollUnit}).
For such variables, upon free, we perform the same checks as for a non-atomic write-only access (Section~\ref{sec:ChecksOnMutate}) with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ being the thread collective assigned to execute the free statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $q^* = \{ \texttt{cuda\_in\_order\_qual} \}$ (def~\ref{sec:gQualTL})
  \item the convergence flag $t$ being false
\end{itemize}

\FloatBarrier
\newpage
\section{Instructions \& Timelines}
\label{sec:Instr}

\subsection{Instr Class}
\label{sec:InstrClass}

\subsection{Instruction Timeline (InstrTL)}
\label{sec:InstrTL}

\subsection{Instruction Convergent Access}
\label{sec:InstrConvergentAccess}

\subsection{Atomic Instructions}
\label{sec:AtomicInstr}

\subsection{Instruction Distributed Memory}
\label{sec:InstrDistributedMemory}

\subsection{Instruction Trailing Barrier Expression}
\label{sec:InstrTrailingBarrierExpr}

\subsection{Special Window}
\label{sec:SpecialWindow}

\FloatBarrier
\newpage
\section{Glossary \& Reference}
\label{sec:Glossary}

% >A

\subsection{$\mathcal{A}$ (Augment Function)}

Abstract machine helper function defined in Section~\ref{sec:Augment}.
Used to modify the set of visibility records (Section~\ref{sec:VisRecordState}) modified by an interpreted \lighttt{Fence} statement (Section~\ref{sec:FenceSemantics}) or \lighttt{Await} statement (Section~\ref{sec:AwaitSemantics}).

\subsection{Agnostic (Collective Type)}
\label{sec:gAgnostic}

A collective type $\delta$ is agnostic if any $\delta.B_i$ is $\top$.

\subsection{Aligned Form (Collective Type)}
\label{sec:gAlignedForm}

A collective type $\delta$ is in aligned form when all box coordinates $\delta.B_i$ satisfy $\delta.B_i \in \{\top, 1, \delta.D_i\}$.
If no box coordinate is $\top$, then if $\delta$ describes two thread collectives, those two thread collectives are either identical or disjoint.

% >B

\subsection{$\mathbb{B}$ (mathbb-B)}
\label{sec:gMathbbB}

$\mathbb{B}$ denotes the set of barrier variable names.
Somewhat confusingly, we conventionally use $z$ to mean an element of $\mathbb{B}$, i.e. $z$ is the name of a barrier variable (def~\ref{sec:gZ}).

\subsection{Barrier Environment}
\label{sec:gBarrierEnv}

This is an implementation detail of camspork, and ``officially'', this is part of the synchronization environment.
See Section~\ref{sec:SyncEnv}.

\subsection{Barrier Expression}
\label{sec:gBarrierExpr}

An expression ``$z[e^*]$'', where $z: \mathbb{B}$ is a barrier variable name and $e^*$ is a tuple of expressions (``$z$'' alone indicates $e^*$ is empty).
Each expression $e_i$ must either be a plain read of a control variable (def~\ref{sec:gMathbbY}) or an interval expression $0:x_i$, $x_i$ being the extent of the $i^{th}$ dimension of $z$'s array size.

A barrier expression $z[e_0, ..., e_{M-1}]$ references all barrier array elements $z[n_0, ..., n_{M-1}]$ where
\begin{itemize}
  \item $n_i = \sigma(e_i)$ (def~\ref{sec:gControlEnv}) for all $e_i$ that are plain reads of a control variable.
  \item $n_i \in [0, x_i - 1]_\mathbb{N}$ for all $e_i$ that are intervals $0:x_i$.
\end{itemize}

% >C

\subsection{Camspork}
\label{sec:gCamspork}

C Abstract Machine for Spork.
C++ library that implements the abstract machine interpreter used for sync-check.

\subsection{Cluster}
\label{sec:gCluster}
Group of \lighttt{clusterDim}-many CTAs (def~\ref{sec:gCta}) that execute concurrently on the same GPC, and can synchronize with cluster sync, or using mbarriers.
When \lighttt{clusterDim = 1} (which is the case by default for Exo-GPU), then CTA and cluster are synonymous.
The PTX variable \lighttt{cluster\_ctarank} is the 0-based index of the CTA in the cluster.

\subsection{Collective Analysis}
\label{sec:gCollAnalysis}

Compilation step that assigns collective tilings (Section~\ref{sec:CollTiling}) to each CUDA-scope statement (def~\ref{sec:gCudaScope}) and performs distributed memory analysis (Section~\ref{sec:DistributedMemory}).

\subsection{Collective Indexing Pair}
\label{sec:gCollIndexingPair}

Object of type $\Omega \times \mathsf{Expr}^*$, where $\Omega$ is a collective tiling (def~\ref{sec:gCollTiling}) and $\mathsf{Expr}^*$ is a tuple of array index expressions.
Used for distributed memory analysis (Section~\ref{sec:DistributedMemory}) of a variable.
Collective indexing pairs are collected from all statements and expressions that index said variable.

\subsection{Collective Tiling}
\label{sec:gCollTiling}

A per-statement attribute of statements at CUDA scope (def~\ref{sec:gCudaScope}).
We denote a collective tiling with $\omega \in \Omega$.
This describes via the function $\textsf{collMap}: \Omega \to \Sigma_c \to \mathcal{P}(\mathbb{N})$ (Section~\ref{sec:CollTilingState}) a mapping between control variable values and the local thread indices (def~\ref{sec:gLocalThreadIndex}) of the thread collective (def~\ref{sec:gThreadCollective}) assigned to execute an instance of the statement (def~\ref{sec:gStatementInstance}).

\subsection{Collective Type (Scope)}
\label{sec:gCollType}

Description of a certain number and arrangement of threads in a cluster, e.g. thread, warp, CTA (Section~\ref{sec:CollType}).
A collective type $\delta$ has a certain dimensionality $M$, and consists of
\begin{itemize}
  \item domain, $\delta.D: \mathbb{N}_{\ge2}^M$ (coordinates are natural numbers at least 2).
  \item box, $\delta.B: \mathbb{N}_\top^M$ (coordinates are natural numbers or $\top$).
\end{itemize}
As well as implied dimension thread pitch values $\delta.P$ (def~\ref{sec:gThreadPitch}).
A statement is at $\delta$-scope when it is in CUDA scope (def~\ref{sec:gCudaScope}) and the thread collectives assigned to execute instances of that statement are always described by $\delta$ (Section~\ref{sec:CollTypeThreadCollective}).


\subsection{Collective Unit (Scope)}
\label{sec:gCollUnit}

Syntactic construct that wraps a collective type.
This may be parameterized based on \lighttt{blockDim} and \lighttt{clusterDim}.
A collective unit instance is $\tau_u$ in the grammar, and is an instance of \lighttt{CollUnit} in Python, with $\top$ represented by \lighttt{None} and other coordinate values represented by an instance of \lighttt{CollSizeExpr}.

A statement is at $\tau_u$-scope when it is in $\delta$-scope (def~\ref{sec:gCollType}), where $\delta$ is unpacked from $\tau_u$ with alignment and 1-padding; we define this after the table.

{
\footnotesize
\centering
\arraycolsep=1.8pt\def\arraystretch{1.0}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{rrlll}
\toprule
& & & \emph{domain} & \emph{box} \\
$\tau_u : \mathrm{CollUnit} $ & $\Coloneqq$ &
  \texttt{standalone\_thread} & \texttt{(1,)} & \texttt{(1,)} \\
  &|& \texttt{$n_1$ * cuda\_thread} & \texttt{(blockDim,)} & \texttt{($n_1$,)} \\
  &|& \texttt{cuda\_quadpair} & \texttt{(blockDim/16, 16)} & \texttt{(2, 4)} \\
  &|& \texttt{$n_1$ * cuda\_warp} & \texttt{(blockDim,)} & \texttt{($n_1$ * 32,)} \\
  &|& \texttt{$n_1$ * cuda\_warpgroup} & \texttt{(blockDim,)} & \texttt{($n_1$ * 128,)} \\
  &|& \texttt{$n_1$ * cuda\_threads\_strided($n_2$, $n_3$)} & \texttt{(blockDim/$n_3$, $n_3$)} & \texttt{($n_1$, $n_2$)} \\
  &|& \texttt{$n_1$ * cuda\_warp\_in\_cluster} & \texttt{(clusterDim, blockDim)} & \texttt{($n_1$, 32)} \\
  &|& \texttt{$n_1$ * cuda\_cta\_in\_cluster} & \texttt{(clusterDim * blockDim,)} & \texttt{($n_1$ * blockDim,)} \\
  &|& \texttt{cuda\_cluster} & \texttt{(clusterDim * blockDim,)} & \texttt{(clusterDim * blockDim,)} \\
  &|& \texttt{$n_1$ * cuda\_cta\_in\_cluster\_strided($n_3$)} & \texttt{(ClusterDim/$n_3$, $n_3$, blockDim)} & \texttt{($n_1$, 1, blockDim)} \\
  &|& \texttt{$n_1$ * cuda\_warp\_in\_cluster\_strided($n_3$)} & \texttt{(clusterDim/$n_3$, $n_3$, blockDim)} & \texttt{($n_1$, 1, 32)} \\
  &|& \texttt{cuda\_agnostic\_sub\_cta} & \texttt{(clusterDim, blockDim)} & \texttt{(1, $\top$)} \\
  &|& \texttt{cuda\_agnostic\_intact\_cta} & \texttt{(clusterDim, blockDim)} & \texttt{($\top$, blockDim)} \\
\bottomrule
\end{tabular}
}

The conversion to a collective type may or may not be \textit{aligned} and may or may not be \textit{1-padded}.
The steps to unpack a collective type from a collective unit are

\begin{itemize}
  \item Substitute concrete values for \lighttt{clusterDim} and \lighttt{blockDim}.
    This converts the domain and box into tuples of rational numbers or $\top$.
  \item Fail if any non-$\top$ coordinate is not a natural number, or if $B_i \ne \top \land B_i \notin [1, D_i]$ for any box coordinate $B_i$ and corresponding domain coordinate $D_i$.
  \item Remove any domain coordinates $D_i$ with $D_i = 1$ and remove corresponding box coordinates $B_i$ (it must be the case that $B_i = 1$ or $B_i = \top$ by the above check).
  \item Initialize the collective type $\delta$ with the box and domain.
  \item Let $f = \frac{\texttt{clusterDim * blockDim}}{\delta.D_0 \times \delta.D_1 \times ...}$; fail if $f \notin \mathbb{N}$.
  \item If $f > 1$, prepend $f$ to $\delta.D$, and prepend $1$ or $\top$ to $\delta.B$, for the 1-padded and non-1-padded cases, respectively.
  \item If the conversion is \textit{aligned}, reshape (Section~\ref{sec:CollTypeReshape}) until the collective type is in aligned form (def~\ref{sec:gAlignedForm}); fail if this cannot be done.
  \item \textbf{NOTE:} not all of these failures seem to be checked by Exo-GPU today.
\end{itemize}

\mainKey{Example 1:} Suppose \lighttt{clusterDim = 2}, \lighttt{blockDim = 256}, and the collective unit has domain (\lighttt{blockDim},), box (128,).

If we have alignment and 1-padding, then
\begin{itemize}
  \item Substitution gives $\delta.D = (256,)$, $\delta.B = (128,)$.
  \item No domain coordinates are 1.
  \item $f = 2$, so we prepend $2$ to $\delta.D$ and $1$ to $\delta.B$ (since 1-padding is on).
    Now $\delta.D = (2, 256)$ and $\delta.B = (1, 128)$.
  \item Since we have alignment, split dimension 1 by 128 to get the final collective type $\delta$, with $\delta.D = (2, 2, 128)$ and $\delta.B = (1, 1, 128)$.
\end{itemize}
If we have neither alignment nor 1-padding, then the result would instead be $\delta.D = (2, 256)$ and $\delta.B = (\top, 128)$.

\mainKey{Example 2:} Suppose \lighttt{clusterDim = 8}, \lighttt{blockDim = 384}, and the collective unit has domain (\lighttt{clusterDim}, \lighttt{blockDim}) and box (2, 128).

If we have alignment, then
\begin{itemize}
  \item Substitution gives $\delta.D = (8, 384), \delta.B = (2, 128)$.
  \item No domain coordinates are 1.
  \item $f = 1$, so no change needed.
  \item Since we have alignment, we do two splits to get $\delta.D = (4, 2, 3, 128)$ and $\delta.B = (1, 2, 1, 128)$.
\end{itemize}
If we don't have alignment, then the result would instead be $\delta.D = (8, 384)$ and $\delta.B = (2, 128)$.
1-padding does not impact this result.

\mainKey{Example 3:} Suppose \lighttt{clusterDim=1}, \lighttt{blockDim = 128}, and the collective unit has domain $(\lighttt{clusterDim}, \lighttt{blockDim})$ and box $(1, \top)$.
\begin{itemize}
  \item Substitution gives $\delta.D = (1, 128), \delta.B = (1, \top)$.
  \item Remove the $0^{th}$ dimension as $\delta.D_0 = 1$. So $\delta.D = (128,), \delta.B = (\top,)$.
  \item $f = 1$, so no change needed.
\end{itemize}
Alignment and 1-padding don't affect this example.
The unpacked collective type is always $\delta.D = (128,), \delta.B = (\top,)$.

\subsection{Control Environment}
\label{sec:gControlEnv}

Function $\sigma: \mathbb{Y} \to \mathbb{Z}$, mapping control variable names (def~\ref{sec:gMathbbY}) to integer values.
This is common to both Exo value semantics, and Exo-GPU abstract machine semantics.

\subsection{CPU Scope}
\label{sec:gCpuScope}

Statements outside of a \lighttt{CudaDeviceFunction} block (def~\ref{sec:gCudaDeviceFunction}), including said statement itself, are at CPU scope.

\subsection{CTA}
\label{sec:gCta}
Cooperative thread array, also known as a ``thread block''.
Group of \lighttt{blockDim}-many CUDA threads that execute concurrently on the same SM, and can be synchronized with \lighttt{\_\_syncthreads()} in CUDA C++.

Exo-GPU only parallelizes on the x-dimension, so \lighttt{threadIdx.x} identifies a thread within a CTA; it ranges from 0 to \lighttt{blockDim - 1}.

\subsection{CUDA Device Function Block}
\label{sec:gCudaDeviceFunction}

Statement that launches its body as a CUDA device function (``kernel''/``grid'') (Section~\ref{sec:CudaDeviceFunction}).

\input{b_samples/CudaDeviceFunction.0.tex}

The above example may be scheduled using

\input{b_samples/CudaDeviceFunction_scheduling.0.tex}

\subsection{CUDA Scope}
\label{sec:gCudaScope}

Statements within a \lighttt{CudaDeviceFunction} block (def~\ref{sec:gCudaDeviceFunction}) are at CUDA scope.

% >D

\subsection{Delta ($\delta: \Delta$)}
\label{sec:gDelta}

$\Delta$ denotes the set of all collective types (def~\ref{sec:gCollType}, Section~\ref{sec:CollType}) and $\delta$ is an element of that set.

\subsection{Device Task}
\label{sec:gDeviceTask}

The body of the inner-most \lighttt{cuda\_tasks} loop is a device task (see~\ref{sec:gCudaDeviceFunction}).

\subsection{Domain}
\label{sec:gDomain}

Attribute $\delta.D$ of a collective type $\delta$ (Section~\ref{sec:CollType}) and attribute $\omega.D$ (Section~\ref{sec:CollTiling}) of a collective tiling $\omega$.
Both are of type $\mathbb{N}_{\ge2}^M$ for some dimension $M: \mathbb{N}$.
This describes the arrangement of threads within a cluster into an $M$-dimensional grid via the \textsf{toLocal} function (def~\ref{sec:gToLocal}).

\subsection{Domain Completion}
\label{sec:gDomainCompletion}

$\textsf{domainCompletion}: \Omega \times \Delta \to \Omega \times \Delta$ takes a collective tiling $\omega$ (def~\ref{sec:gCollTiling}) and a collective type $\delta$ (def~\ref{sec:gCollType}) and returns $(\omega', \delta')$ such that $\omega'.D = \delta'.D$ (def~\ref{sec:gDomain}), with $\omega$ converted to $\omega'$ and $\delta$ converted to $\delta'$ using only reshape operations.
This ensures the ``meaning'' of $\omega'$ and $\delta'$ is the same as $\omega$ and $\delta$, except that $\delta'$ may imply stricter alignment requirements.

An example (inefficient) implementation sketch of domain completion is given:

\begin{itemize}
  \item If $\omega.D = \delta.D$, return $(\omega, \delta)$.
  \item If $\prod \omega.D_i \ne \prod \delta.D_i$, fail (the implied cluster thread count isn't equal).
  \item Calculate the thread pitch sets (def~\ref{sec:gThreadPitch}) $\omega.P$ and $\delta.P$.
  \item Pick $p \in \omega.P \setminus \delta.P$, if it exists, then,
  \begin{itemize}
    \item Let $\delta.P_i = \max \{ a \in \delta.P \mid a < p \}$.
    \item Let $f = p / \delta.P_i$; fail if this is not an integer.
    \item Split the $i^{th}$ dimension of $\delta$ by $f$ (Section~\ref{sec:CollTypeReshape}); this has the effect of adding $p$ to the thread pitch set of $\delta$.
  \end{itemize}
  \item Otherwise, pick $p \in \delta.P \setminus \omega.P$, which must exist, then,
  \begin{itemize}
    \item Let $\omega.P_i = \max \{ a \in \omega.P \mid a < p \}$.
    \item Let $f = p / \omega.P_i$; fail if this is not an integer.
    \item Split the $i^{th}$ dimension of $\omega$ by $f$ (Section~\ref{sec:CollTilingReshape}); this has the effect of adding $p$ to the thread pitch set of $\omega$.
  \end{itemize}
  \item Return $\mathsf{domainCompletion}(\omega, \delta)$.
\end{itemize}

\subsection{Domain Completion Type Only}
\label{sec:gDomainCompletionTypeOnly}

$\textsf{domainCompletionTypeOnly}: \Delta \times \Delta \to \Delta \times \Delta$ is defined as $\textsf{domainCompletion}$ (def~\ref{sec:gDomainCompletion}), but with the role of $\omega: \Omega$ replaced with $\delta_2: \Delta$.

% >E
% >F
% >G

\subsection{Global Thread ID ($\mathbb{G}$, mathbb-G)}
\label{sec:gGlobalThreadID}

Pair of task ID $\iota: \mathbb{I}$ (Section~\ref{sec:SyncSemanticsThreadMapping}) and local thread index $n \in \mathbb{N}$ (def~\ref{sec:gLocalThreadIndex}).

% >H

\subsection{Home Barrier (Expression)}
\label{sec:gHomeBarrier}

An \lighttt{Arrive} statement (def~\ref{sec:gSyncStmt}) may take multiple barrier expressions (def~\ref{sec:gBarrierExpr}).
The intersection of all barrier array elements referenced by the barrier expressions must be a single barrier array element; this is the home barrier.
The home barrier expression is an expression constructed to reference this home barrier; it is constructed from the barrier expressions $z_0[e_0^*] ... z_{E-1}[e_{E-1}^*]$ attached to an \lighttt{Arrive} statement as follows:
\begin{itemize}
  \item All expressions must use the same barrier variable name $z$.
  \item All tuples $e^*$ must have the same length $M$.
  \item For each $j \in [0, M-1]_\mathbb{N}$, at least one $e^*$ must have a non-interval (point) as its $j^{th}$ element, and all such tuples must match in their $j^{th}$ element. Call this common expression $e'_j$.
\end{itemize}
The home barrier expression is $z[e'_0, ..., e'_{M-1}]$.

Examples

\begin{tabular}{l l}
\textbf{Arrive statement} & \textbf{Home barrier expression} \\
\texttt{Arrive(...) >> $z$[$y_1, y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z$[$y_1$, :]} & Missing point expression for rightmost dimension \\
\texttt{Arrive(...) >> $z$[$y_1$, :] >> $z$[$y_1$, $y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z$[$y_1$, :] >> $z$[:, $y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z_1$[$y_1$, :] >> $z_2$[:, $y_2$]} & Invalid, mismatched barrier variable names $z$
\end{tabular}

% >I

\subsection{\textrm{I}ota, mathbb-\textrm{I} ($\iota: \mathbb{I}$)}
\label{sec:gIota}

$\mathbb{I}$ denotes the set of all task IDs (Section~\ref{sec:SyncSemanticsThreadMapping}) and $\iota$ is an element of that set.

% >J
% >K
% >L

\subsection{Local Thread Index}
\label{sec:gLocalThreadIndex}

0-based integer index uniquely identifying a thread within a cluster.
The threads are indexed lexicographically by (CTA index, thread index in CTA).
The closed-form equation is \lighttt{cluster\_ctarank * blockDim.x + threadIdx.x}

Note, not to be confused with linear thread IDs (tid) in camspork, which also encodes the task index (Section~\ref{sec:SyncSemanticsThreadMapping}).

\subsection{Loop Mode}
\label{sec:gLoopMode}

Each Exo loop has an included loop mode object, which is one of these Python objects:
\begin{itemize}
  \item \lighttt{Seq(pragma\_unroll: Optional[int])}: sequential loop.
  \item \lighttt{Par()}: OpenMP parallel-for.
  \item \lighttt{CudaTasks()}: distribute iterations (device tasks,~\ref{sec:gDeviceTask}) to CUDA clusters; defines new task IDs in abstract machine semantics (Section~\ref{sec:SyncSemanticsThreadMapping}).
  \item \lighttt{CudaThreads(unit: CollUnit)}: distribute iterations to thread collectives within a cluster.
\end{itemize}
This does not affect the sequential semantics of the loop.

The respective frontend syntax is

\input{b_samples/loop_modes.0.tex}

Scheduling functions:
\begin{itemize}
  \item \texttt{set\_loop\_mode}: use new loop mode object.
  \item \texttt{update\_loop\_mode}: modify attribute of loop mode.
  \item \texttt{parallelize\_loop}: use \lighttt{Par()} as the loop mode (legacy).
\end{itemize}

% >M
% >N

\subsection{$\mathbb{N}$ (mathbb-N)}

Set of natural numbers (non-negative integers).
We use $\mathbb{N}_{\ge x}$ to mean the set of natural numbers greater-than-or-equal-to $x$, and $\mathbb{N}_\top$ to mean the set of natural numbers and the symbol $\top$ (top).

$[a, b]_\mathbb{N}$ denotes the set of natural numbers $n$ where $a \le n \le b$.

% >O
\subsection{Omega ($\omega: \Omega$)}
\label{sec:gOmega}

$\Omega$ denotes the set of all collective tilings (def~\ref{sec:gCollTiling}, Section~\ref{sec:CollTiling}) and $\omega$ is an element of that set.

\subsection{Out-of-order Non-convergent Abstract Machine Optimization}
\label{sec:gOooOpt}

TODO

% >P

\subsection{Pending Await ($\mathbb{P}$ / mathbb-P)}
\label{sec:gPendingAwait}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
Tuple of type $\mathbb{B} \times \mathbb{Z}^* \times \mathbb{Z}$ where the respective tuple elements are
\begin{itemize}
  \item Barrier variable name
  \item Array indices
  \item Arrive count
\end{itemize}
Note the PLDI submission formulates this differently, as a function $p : \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathbb{N})$.
Given $z: \mathbb{B}$ (def~\ref{sec:gMathbbB}), $i^* : \mathbb{Z}^*$, and $p'$ is a set of pending awaits as currently formulated, the correspondence between $p'$ and equivalent $p$ is $(z, i^*, n) \in p' \iff n \in p(z, i^*)$.

\subsection{Power Set ($\mathcal{P}$ / mathcal-P)}
\label{sec:gPowerSet}

$a^* \in \mathcal{P}(A)$ or $a^*: \mathcal{P}(A)$ means that $a^*$ is of type ``set of $A$''.

% >Q

\subsection{Qualitative Timelines (QualTL)}
\label{sec:gQualTL}

Additional description on each memory access, beyond the IDs of the thread(s) performing the access.
A synchronization timeline (SyncTL, def~\ref{sec:gSyncTL}) groups together qualitative timelines.

\input{spork_b/QualTL.tex}

% >R
\subsection{$\mathbb{R}$ (mathbb-R)}
\label{sec:gMathbbR}

Used in the PLDI submission to mean $\mathsf{VisRecord}$ (def~\ref{sec:gVisRecord}).

% >S

\subsection{$\mathbb{S}$ (mathbb-S)}
\label{sec:gMathbbS}

Used in the PLDI submission to mean the set of all visibility functions, which corresponds to visibility sets (def~\ref{sec:gVisSet}) in Exo-GPU as it is currently formulated.
Note ``visibility function'' is not a term used in the PLDI submission; this is retroactive terminology for this deprecated concept.

\subsection{Sigma ($\sigma: \Sigma$)}
\label{sec:gSigma}

$\Sigma$ denotes the set of all control environments (def~\ref{sec:gControlEnv}) and $\sigma$ is an element of that set.
Note, historically sometimes $\Sigma_c$ or $\sigma_c$ are used.

\subsection{Square Brackets}
\label{sec:gSquareBrackets}

May denote indexing an array, or function update notation (def~\ref{sec:gUpdateNotation}).

\subsection{Statement Instance}
\label{sec:gStatementInstance}

A \emph{statement} is a syntactic construct, while a \emph{statement instance} is a single interpretation/``execution'' of a statement. For example, ``\lighttt{for i in seq(0, 10): $s_1$; $s_2$}'' is a loop containing two statements: $s_1$ and $s_2$, and when the loop is executed, 20 statement instances are created (10 instances of $s_1$ and 10 instances of $s_2$).

\subsection{Sync-Exempt}
\label{sec:gSyncExempt}

Certain memory types are sync-exempt, as defined by their Python\\
\lighttt{AllocableMemWin.sync\_exempt()} function.
All reads and writes to array elements stored in sync-exempt memory type are no-ops in the abstract machine (Section~\ref{sec:SyncSemantics}), i.e., no hazards will ever be diagnosed for variables stored in sync-exempt memory.

\subsection{Synchronization Environment (SyncEnv)}
\label{sec:gSyncEnv}

See Section~\ref{sec:SyncEnv}.

\subsection{Synchronization Statement}
\label{sec:gSyncStmt}

One of

\hphantom{spacing}
\texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}
\hfill
\texttt{Arrive($\tau_s^\mathrm{pre}$) >}\texttt{> $e$}
\hfill
\texttt{Await($e, \tau_s^\mathrm{post}, n$)}
\hphantom{spacing}

where $\tau_s^\mathrm{pre}$ and $\tau_s^\mathrm{post}$ are \myKeyA{sync timelines} (\textsf{SyncTL}, def~\ref{sec:gSyncTL}), which filter the set of qualitative timelines (def~\ref{sec:gQualTL}) of memory accesses that are synchronized, and $e$ and $n$ are a barrier expression (def~\ref{sec:gBarrierExpr}) and an integer, which together control pairing of executed \lighttt{Arrive} and \lighttt{Await} instances.
The \lighttt{Arrive} statement may take additional barrier expressions, separated by \texttt{>}\texttt{>}.
Currently, an \lighttt{Await} statement may take only one barrier expression, and it must reference only a single barrier array element, i.e. it must be its own home barrier expression (def~\ref{sec:gHomeBarrier}).

Scheduling functions:
\begin{itemize}
  \item \texttt{insert\_barrier\_alloc}: insert allocation of a barrier variable
  \item \texttt{insert\_fence}
  \item \texttt{insert\_arrive}
  \item \texttt{insert\_await}
\end{itemize}

\subsection{Synchronization Timeline (SyncTL)}
\label{sec:gSyncTL}

Parameter for synchronization statements (\lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}), which are defined as a composition of a \myKeyA{full timeline set} (set of \textsf{QualTL}, def~\ref{sec:gQualTL}) and a \myKeyA{temporal timeline set} (set of \textsf{QualTL}).
For a given $\tau_s$, these are denoted as $\tau_s.\mathrm{full}$ and $\tau_s.\mathrm{temp}$ respectively.

We list the sync timelines $\tau_s$ in the following table, where ``temp.'' in a qualitative timeline's column indicates membership of the qualitative timeline in $\tau_s.\mathrm{temp}$, and ``full'' indicates membership in \emph{both} $\tau_s.\mathrm{full}$ and $\tau_s.\mathrm{temp}$. The PLDI submission also had ``transitive'', which is gone now.

{
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{|r|l l|l l|l l l| l l l l|l|}
\hline
$\tau_s$ & cpu & strm & cuda1 & cuda2 & Sm80 & tmaS & tmaG & wgA & wgD & wgS & wg0 & async \\
\hline
\texttt{empty\_sync\_tl} &  &  &  &  &  &  &  &  &  &  &  & \\
\texttt{cpu\_in\_order} & full &  &  &  &  &  &  &  &  &  &  & \\
\texttt{cuda\_stream\_sync} &  & full & full & full & full & full & full & full & full & full &  & full\\
\texttt{cuda\_in\_order} &  &  & full & full &  &  &  &  &  &  & temp. & temp.\\
\texttt{cuda\_temporal} &  &  & temp. & temp. &  &  &  &  &  &  & temp. & temp.\\
\texttt{Sm80\_cp\_async} &  &  &  &  & full &  &  &  &  &  &  & \\
\texttt{Sm80\_generic} &  &  & full & full & full &  &  &  &  &  & temp. & temp.\\
\texttt{tma\_to\_smem\_async} &  &  &  &  &  & full &  &  &  &  &  & \\
\texttt{tma\_to\_gmem\_async} &  &  &  &  &  &  & full &  &  &  &  & \\
\texttt{wgmma\_async\_smem} &  &  &  &  &  &  &  &  &  & full &  & \\
\texttt{wgmma\_fence\_1} &  &  & full &  &  &  &  & full & full &  &  & \\
\texttt{wgmma\_fence\_2} &  &  &  &  &  &  &  & full & full &  &  & \\
\texttt{wgmma\_async} &  &  &  &  &  &  &  & full & full & full &  & \\
\texttt{cuda\_generic\_and\_async\_proxy} &  &  & full & full &  &  &  &  &  &  & temp. & full\\
\hline
\end{tabular}
}

\textsf{QualTL} key:

\input{spork_b/QualTL.tex}

% NB note alphabetical sorting
%           |
% Synchronization
% Synchronizes
\subsection{Synchronizes-with}
\label{sec:gSynchronizesWith}

camspork's name for the witness function (Section~\ref{sec:Witness}).

% >T
\subsection{Thread Collective}
\label{sec:gThreadCollective}

Set of threads assigned to execute one statement instance (def~\ref{sec:gStatementInstance}).
On the CUDA device, the threads in the thread collective are uniquely identified by its cluster index (which is not statically analyzed) and its local thread index (def~\ref{sec:gLocalThreadIndex}), which is analyzed by collective analysis (Section~\ref{sec:CollTiling}).
In the context of the abstract machine, the cluster index is replaced with a task index, since the mapping between tasks and clusters is a hardware-dependent detail we abstract over.
Specifically, a thread collective in this context is a set of global thread IDs (def~\ref{sec:gGlobalThreadID}).

\subsection{Thread Cuboid}
\label{sec:gThreadCuboid}

camspork implementation concept, which takes the place of the thread mapping (Section~\ref{sec:SyncSemanticsThreadMapping}).
The thread cuboid has a task ID $\iota: \mathbb{I}$, and equal-dimensional integer tuples domain ($D$), box ($B$), and offset ($\textit{Off}$).
This encodes via the \textsf{toLocal} function (def~\ref{sec:gToLocal}) the thread collective (def~\ref{sec:gThreadCollective}) of global thread IDs
\begin{equation*}
  \{ (\iota, \mathsf{toLocal}(D, c) \mid c \in
      [\textit{Off}_0, \textit{Off}_0 + B_0 - 1]_\mathbb{N} \times ...
      [\textit{Off}_1, \textit{Off}_1 + B_1 - 1]_\mathbb{N} \times ...  \}
\end{equation*}

\subsection{Thread Pitch (Set)}
\label{sec:gThreadPitch}

The thread pitch is used in multiple contexts.
In all cases, it describes the ``distance'', in local thread indices (def~\ref{sec:gLocalThreadIndex}), between adjacent items of some sort.

\mainKey{\texttt{cuda\_threads} Loop Iterator:} Let $\mu: \mathcal{P}(\mathbb{N})$ be the local thread indices of the thread collective executing the 0th iteration of the loop.
The local thread indices of the thread collective execucting the $j^{th}$ iteration of the loop are $\{t + jp \mid t \in \mu\}$, $p$ being the thread pitch of the loop iterator.
If the loop has no more than 1 iteration, then the thread pitch of the loop iterator is 0.

\mainKey{Distributed Memory:} Let $\mu: \mathcal{P}(\mathbb{N})$ be the local thread indices of the thread collective allocating the physical memory holding $x[0, ..., 0]$, and let the deduced thread pitch tuple for the variable $x$ be $(p_0, ..., p_{M-1})$.
Then the local thread indices of the thread collective for $x[i_0, i_1, ...]$ are
\begin{equation*}
  \left \{ t + \sum_{k=0}^{M-1} p_k i_k \mid t \in \mu \right \}
\end{equation*}

\mainKey{Domain:} For a domain $(D_0, ..., D_{M-1})$, we define respective dimension thread pitch values as
\begin{equation*}
    P_m = \prod_{k=m+1}^{M-1} D_k
\end{equation*}
As a shorthand, we say $\delta.P_k$ or $\omega.P_k$ to mean the $k^{th}$ dimension thread pitch defined above, with respect to $\delta.D$ or $\omega.D$.
The thread pitch set of $D$ is $\{P_0, ..., P_{M-1}\}$; note $P_{M-1} = 1$ always.

\mainKey{Collective Tiling/Type:} The thread pitch set of a collective tiling/type is that of its domain.

\subsection{Timeline Signature (TlSig)}
\label{sec:gTlSig}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
Triple $(g, q, v)$ of type $\mathbb{G} \times \mathsf{QualTL} \times \mathsf{VF}$, i.e. a global thread id $g$ (def~\ref{sec:gGlobalThreadID}), qualitative timeline $q$ (def~\ref{sec:gQualTL}), and visibility flag $v$ (def~\ref{sec:gVisFlag}).
These are composed by visibility records (def~\ref{sec:gVisRecord}).

\subsection{toLocal}
\label{sec:gToLocal}

We define the mapping $\mathsf{toLocal}: \mathbb{N}^M \to \mathbb{N}^M \to \mathbb{N}$, which converts a domain (def~\ref{sec:gDomain}) and coordinates to a local thread index, as
\begin{align*}
    \mathsf{toLocal}((D_0,...,D_{M-1}), (c_0,...,c_{M-1})) \mapsto \sum_{k=0}^{M-1} c_k \times D_{k+1} \times ... \times D_{M-1}
\end{align*}
i.e. the coordinates $[0, D_0-1]_\mathbb{N} \times ... \times [0, D_{M-1}-1]_\mathbb{N}$ get mapped to local thread indices (def~\ref{sec:gLocalThreadIndex}) in lexicographical order.
For this definition to work as expected, the product of the domain coordinates $D_0 \times ... \times D_{M-1}$ must be equal to the number of threads in the cluster (\lighttt{clusterDim.x * blockDim.x}).

% >U

\subsection{Update Notation}
\label{sec:gUpdateNotation}

$\rho[a_0 \to b_0, a_1 \to b_1, ...]$ means the function $\rho$ with outputs for $a_0, a_1,...$ modified:
\begin{equation*}
    \rho[...](x) = \begin{cases}
        b_0 & \text{if } x = a_0 \\
        b_1 & \text{if } x = a_1 \\
        ... & ... \\
        \rho(x) & \text{if no other case}
    \end{cases}
\end{equation*}

% >V

\subsection{Visibility Flag ($\mathsf{VF}$)}
\label{sec:gVisFlag}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
One of
\begin{itemize}
  \item $\mathsf{VF_{atom}}$, \texttt{vis\_flag\_atomic\_only}
  \item $\mathsf{VF_{temp}}$, \texttt{vis\_flag\_temporal}
  \item $\mathsf{VF_{full}}$, \texttt{vis\_flag\_full}
  \item $\mathsf{VF_{issue}}$, \texttt{vis\_flag\_issue}
\end{itemize}
These are part of a timeline signature (def~\ref{sec:gTlSig}).

\subsection{Visibility Record (VisRecord)}
\label{sec:gVisRecord}

Defined in abstract machine semantics (Section~\ref{sec:VisRecordState}).
Pair of type $\mathcal{P}(\mathsf{TlSig}) \times \mathcal{P}(\mathbb{P})$, i.e.
\begin{itemize}
  \item visibility set, which is a set of timeline signatures (def~\ref{sec:gTlSig})
  \item set of pending awaits (def~\ref{sec:gPendingAwait}).
\end{itemize}

\subsection{Visibility Set}
\label{sec:gVisSet}

Note, the PLDI submission incorrectly uses ``visibility set'' to mean ``visibility record'' sometimes.

Set of timeline signatures (def~\ref{sec:gTlSig}).
This differs from the PLDI submission nomenclature.
The PLDI submission used a visibility function $\mathbb{S} \Coloneqq \mathbb{G} \to \mathsf{QualTL} \to \mathsf{VL}$, with \textsf{VL} being the deprecated ``visibility level'' value.

Approximately, the correspondence between a visibility function $f$ and a visibility set $s$ is
\begin{itemize}
  \item Each visibility level corresponds to some set of visibility flags (def~\ref{sec:gVisFlag}), specifically, none corresponds to $\{\}$, atomic-only corresponds to $\{\mathsf{VF_{atom}}\}$, unordered corresponds to either $\{\mathsf{VF_{issue}}\}$ or $\{\mathsf{VF_{issue}}, \mathsf{VF_{atom}}\}$ (this ambiguity being a reason for this change, and abandoning the strict ordering of visiblity levels), temporally-ordered corresponds to $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}\}$ or $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}, \mathsf{VF_{atom}}\}$, and fully-ordered corresponds to $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}}, \mathsf{VF_{atom}}\}$.
  \item Given visibility function $f: \mathbb{S}$, $g: \mathbb{G}$ (def~\ref{sec:gGlobalThreadID}), $q: \mathsf{QualTL}$ (def~\ref{sec:gQualTL}), and the visibility level $v$ is $v = f(g, q)$, then the visibility set $s$ contains $(g, q, v')$ if and only if $v'$ is a visibility flag in the set of visibility flags corresponding to the visibility level $v$.
\end{itemize}

% >W

\subsection{Warp}
\label{sec:gWarp}

32 CUDA threads with consecutive \lighttt{threadIdx.x} values, aligned so that the lowest index is a multiple of 32.
(Note, this is simplified from the real CUDA definition, which takes into account the y and z dimensions that Exo-GPU does not parallelize on).

\subsection{Warp Variable}
\label{sec:gWarpVariable}

The warp variables for a specific CUDA device function are specified by the \lighttt{warp\_config: List[CudaWarpConfig]} parameter of \lighttt{CudaDeviceFunction}.
This specifies a name and register count for a certain number of warps in the CTA.
See example~\ref{sec:gCudaDeviceFunction}, Section~\ref{sec:CudaDeviceFunction}.

If \lighttt{blockDim} is given instead of \lighttt{warp\_config}, then there's implicitly a single warp variable with name \lighttt{""}, warp count $\lighttt{blockDim}/32$, and no register count modification.

The ``prefix'' of a warp variable is the sum of the warp counts of the warp variables specified before itself.

The CUDA device function is compiled separately for each warp variable, with code paths for other warp variables statically elimintated.

\subsection{Warpgroup}
\label{sec:gWarpgroup}

128 CUDA threads with consecutive \lighttt{threadIdx.x} values, aligned so that the lowest index is a multiple of 128.
(Note, this is simplified from the real CUDA definition, which takes into account the y and z dimensions that Exo-GPU does not parallelize on).

\subsection{$\mathcal{W}$ (Witness Function)}

Abstract machine helper function defined in Section~\ref{sec:Witness}.
Used to filter the set of visibility records (Section~\ref{sec:VisRecordState}) modified by an interpreted \lighttt{Fence} statement (Section~\ref{sec:FenceSemantics}) or \lighttt{Arrive} statement (Section~\ref{sec:ArriveSemantics}).

% >X

\subsection{$\mathbb{X}$ (mathbb-x)}
\label{sec:gMathbbX}

Set of data variable names.
These are variables that are not of control ($\mathbb{Y}$, def~\ref{sec:gMathbbY}) or barrier ($\mathbb{B}$, def~\ref{sec:gMathbbB}) type.

% >Y

\subsection{$\mathbb{Y}$ (mathbb-y)}
\label{sec:gMathbbY}

Set of control variable names: names of index and size variables (either function parameters of index/size type, or loop iterators).

% >Z

\subsection{$z: \mathbb{B}$}
\label{sec:gZ}

Somewhat confusingly, we conventionally use $z$ to mean an element of $\mathbb{B}$, i.e. $z$ is the name of a barrier variable (def~\ref{sec:gMathbbB}).

\subsection{$\mathbb{Z}$ (mathbb-z)}
\label{sec:gMathbbZ}

Set of all integers.

\end{document}
