% exocc b_samples.py && python3 code_to_tex.py b_samples.py b_samples && xelatex spork_b.tex </dev/null
\input{whitepaper_common.tex}

\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{line} = [thick]

\tikzstyle{redstyle} = [draw=redBoxFg, fill=redBoxBg, text=redBoxFg]
\tikzstyle{yellowstyle} = [draw=yellowBoxFg, fill=yellowBoxBg, text=yellowBoxFg]
\tikzstyle{greenstyle} = [draw=greenBoxFg, fill=greenBoxBg, text=greenBoxFg]
\tikzstyle{bluestyle} = [draw=blueBoxFg, fill=blueBoxBg, text=blueBoxFg]
\tikzstyle{violetstyle} = [draw=violetBoxFg, fill=violetBoxBg, text=violetBoxFg]
\tikzstyle{nRedstyle} = [draw=nRedBoxFg, fill=nRedBoxBg, text=nRedBoxFg]
\tikzstyle{nGoldstyle} = [draw=nGoldBoxFg, fill=nGoldBoxBg, text=nGoldBoxFg]
\tikzstyle{nGreenstyle} = [draw=nGreenBoxFg, fill=nGreenBoxBg, text=nGreenBoxFg]
\tikzstyle{nBluestyle} = [draw=nBlueBoxFg, fill=nBlueBoxBg, text=nBlueBoxFg]
\tikzstyle{nPurplestyle} = [draw=nPurpleBoxFg, fill=nPurpleBoxBg, text=nPurpleBoxFg]

\tikzstyle{CollTypeExampleStyle} = [rectangle, draw=black, text centered, text width=23mm]
\tikzstyle{CollTilingExampleStyle} = [rectangle, draw=black, text centered, text width=12.5mm]

\begin{document}

% deriveCollTiling
% Instructions, use deriveCollTiling for memory
% out-of-order non-convergent abstract machine optimization

\myTitle{Exo-GPU (Spork) Guide}

\myChapterLink{sec:Overview}{Overview}

\myChapterLink{sec:CudaDeviceFunction}{Cuda Device Function \& Warp Specialization}

Launching kernels, distributing work to clusters with \lighttt{cuda\_tasks} loops, \lighttt{CudaWarps} blocks.

\myChapterLink{sec:CollType}{Collective Units \& Collective Types}

Modeling subdivisions of threads in the cluster, e.g. threads, warps, warpgroups, CTAs, CTA pairs.

\myChapterLink{sec:CollTiling}{Collective Tiling}

Distributing work to threads-in-cluster with \lighttt{cuda\_threads} loops and \lighttt{CudaWarps} blocks.

\myChapterLink{sec:DistributedMemory}{Distributed Memory}

Sharding tensor and barrier allocations onto different groups of threads.

\myChapterLink{sec:SyncUsage}{Synchronization Usage}

\lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}, barrier mechanisms, home barrier, guarding, timelines.

\myChapterLink{sec:SyncSemantics} {Synchronization Semantics}

Abstract machine semantics, how to read sync-check (\lighttt{camspork}) errors.

\myChapterLink{sec:Instr}{Instructions \& Timelines}

Instruction classes, timelines, atomic instructions, special window e.g., \lighttt{CUtensorMap}.

\myChapterLink{sec:Glossary}{Glossary \& Reference}

\section{Overview}
\label{sec:Overview}


This document assumes knowledge of CPU-only Exo.
We describe the concepts of the GPU extension.
All Exo code continues to be CPU code by default (we say that such code is at \myKeyA{CPU-scope}).
To move code to the GPU, wrap it inside a \lighttt{with CudaDeviceFunction} block (Section~\ref{sec:CudaDeviceFunction}); this defines the \lighttt{clusterDim} (number of CTAs per cluster, def~\ref{sec:gCluster}) and \lighttt{blockDim} (number of threads per CTA, def~\ref{sec:gCta}), and may also define \myKeyA{warp variables} (def~\ref{sec:gWarpVariable}) giving names to subcollections of warps (def~\ref{sec:gWarp}) for the purposes of warp specialization.
The statements within are at \myKeyA{CUDA scope} and are converted to CUDA C++ code.

We generalize loops in Exo to sequential and parallel for loops, distinguished by their \myKeyA{loop mode} (def~\ref{sec:gLoopMode}).
The body of the \lighttt{CudaDeviceFunction} block must contain only a single statement: a nest of one or more \lighttt{cuda\_tasks} loops.
The body of the inner-most \lighttt{cuda\_tasks} loop is a \myKeyA{device task}; one instance of the device task is assigned to one CUDA cluster for execution (see example~\ref{sec:gCudaDeviceFunction}).
Within the device task, each statement instance (def~\ref{sec:gStmtInstance}) is executed by a set of threads within the cluster; this is a \myKeyA{thread collective}.

A \lighttt{cuda\_threads(0, $c_\text{hi}$, unit=\_)} loop may appear anywhere inside a device task.
This statement divides its executing thread collective into $c_\text{hi}$-many disjoint subsets (possibly with some inactive threads left over), and assigns one to execute each iteration of the loop.
The \lighttt{unit} parameter is a \myKeyA{collective unit} (def~\ref{sec:gCollUnit}), from which a \myKeyA{collective type} $\delta$ is unpacked (Section~\ref{sec:CollType}).
The collective type describes a number and arrangement of threads (e.g. single thread, warp, CTA, CTA pair).
In the common case, the thread collectives assigned to each iteration are described by this unpacked $\delta$.

This design encodes uniform execution as a mostly syntactic property of the language.
When the thread collectives that execute instances of a statement are described by a collective type $\delta$, the compiler deduces that the statement is at $\delta$-scope (e.g. warp-scope, CTA-scope, as in the top of Figure~\ref{fig:OverviewThreads}).
The underlying static analysis for this is based on \myKeyA{local thread indices} (def~\ref{sec:gLocalThreadIndex}), which number the threads within a cluster lexicographically based on CTA index, then thread index.
The threads within a thread collective need not have a contiguous range of local thread indices (e.g., ``all even numbered CTAs in a cluster'' is a valid thread collective).

Based on this static analysis, we define a \myKeyA{collective tiling} (Section~\ref{sec:CollTiling}) and thread mapping function annotating each CUDA-scope statement, we define \myKeyA{distributed memory} (Section~\ref{sec:DistributedMemory}), which maps shards of an array onto different thread collectives for storage, and we define correct synchronization (Section~\ref{sec:SyncSemantics}), which we currently can check for a selection of concrete problem sizes.

\subsection{Distributed Memory Overview}

Each \lighttt{cuda\_threads} loop iterator indirectly defines a \myKeyA{thread pitch} (def~\ref{sec:gThreadPitch}).
This is the difference, in local thread indices (def~\ref{sec:gLocalThreadIndex}), between thread collectives assigned to consecutive loop iterations.
Distributed memory is deduced from the indexing pattern of an array, and attempts to map shards of the array into thread collectives described by the collective type specified by the memory type.
This flexibility allows memory to be sharded at different levels of the CUDA thread hierarchy (e.g. per-CTA shared memory shards, per-thread register shards).
The deduction assigns a thread pitch to each distributed dimension; elements that are adjacent on a dimension are resident in thread collectives whose local thread indices differ by that dimension's thread pitch.
We define requirements for distributed memory deduction (Section~\ref{sec:DistributedMemory}); in particular, the index used to index a distributed dimension must be a plain read of an iterator with the same thread pitch as that dimension (Figure~\ref{fig:OverviewThreads}, bottom).

\begin{figure}[t]
\codehrule
\input{b_samples/OverviewThreads.0.tex}
\caption{Threads \& Distributed Memory Example Code}
\label{fig:OverviewThreads}
\codehrule
\end{figure}

\subsection{Synchronization Overview}

Exo-GPU introduces synchronization statements: \lighttt{Fence}, \lighttt{Arrive}, and \lighttt{Await}, as well as the ability to allocate barrier variables, which control pairing between \lighttt{Arrive} and \lighttt{Await} statements.
Like other Exo statements, threads execute synchronization statements in program order.

We view each memory access (read or write) as being performed by a certain thread collective and \myKeyA{qualitative timeline} (\textsf{QualTL}, \ref{sec:gQualTL}).
We need this latter attribute to be able to reason about async instructions; the qualitative timeline of a memory access varies depending on what instruction performs the access.

A \lighttt{Fence} statement instance synchronizes the threads within the thread collective that executes it, e.g. a \lighttt{Fence} at warp-scope corresponds to a \lighttt{\_\_syncwarp}-like construct, and a \lighttt{Fence} at CTA-scope corresponds to a \lighttt{\_\_syncthreads}-like construct (Figure~\ref{fig:OverviewSyncExample}).
``Paired'' instances of an \lighttt{Arrive} and an \lighttt{Await} statement implement a split-barrier construct.
We use this syntax:

\hphantom{spacing}
\texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}
\hfill
\texttt{Arrive($\tau_s^\mathrm{pre}$) >}\texttt{> $e$}
\hfill
\texttt{Await($e, \tau_s^\mathrm{post}, n$)}
\hphantom{spacing}

where $\tau_s^\mathrm{pre}$ and $\tau_s^\mathrm{post}$ are \myKeyA{sync timelines} (\textsf{SyncTL}, \ref{sec:gSyncTL}), which filter the set of qualitative timelines of memory accesses that are synchronized, and $e$ and $n$ are a barrier expression (def~\ref{sec:gBarrierExpr}) and an integer, which together control pairing of executed \lighttt{Arrive} and \lighttt{Await} instances.

The barrier variable itself is allocated with the syntax ``\texttt{<name>: barrier[$n^*$] @ $\pi_z$}'', where $n^*$ defines the array size of the barrier variable and $\pi_z$ defines the completion mechanism (\lighttt{CudaMbarrier}, \lighttt{CudaCommitGroup}, or \lighttt{CudaClusterSync}).
The array is subject to distributed memory analysis.

The goal of synchronization checking is to validate \myKeyA{sequential-parallel equivalence}.
The semantics of an Exo program (which defines the numerical outputs) continue to be defined sequentially; we can, in a sense, view the new Exo-GPU constructs as ``annotations'' on sequential code.
In particular, the semantics of parallel loops are identical to sequential loops, and async instruction calls are interpreted as if they were non-async instructions.
We want to convince ourselves that the generated parallel CUDA program generates the outputs that sequential semantics specify it will output.

Although this is not the formulation formally used by the abstract machine used for synchronization checking (Section~\ref{sec:SyncSemantics}), it's useful to reason about correctness in terms of dependency edges between statement instances.
Take a sequential trace of an Exo-GPU program, decomposed into reads and writes to array elements, and instances of \lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}.
For each pair of reads/writes to the same array element (other than two reads) to be safe, there must be a path from the earlier read/write to the later read/write via dependency edges.
Given the statement to the left of the $\to$ appears earlier in the sequential trace than the statement to the right, and there is a thread in common to their executing thread collectives, we have the following dependency edges.
Italicized values are defined in Section~\ref{sec:SyncUsage}.

\begin{itemize}
  \item \texttt{Read|Write $\to$ Read|Write}\\when the prior memory access is not \emph{out-of-order}, and the prior memory access uses a qualitative timeline that is in the \emph{extended qualitative timeline set} of the subsequent memory access.
  \item \texttt{Read|Mutate $\to$ Fence($\tau_s^\text{pre}$, \_)|Arrive($\tau_s^\text{pre}$) >}\texttt{> \_}\\when the qualitative timeline of the memory access is in the \textit{full timeline set} of $\tau_s^\text{pre}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Read}\\when the qualitative timeline of the read is in the \textit{full timeline set} of $\tau_s^\text{post}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Write}\\when the qualitative timeline of the read is in the \textit{temporal timeline set} of $\tau_s^\text{post}$.
  \item \texttt{Fence(\_, $\tau_s^\text{post}$)|Await(\_, $\tau_s^\text{post}$, \_) $\to$ Fence($\tau_s^\text{pre}$, \_)|Arrive($\tau_s^\text{pre}$) >}\texttt{> \_}\\when there is a qualitative timeline in common to the \textit{full timeline sets} of $\tau_s^\text{pre}$ and $\tau_s^\text{post}$.
\end{itemize}

Finally, we have special handling for atomics, there are dependency edges from instances of \lighttt{Arrive(\_) >}\lighttt{> $b$} and \lighttt{Await($b$, \_, \_)} defined by arrive/await pairing, and dependency edges from reads/writes to \lighttt{Await} instances directly for instructions that take a barrier directly (currently, only TMA-to-SMEM instructions).

\begin{figure}[h!]
\codehrule
\input{b_samples/OverviewSyncExample.0.tex}
\caption{Examples of Synchronization Statements}
\label{fig:OverviewSyncExample}
\codehrule
\end{figure}




\FloatBarrier
\newpage
\section{Cuda Device Function \& Warp Specialization}
\label{sec:CudaDeviceFunction}

Wrap code with a \lighttt{with CudaDeviceFunction(...):} statement to transform it to CUDA.
The body of the \lighttt{CudaDeviceFunction} statement must consist of exactly one statement: a nest of one or more \lighttt{cuda\_tasks} loops.
The body of the inner-most \lighttt{cuda\_tasks} loop is a \myKeyA{device task}; each is assigned to a CUDA cluster for execution.
We implement a persistent-kernel design, so multiple tasks may be co-located on the same cluster.
The shape of the \lighttt{cuda\_tasks} iteration space must be a cuboid, i.e., the loop bounds of one \lighttt{cuda\_tasks} loop must not be dependent on another \lighttt{cuda\_tasks} loop.

The \lighttt{CudaDeviceFunction} object is a Python object, containing attributes
\begin{itemize}
  \item \lighttt{clusterDim} (default 1), number of CTAs per cluster.
  \item \lighttt{blocks\_per\_sm} (default 1), number of CTAs concurrently executing per hardware SM.
  \item \lighttt{blockDim}, number of threads per CTA.
  \item \lighttt{warp\_config}, list of \lighttt{CudaWarpConfig} objects.
\end{itemize}
Exactly one of \lighttt{blockDim} or \lighttt{warp\_config} must be given.
The latter is intended for kernels with warp specialization, where we partition the warps in the CTA into named groups of warps, possibly with a different number of registers each.
Each \lighttt{CudaWarpConfig} defines a \myKeyA{warp variable}, and has attributes
\begin{itemize}
  \item \lighttt{name: str}, the name of the warp variable.
  \item \lighttt{count: int}, number of warps.
  \item \lighttt{setmaxnreg\_dec: Optional[int]}, registers per thread; regs allocated by \lighttt{setmaxnreg.dec}.
  \item \lighttt{setmaxnreg\_inc: Optional[int]}, registers per thread; regs allocated by \lighttt{setmaxnreg.inc}.
\end{itemize}
The \lighttt{blockDim} of the CTA is implicitly 32 times the sum of the number of warps defined.
Within the device task, a \lighttt{with CudaWarps(name=<str>)} statement may be used to restrict the body of the statement to only execute on the subset of warps named (Figure~\ref{fig:CudaDeviceFunction0}).

\begin{figure}[h]
\codehrule
\input{b_samples/CudaDeviceFunction.0.tex}
\caption{Kernel launch with warp specialization}
\label{fig:CudaDeviceFunction0}
\codehrule
\end{figure}

\FloatBarrier
\newpage
\section{Collective Units \& Collective Types}
\label{sec:CollType}

We use collective types $\delta$ to describe a quantity and arrangement of threads within a cluster, such as ``single thread'', ``warp'', ``CTA'', ``one warp from a pair of CTAs''.
These are unpacked from a collective unit $\tau_u$ defined in the frontend language (def~\ref{sec:gCollUnit}).
A collective type consists of two equal-length tuples: a domain and a box.
The dimension $M$ of the collective type is the length of these tuples.
The \myKeyA{domain} ($\delta.D_0...\delta.D_{M-1}$): $\mathbb{N}_{\ge2}^M$ describes an organization of the threads in a cluster into an $M$-dimensional grid.
The \myKeyA{box} ($\delta.B_0...\delta.B_{M-1}$): $\mathbb{N}_\top^M$ describes the number of threads on each dimension to select (the special value $\top$ indicates ``no requirement'').

We first define a linear ordering of threads in a cluster, then extend to multidimensional coordinates.
The local thread index of a thread is \lighttt{cluster\_ctarank * blockDim.x + threadIdx.x}
i.e., the threads in a cluster are numbered in (\lighttt{cluster\_ctarank, threadIdx.x})-lexicographical order (Exo-GPU parallelizes on the x dimension only).

For a given domain, we derive the \myKeyA{dimension thread pitch} $\delta.P_i$:
\begin{align*}
    \delta.P_i = \prod_{k=i+1}^{M-1} \delta.D_k
\end{align*}
and we define the mapping $\mathsf{toLocal}(D, c)$ (def~\ref{sec:gToLocal}), which converts a domain $D$ and coordinates $c$ to a local thread index; the coordinates $[0, \delta.D_0-1]_\mathbb{N} \times ... \times [0, \delta.D_{M-1}-1]_\mathbb{N}$ get mapped to local thread indices in lexicographical order.
The product of the domain coordinates $\delta.D_0 \times ... \times \delta.D_{M-1}$ must be equal to the number of threads in the cluster (\lighttt{clusterDim.x * blockDim.x}).

\subsection{Collective Types \& Thread Collectives}
\label{sec:CollTypeThreadCollective}

A thread collective is described by a collective type $\delta$ when all threads are in the same cluster, and, with $\mu: \mathcal{P}(\mathbb{N})$ being the set of local thread indices of the threads, there exist sets $C_0 ... C_{M-1}: \mathcal{P}(\mathbb{N})$ such that
\begin{itemize}
  \item $C_i \subseteq [0, \delta.D_i - 1]$
  \item $\delta.B_i \ne \top \implies \exists x.\; C_i = [x, x + \delta.B_i - 1]_\mathbb{N}$.
  \item $\mu = \{ \mathsf{toLocal}(\delta.D, c) \mid c \in C_0 \times ... \times C_{M-1}\}$
\end{itemize}
If we have no $\top$ box coordinates, this is specifying that the thread collective forms a cuboid of dimensions $\delta.B_0 \times ... \times \delta.B_{M-1}$ when its threads are arranged in the grid implied by $\delta.D$; therefore, the number of threads in the thread collective is the product of the box coordinates (Figure~\ref{fig:CollTypeExample}).

\begin{figure}[h!]
\sffamily
\hspace{-12mm}
\begin{tikzpicture}[node distance=0mm]
\node(t000a) [CollTypeExampleStyle] {(0, 0)\\0 $ \notin \mu$\\0, 0, 0, 0};
\node(t000b) [CollTypeExampleStyle, right=of t000a, xshift=6mm] {(0, 127)\\127 $ \notin \mu$\\0, 0, 0, 127};
\draw [dotted] (t000a) -- (t000b);
\node(t001a) [CollTypeExampleStyle, right=of t000b, xshift=2mm, yshift=-3mm] {(0, 128)\\128 $ \notin \mu$\\\blueBox{0}, 0, 1, 0};
\node(t001b) [CollTypeExampleStyle, right=of t001a, xshift=6mm] {(0, 255)\\255 $ \notin \mu$\\0, 0, 1, 127};
\draw [dotted] (t001a) -- (t001b);
\node(t002a) [CollTypeExampleStyle, right=of t001b, xshift=2mm, yshift=-3mm] {(0, 256)\\256 $ \notin \mu$\\0, 0, \violetBox{2}, \redBox{0}};
\node(t002b) [CollTypeExampleStyle, right=of t002a, xshift=6mm] {(0, 383)\\383 $ \notin \mu$\\0, 0, \violetBox{2}, \redBox{127}};
\draw [dotted] (t002a) -- (t002b);
\node(t010a) [CollTypeExampleStyle, below=of t000a, yshift=-2mm] {(1, 0)\\384 $ \notin \mu$\\0, 1, 0, 0};
\node(t010b) [CollTypeExampleStyle, right=of t010a, xshift=6mm] {(1, 127)\\511 $ \notin \mu$\\0, 1, 0, 127};
\draw [dotted] (t010a) -- (t010b);
\node(t011a) [CollTypeExampleStyle, right=of t010b, xshift=2mm, yshift=-3mm] {(1, 128)\\512 $ \notin \mu$\\0, 1, 1, 0};
\node(t011b) [CollTypeExampleStyle, right=of t011a, xshift=6mm] {(1, 255)\\639 $ \notin \mu$\\0, 1, 1, 127};
\draw [dotted] (t011a) -- (t011b);
\node(t012a) [CollTypeExampleStyle, right=of t011b, xshift=2mm, yshift=-3mm] {(1, 256)\\640 $ \notin \mu$\\0, 1, 2, 0};
\node(t012b) [CollTypeExampleStyle, right=of t012a, xshift=6mm] {(1, 383)\\767 $ \notin \mu$\\0, 1, 2, 127};
\draw [dotted] (t012a) -- (t012b);

\node(t100a) [CollTypeExampleStyle, below=of t010a, yshift=-4mm, xshift=-4mm] {(2, 0)\\768 $ \notin \mu$\\\blueBox{1}, \yellowBox{0}, 0, 0};
\node(t100b) [CollTypeExampleStyle, right=of t100a, xshift=6mm] {(2, 127)\\895 $ \notin \mu$\\1, 0, 0, 127};
\draw [dotted] (t100a) -- (t100b);
\node(t101a) [CollTypeExampleStyle, right=of t100b, xshift=2mm, yshift=-3mm] {(2, 128)\\896 $ \notin \mu$\\\blueBox{1}, 0, 1, 0};
\node(t101b) [CollTypeExampleStyle, right=of t101a, xshift=6mm] {(2, 255)\\1023 $ \notin \mu$\\1, 0, 1, 127};
\draw [dotted] (t101a) -- (t101b);
\node(t102a) [CollTypeExampleStyle, greenstyle, right=of t101b, xshift=2mm, yshift=-3mm] {(2, 256)\\1024 $ \in \mu$\\1, 0, 2, 0};
\node(t102b) [CollTypeExampleStyle, greenstyle, right=of t102a, xshift=6mm] {(2, 383)\\1151 $ \in \mu$\\1, 0, 2, 127};
\draw [dotted] (t102a) -- (t102b);
\node(t110a) [CollTypeExampleStyle, below=of t100a, yshift=-2mm] {(3, 0)\\1152 $ \notin \mu$\\\blueBox{1}, \yellowBox{1}, 0, 0};
\node(t110b) [CollTypeExampleStyle, right=of t110a, xshift=6mm] {(3, 127)\\1279 $ \notin \mu$\\1, 1, 0, 127};
\draw [dotted] (t110a) -- (t110b);
\node(t111a) [CollTypeExampleStyle, right=of t110b, xshift=2mm, yshift=-3mm] {(3, 128)\\1280 $ \notin \mu$\\1, 1, 1, 0};
\node(t111b) [CollTypeExampleStyle, right=of t111a, xshift=6mm] {(3, 255)\\1407 $ \notin \mu$\\1, 1, 1, 127};
\draw [dotted] (t111a) -- (t111b);
\node(t112a) [CollTypeExampleStyle, greenstyle, right=of t111b, xshift=2mm, yshift=-3mm] {(3, 256)\\1408 $ \in \mu$\\1, 1, 2, 0};
\node(t112b) [CollTypeExampleStyle, greenstyle, right=of t112a, xshift=6mm] {(3, 383)\\1535 $ \in \mu$\\1, 1, 2, 127};
\draw [dotted] (t112a) -- (t112b);

\node(t200a) [CollTypeExampleStyle, below=of t110a, yshift=-4mm, xshift=-4mm] {(4, 0)\\1536 $ \notin \mu$\\2, 0, 0, 0};
\node(t200b) [CollTypeExampleStyle, right=of t200a, xshift=6mm] {(4, 127)\\1663 $ \notin \mu$\\2, 0, 0, 127};
\draw [dotted] (t200a) -- (t200b);
\node(t201a) [CollTypeExampleStyle, right=of t200b, xshift=2mm, yshift=-3mm] {(4, 128)\\1664 $ \notin \mu$\\\blueBox{2}, \yellowBox{0}, 1, 0};
\node(t201b) [CollTypeExampleStyle, right=of t201a, xshift=6mm] {(4, 255)\\1791 $ \notin \mu$\\2, 0, 1, 127};
\draw [dotted] (t201a) -- (t201b);
\node(t202a) [CollTypeExampleStyle, right=of t201b, xshift=2mm, yshift=-3mm] {(4, 256)\\1792 $ \notin \mu$\\2, 0, 2, \redBox{0}};
\node(t202b) [CollTypeExampleStyle, right=of t202a, xshift=6mm] {(4, 383)\\1919 $ \notin \mu$\\2, 0, 2, \redBox{127}};
\draw [dotted] (t202a) -- (t202b);
\node(t210a) [CollTypeExampleStyle, below=of t200a, yshift=-2mm] {(5, 0)\\1920 $ \notin \mu$\\2, 1, \violetBox{0}, 0};
\node(t210b) [CollTypeExampleStyle, right=of t210a, xshift=6mm] {(5, 127)\\2047 $ \notin \mu$\\2, 1, 0, 127};
\draw [dotted] (t210a) -- (t210b);
\node(t211a) [CollTypeExampleStyle, right=of t210b, xshift=2mm, yshift=-3mm] {(5, 128)\\2048 $ \notin \mu$\\2, \yellowBox{1}, \violetBox{1}, 0};
\node(t211b) [CollTypeExampleStyle, right=of t211a, xshift=6mm] {(5, 255)\\2175 $ \notin \mu$\\2, 1, 1, 127};
\draw [dotted] (t211a) -- (t211b);
\node(t212a) [CollTypeExampleStyle, right=of t211b, xshift=2mm, yshift=-3mm] {(5, 256)\\2176 $ \notin \mu$\\2, 1, \violetBox{2}, 0};
\node(t212b) [CollTypeExampleStyle, right=of t212a, xshift=6mm] {(5, 383)\\2303 $ \notin \mu$\\2, 1, 2, 127};
\draw [dotted] (t212a) -- (t212b);

\node(t300a) [CollTypeExampleStyle, below=of t210a, yshift=-4mm, xshift=-4mm] {(6, 0)\\2304 $ \notin \mu$\\3, 0, 0, 0};
\node(t300b) [CollTypeExampleStyle, right=of t300a, xshift=6mm] {(6, 127)\\2431 $ \notin \mu$\\3, 0, 0, 127};
\draw [dotted] (t300a) -- (t300b);
\node(t301a) [CollTypeExampleStyle, right=of t300b, xshift=2mm, yshift=-3mm] {(6, 128)\\2432 $ \notin \mu$\\\blueBox{3}, 0, 1, 0};
\node(t301b) [CollTypeExampleStyle, right=of t301a, xshift=6mm] {(6, 255)\\2559 $ \notin \mu$\\3, 0, 1, 127};
\draw [dotted] (t301a) -- (t301b);
\node(t302a) [CollTypeExampleStyle, right=of t301b, xshift=2mm, yshift=-3mm] {(6, 256)\\2560 $ \notin \mu$\\3, 0, 2, 0};
\node(t302b) [CollTypeExampleStyle, right=of t302a, xshift=6mm] {(6, 383)\\2687 $ \notin \mu$\\3, 0, 2, 127};
\draw [dotted] (t302a) -- (t302b);
\node(t310a) [CollTypeExampleStyle, below=of t300a, yshift=-2mm] {(7, 0)\\2688 $ \notin \mu$\\3, 1, 0, 0};
\node(t310b) [CollTypeExampleStyle, right=of t310a, xshift=6mm] {(7, 127)\\2815 $ \notin \mu$\\3, 1, 0, 127};
\draw [dotted] (t310a) -- (t310b);
\node(t311a) [CollTypeExampleStyle, right=of t310b, xshift=2mm, yshift=-3mm] {(7, 128)\\2816 $ \notin \mu$\\3, 1, 1, 0};
\node(t311b) [CollTypeExampleStyle, right=of t311a, xshift=6mm] {(7, 255)\\2943 $ \notin \mu$\\3, 1, 1, 127};
\draw [dotted] (t311a) -- (t311b);
\node(t312a) [CollTypeExampleStyle, right=of t311b, xshift=2mm, yshift=-3mm] {(7, 256)\\2944 $ \notin \mu$\\3, 1, 2, 0};
\node(t312b) [CollTypeExampleStyle, right=of t312a, xshift=6mm] {(7, 383)\\3071 $ \notin \mu$\\3, 1, 2, 127};
\draw [dotted] (t312a) -- (t312b);

\draw[line, bluestyle] (t100a.north west) -- ($(t100a.north west) - (14mm, 0mm)$);
\draw[line, bluestyle] (t110a.north west) -- ($(t110a.north west) - (14mm, 0mm)$);
\draw[line, bluestyle] ($(t110a.north west) - (14mm, 0mm)$) -- ($(t100a.north west) - (14mm, 0mm)$);
\node(dim0) [anchor=south west] at($(t100a.north west) - (14mm, 0mm)$) {\blueBox{dim 0: $[1, 1]_\mathbb{N}, \delta.B_0=1$}};

\draw[line, yellowstyle] (t100a.south west) -- ($(t100a.south west) - (12mm, 0mm)$);
\draw[line, yellowstyle] (t110a.south west) -- ($(t110a.south west) - (12mm, 0mm)$);
\draw[line, yellowstyle] ($(t110a.south west) - (12mm, 0mm)$) -- ($(t100a.south west) - (12mm, 0mm)$);
\node(dim1) [anchor=north west] at($(t110a.south west) - (12mm, 0mm)$) {\yellowBox{dim 1: $[0, 1]_\mathbb{N}, \delta.B_1=2$}};

\draw[line, violetstyle] (t002a.north west) -- ($(t002a.north west) + (0mm, 15mm)$);
\draw[line, violetstyle] (t002b.north west) -- ($(t002b.north west) + (0mm, 15mm)$);
\draw[line, violetstyle] ($(t002a.north west) + (0mm, 15mm)$) -- ($(t002b.north west) + (0mm, 15mm)$);
\node(dim2) [anchor=south] at($(t002a.north west)!0.5!(t002b.north west)+(0mm, 15mm)$) {\violetBox{dim 2: $[2, 2]_\mathbb{N}, \delta.B_2=1$}};

\draw[line, redstyle] (t002a.north east) -- ($(t002a.north east) + (0mm, 4mm)$);
\draw[line, redstyle] (t002b.north east) -- ($(t002b.north east) + (0mm, 4mm)$);
\draw[line, redstyle] ($(t002a.north east) + (0mm, 4mm)$) -- ($(t002b.north east) + (0mm, 4mm)$);
\node(dim3) [anchor=south east] at($(t002b.north east)+(0mm, 4mm)$) {\redBox{dim 3: $[0, 127]_\mathbb{N}, \delta.B_3=128$}};

\draw[thick, <->, bluestyle] ($(t001a.north west) - (3mm, 0mm)$) -- ($(t311a.south west) - (3mm, 0mm)$);
\node(D0) [anchor=center] at($(t001a.north west)!0.5!(t311a.south west) - (3mm, 0mm)$) {\blueBox{$\delta.D_0=4$}};

\draw[thick, <->, yellowstyle] ($(t201a.north east)+(3mm, 0mm)$) -- ($(t211a.south east)+(3mm, 0mm)$);
\node(D1) [anchor=center] at($(t201a.north east)!0.5!(t211a.south east)+(3mm, 0mm)$) {\yellowBox{$\delta.D_1=2$}};

\draw[thick, <->, violetstyle] ($(t210a.south west)-(0mm, 2mm)$) -- ($(t212b.south east)-(0mm, 2mm)$);
\node(D2) [anchor=center] at($(t210a.south west)!0.6!(t212b.south east)-(0mm, 2mm)$) {\violetBox{$\delta.D_2=3$}};

\draw[thick, <->, redstyle] ($(t202a.south west) + (2mm, -1.3mm)$) -- ($(t202b.south east) + (-2mm, -1.3mm)$);
\node(D3) [anchor=center] at($(t202a.south west)!0.5!(t202b.south east) + (0mm, -1.3mm)$) {\redBox{$\delta.D_3=128$}};

\node(legend) [rectangle, draw=black, text centered, text width=5cm, above=of t000a, xshift=40mm, yshift=2mm] {(cluster\_ctarank, threadIdx.x)\\local thread index $\in \mu$\\coordinates};
\end{tikzpicture}
\caption{Example of a thread collective being described by the collective type $\delta$ with $\delta.B = (1, 2, 1, 128)$ and $\delta.D = (4, 2, 3, 128)$, which is a collective type in aligned form (def~\ref{sec:gAlignedForm}). \lighttt{clusterDim = 8} and \lighttt{blockDim = 384}. $\mu = \{ \mathsf{toLocal}(\delta.D, c) \mid c \in [1, 1]_\mathbb{N} \times [0, 1]_\mathbb{N} \times [2, 2]_\mathbb{N} \times [0, 127]_\mathbb{N}\}$ (def~\ref{sec:gToLocal}). This is one warpgroup per CTA of a CTA pair.}
\label{fig:CollTypeExample}
\end{figure}

\subsection{Collective Type Reshape}
\label{sec:CollTypeReshape}

Reshaping a collective type means to apply a series of dimension split operations to yield a new collective type.
We split the $k^{th}$ dimension of a collective type by a factor $f \in \mathbb{N}$ by
\begin{itemize}
  \item Inserting the coordinate pair $(\delta.D_k / f, f)$ in place of $\delta.D_k$.
  \item Inserting the coordinate pair $(\delta.B_k / f, f)$ in place of $\delta.B_k$, if $\delta.B_k \ne \top$ and $\delta.B_k \ge f$.
  \item Inserting the coordinate pair $(1, \delta.B_k)$ in place of $\delta.B_k$, if $\delta.B_k \ne \top$ and $\delta.B_k < f$.
  \item Inserting the coordinate pair $(\top, \top)$ in place of $\delta.B_k$, if $\delta.B_k = \top$.
\end{itemize}
e.g. if $\delta.D = (4, 384)$ and $\delta.B = (1, 128)$, then splitting dimension 1 by $32$ gives $\delta.D = (4, 12, 32)$ and $\delta.B = (1, 4, 32)$.

\subsection{Collective Unit to Collective Type}
\label{sec:CollUnit}

Collective units are also parameterized by a pair of $M$-tuples (domain and box), with coordinates being integer expressions of \lighttt{blockDim} and \lighttt{clusterDim}, or $\top$ in the case of the box.
This is represented in the frontend language as a Python \lighttt{CollUnit} object (the code consistently abbreviates ``collective'' as ``coll''), with $\top$ represented by \lighttt{None}.
We describe this further in def~\ref{sec:gCollUnit}.

\FloatBarrier
\newpage
\useMainSub
\section{Collective Tiling}
\label{sec:CollTiling}

The \lighttt{cuda\_tasks} loops assign instances of device tasks (def~\ref{sec:gDeviceTask}) to different clusters on the system, and the user has no control (yet) over the mapping between device tasks and clusters.
On the other hand, the \lighttt{cuda\_threads} loop, which assigns work to threads within a cluster, provides the user with tight control over this work mapping.

Each statement at CUDA scope (def~\ref{sec:gCudaScope}) has a deduced \myKeyA{collective tiling} attribute.
The collective tiling describes an arrangement of the threads in the cluster into a multidimensional space, in the same manner as a collective type's domain (def~\ref{sec:gDomain}), and groups \lighttt{cuda\_threads} loop iterators by the dimension they operate on.
The statement's collective tiling may be converted to
\begin{itemize}
  \item an \myKeyA{output collective type}.
    The thread collective assigned to execute an instance of this statement will always be described (Section~\ref{sec:CollTypeThreadCollective}) by this collective type.
  \item a \myKeyA{thread mapping function} of type $\Sigma_c \to \mathcal{P}(\mathbb{N})$; this converts the control environment $\sigma_c:\Sigma_c$ (i.e. $\sigma_c: \mathbb{Y} \to \mathbb{Z}$; the per-control-variable values) to the local thread indices (def~\ref{sec:gLocalThreadIndex}) of the thread collective assigned to execute an instance of this statement.
  NB this is using syntax from the PLDI submission.
  \item a per-\lighttt{cuda\_threads} iterator \myKeyA{thread pitch}: $\mathbb{N}$ (def~\ref{sec:gThreadPitch}).
  \item a per-\lighttt{cuda\_threads} iterator \myKeyA{tiled dimension index}: $\mathbb{N}_\bot$.
\end{itemize}
The \myKeyA{domain} and \myKeyA{box} of a collective tiling $\omega$ are that of its output collective type, and for brevity, we denote it as $\omega.D$ and $\omega.B$ respectively.

\subsection{Collective Tiling State}
\label{sec:CollTilingState}

A collective tiling $\omega: \Omega$ (of some dimensionality $M$) is an $M$-tuple of \myKeyA{collective dimension descriptors} $(\omega_0 ... \omega_{M-1})$.
Each descriptor $\omega_i$ consists of
\begin{itemize}
  \item $\omega_i.n: \mathbb{N}$, dimension extent
  \item $\omega_i.\textit{ops}: \mathcal{O}^*$, tuple of \myKeyA{collective dimension operators}
\end{itemize}
where $\mathcal{O}$ is $\mathbb{Y} \times \mathbb{N}^3$, with each attribute denoted as
\begin{itemize}
  \item $\omega_i.\textit{ops}_j.\textit{iter}: \mathbb{Y}$ (name of a control variable; no two ops in $\omega$ can have the same \textit{iter})
  \item $\omega_i.\textit{ops}_j.\textit{offset}: \mathbb{N}$
  \item $\omega_i.\textit{ops}_j.\textit{box}: \mathbb{N}$
  \item $\omega_i.\textit{ops}_j.\textit{tileCount}: \mathbb{N}$
\end{itemize}
The \myKeyA{output collective type} of $\omega$ is a collective type $\delta = (\omega.D, \omega.B)$ defined by
\begin{itemize}
  \item $\omega.D_i = \omega_i.n$
  \item $\omega.B_i$ is $\omega_i.n$ if $\omega_i.\textit{ops}$ is empty, otherwise it is the $\textit{box}$ of the last op.
  \item (recall dimension thread pitch values $\omega.P_i$ are implicit, as in def~\ref{sec:gThreadPitch}).
\end{itemize}
The \myKeyA{thread mapping function} selects, for each collective dimension descriptor $\omega_i$, an interval of size $\omega.B_i$ based on an affine transform of the values of the iterators associated with that dimension.
These intervals together select a sub-grid of the $M$-dimensional space of threads-in-cluster.

We derive the thread mapping from $\omega$ based on the function
\begin{align*}
    & \textsf{collMap}: \Omega \to \Sigma_c \to \mathcal{P}(\mathbb{N}) \\
    & (\omega_0, ..., \omega_{M-1}) \mapsto \sigma_c \mapsto \{ \textsf{toLocal}(\omega.D, c) \mid
      c \in [x_0, x_0 + \omega.B_0 - 1]_\mathbb{N} \times ... \times
            [x_{M-1}, x_{M-1} + \omega.B_{M-1} - 1]_\mathbb{N} \} \\
    & \text{ with } x_i = \sum_{\textit{op} \in \omega_i.\textit{ops}} \textit{op}.\textit{offset} + \sigma_c(\textit{op}.\textit{iter}) \textit{op}.\textit{box}
\end{align*}
where \textsf{toLocal} is def~\ref{sec:gToLocal} and the actual thread mapping function is $\textsf{collMap}(\omega)$ (partial evaluation).

For a given $y: \mathbb{Y}$, let $\textit{op}_y = \omega_i.\textit{ops}_j$ such that $y = \textit{op}_y.\textit{iter}$, if it exists.
The \myKeyA{thread pitch} and \myKeyA{tiled dimension index} of $y: \mathbb{Y}$ as defined by $\omega$ is
\begin{itemize}
  \item 0 and $i$, if $\textit{op}_y$ exists and $\textit{op}_y.\textit{tileCount} \le 1$
  \item $(\omega.P_i)(\textit{op}_y.\textit{box})$ and $i$, if $\textit{op}_y$ exists and $\textit{op}_y.\textit{tileCount} > 1$
  \item 0 and $\bot$ if $\textit{op}_y$ does not exist.
\end{itemize}

\subsection{Collective Tiling Reshape \& Domain Completion}
\label{sec:CollTilingReshape}

Similar to collective types (Section~\ref{sec:CollTypeReshape}), collective tilings may be reshaped by splitting dimensions.
We split the $k^{th}$ dimension of $\omega$ by a factor $f: \mathbb{N}$ by replacing the collective dimension descriptor $\omega_k$ with a pair $(\omega_\text{hi}, \omega_\text{lo})$ defined by
\begin{itemize}
  \item $\omega_\text{hi}.n = \omega_k.n / f$
  \item $\omega_\text{hi}.\textit{ops}$ is all $\textit{op} \in \omega_k.\textit{ops}$ with $\textit{op}.\textit{box} \ge f$, modified by dividing both $\textit{op}.\textit{box}$ and $\textit{op}.\textit{offset}$ by $f$. This fails if any division gives a non-integer.
  \item $\omega_\text{lo}.n = f$
  \item $\omega_\text{lo}.\textit{ops}$ is all $\textit{op} \in \omega_k.\textit{ops}$ with $\textit{op}.\textit{box} < f$, unmodified.
\end{itemize}

This mirrors Section~\ref{sec:CollTypeReshape}, in that this results in the single domain coordinate $\omega.D_k$ being replaced with $\omega.D_k / f$ and $f$.
We use the function $\textsf{domainCompletion}: \Omega \times \Delta \to \Omega \times \Delta$ (def~\ref{sec:gDomainCompletion}) to reshape a collective tiling and a collective type so that they have the same domain.

\subsection{Collective Type Matching}
\label{sec:CollTilingCollTypeMatching}

Given a collective type $\delta_0$ and collective tiling $\omega_0$, we can query if $\omega_0$ \myKeyA{matches} $\delta_0$.
Let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
Then the query returns true if domain completion succeeded and $\omega.B = \delta.B$.
This certifies that statements with collective tiling $\omega_0$ are at $\delta_0$-scope (def~\ref{sec:gCollType}).

\subsection{Derived Collective Tilings}
\label{sec:DerivedCollTiling}

The collective tiling of each statement is the same as that of its parent, except that \lighttt{with CudaDeviceFunction}, \lighttt{for cuda\_threads}, and \lighttt{with CudaWarps} assign a new collective tiling for their children.
The latter two are defined based on the \textsf{deriveCollTiling} function (def~\ref{sec:gDeriveCollTiling}); here we just summarize the collective tilings.

\subsection{CudaDeviceFunction}
\label{sec:CollTilingCudaDeviceFunction}

If \lighttt{clusterDim = 1}, then $\omega$ is one-dimensional, with $\omega_0.n = \lighttt{blockDim}$.
Otherwise, $\omega$ is two-dimensional, with $\omega_0.n = \lighttt{clusterDim}$ and $\omega_1.n = \lighttt{blockDim}$.
In both cases, $\omega_*.\textit{ops}$ is empty.

\subsection{cuda\_threads Loops}
\label{sec:CollTilingCudaThreads}

A \lighttt{cuda\_threads} loop must be in the form \lighttt{for $y$ in cuda\_threads(0, $c_\text{hi}$, unit=$\tau_u$)}, with $c_\text{hi}$ a positive constant integer.
Let $\omega_\text{raw}$ be the collective tiling of the loop statement, and let $\delta_\text{raw}$ be the collective type unpacked from $\tau_u$ without alignment and without 1-padding (def~\ref{sec:gCollUnit}).
Let $(\omega, \delta) = \textsf{domainCompletion}(\omega_\text{raw}, \delta_\text{raw})$ (def~\ref{sec:gDomainCompletion}) so that $\omega$ and $\delta$ have the same domain.

The tiled dimension index $k$ is the value such that $\delta.B_k \notin \{ \top, \omega.B_k, \omega.D_k \}$.
If no such $k$ exists, then the collective tiling of the child statements is $\omega$, and $c_\text{hi}$ must be 1 (trivial tiling).

If multiple such $k$ exist, then the loop is ill-formed (ambiguous tiling).

If $k$ exists uniquely, then we must have $c_\text{hi} \delta.B_k \le \omega.B_k$, otherwise the loop is ill-formed (not enough threads).
In this case, the collective tiling $\omega'$ of the child statements is like $\omega$, but with a new $\textit{op}: \mathcal{O}$ appended to $\omega'_k.\textit{ops}$, with
\begin{itemize}
  \item $\textit{op}.\textit{iter} = y$
  \item $\textit{op}.\textit{offset} = 0$
  \item $\textit{op}.\textit{box} = \delta.B_k$
  \item $\textit{op}.\textit{tileCount} = c_\text{hi}$
\end{itemize}
so that the output collective type of $\omega'$ is the same as that of $\omega$, except that $\omega'.B_k = \delta.B_k$.
If all other box coordinates of $\omega'$ already match those of $\delta$, then the output collective type of $\omega'$ is $\delta$.
This is commonly the case, but we designed this to allow for mismatches on dimensions other than $k$ to make it easier to place CTA-in-cluster loops inside thread-in-CTA loops or \lighttt{with CudaWarps} blocks.

The generated collective tiling is more precisely defined by (def~\ref{sec:gDeriveCollTiling})
\begin{equation*}
    \mathsf{deriveCollTiling}(\omega_\text{raw}, y, \delta_\text{raw}, 0, c_\text{hi}, c_\text{hi})
\end{equation*}

\subsection{CudaWarps}
\label{sec:CollTilingCudaWarps}

% Pants-on-fire simplified explanation for the end user.
We describe this somewhat informally in terms of the \lighttt{cuda\_threads} loop behavior.
A \lighttt{with CudaWarps(lo, hi, name=...)} statement has the following defaults:
\begin{itemize}
  \item \lighttt{lo = 0} if not given.
  \item \lighttt{hi}, if not given, is the number of warps of the warp variable named.
  \item \lighttt{name} is \lighttt{""} if this is a top-level case (see below), or the same as the parent \lighttt{with CudaWarps} statement if this is a nested case.
\end{itemize}

\lighttt{with CudaWarps} statements that appear in CUDA device functions with at least two warp variables and with no other \lighttt{with CudaWarps} as a direct or indirect parent are a top-level case.
A top-level case must appear in \lighttt{cuda\_agnostic\_intact\_cta}-scope (def~\ref{sec:gCollUnit}).
The \lighttt{true\_lo} and \lighttt{true\_hi} of the statement are \lighttt{lo + p} and \lighttt{hi + p}, \lighttt{p} being the prefix of the warp variable (def~\ref{sec:gWarpVariable}).
All other cases are nested cases, which have \lighttt{true\_lo} and \lighttt{true\_hi} being the same as \lighttt{lo} and \lighttt{hi}.

The \lighttt{with CudaWarps} statement defines the collective tiling $\omega'$ of its child statements in a similar manner as \lighttt{for \_ in cuda\_threads(0, true\_hi, unit=cuda\_warp)}, except that the threads that would have executed iterations \lighttt{true\_lo} through \lighttt{true\_hi - 1} instead cooperate to execute the statement body.
The \lighttt{with CudaWarps} statement defines a dummy iterator variable $y$ and adds a new collective dimension operator to $\omega'$ for $y$ (except in case of a trivial tiling).

The generated collective tiling is more precisely defined by (def~\ref{sec:gDeriveCollTiling})
\begin{equation*}
    \mathsf{deriveCollTiling}(\omega_\text{raw}, y, \delta_\text{warp}, \texttt{true\_lo}, \texttt{true\_hi}, 1)
\end{equation*}
where $\delta_\text{warp}$ is unpacked from \lighttt{cuda\_warp} with alignment and 1-padding (def~\ref{sec:gCollUnit}).

\subsection{Collective Tiling Figure}
\label{sec:CollTilingFigure}

We will illustrate the collective tiling that annotates the inner-most statement of the following example proc.
The illustration is on a separate page.

\filbreak
\input{b_samples/for_CollTiling_figure.0.tex}

where we note that
\begin{itemize}
  \item The \texttt{\violetBox{n\_cta}} loop has a collective unit (def~\ref{sec:gCollUnit}) of \lighttt{4 * cuda\_cta\_in\_cluster\_strided(2)}, indicating that each iteration is executed cooperatively by a thread collective comprising 4 CTAs, with \lighttt{cluster\_ctarank} of the CTAs in the thread collective increasing by 2's.
  \item All \lighttt{unit} parameters are documented (def~\ref{sec:gCollUnit}).
  \item The \lighttt{CudaWarps} statement has the effect of deactivating the 0th warpgroup (def~\ref{sec:gWarpgroup}) of a CTA.
    The Exo-GPU compiler generates a hidden \lighttt{CudaWarps\_consumer\_None\_None} variable associated with this statement.
\end{itemize}

\filbreak
{
\sffamily
\begin{tikzpicture}[node distance=0mm]
\input{b_CollTiling_autogen.tex}

\node(keyText) [anchor=south west, yshift=60mm, xshift=-5mm] at(cta1.north west) {\textbf{KEY:}};
\node(keyThread) [CollTilingExampleStyle, anchor=west] at(keyText.east) {tid\\$c_0$\\$c_1$\\$c_2$};
\node(keyTid) [anchor=west, text width=40mm] at(keyThread.east) {where ``tid'' is the local thread index given by toLocal($\omega.D, (c_0, c_1, c_2)$) (def~\ref{sec:gLocalThreadIndex},~def~\ref{sec:gToLocal}).};
\node(keyCta) [anchor=north west, yellowstyle, yshift=-2mm] at(keyThread.south west) {\texttt{cluster\_ctarank} (def~\ref{sec:gCluster})$\rightarrow$};

\node(ops_2_2_value) [anchor=south east, bluestyle, text width=80mm, yshift=+35mm] at(t_0_0_383.north east)  {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=128, \textit{iter}=\texttt{t}$};
\node(ops_2_1_value) [anchor=south east, bluestyle, text width=80mm, yshift=+1mm] at(ops_2_2_value.north east)  {\rmfamily $\textit{offset}=0$, $\textit{box}=128$, $\textit{tileCount}=2, \textit{iter}=\texttt{wg}$};
\node(ops_2_0_value) [anchor=south east, bluestyle, text width=80mm, yshift=+1mm] at(ops_2_1_value.north east)  {\rmfamily $\textit{offset}=128$, $\textit{box}=256$, $\textit{tileCount}=1$,\\$\textit{iter}=\texttt{CudaWarps\_consumer\_None\_None}$};
\node(ops_1_0_value) [anchor=south east, violetstyle, text width=80mm, yshift=+1mm] at(ops_2_0_value.north east) {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=2$, $\textit{iter}=\texttt{n\_cta}$};
\node(ops_0_0_value) [anchor=south east, greenstyle, text width=80mm, yshift=+1mm] at(ops_1_0_value.north east) {\rmfamily $\textit{offset}=0$, $\textit{box}=1$, $\textit{tileCount}=4$, $\textit{iter}=\texttt{m\_cta}$};

\node(ops_2_2_label) [anchor=east] at(ops_2_2_value.west) {\rmfamily $\omega_2.\textit{ops}_2$};
\node(ops_2_1_label) [anchor=east] at(ops_2_1_value.west) {\rmfamily $\omega_2.\textit{ops}_1$};
\node(ops_2_0_label) [anchor=east] at(ops_2_0_value.west) {\rmfamily $\omega_2.\textit{ops}_0$};
\node(ops_1_0_label) [anchor=east] at(ops_1_0_value.west) {\rmfamily $\omega_1.\textit{ops}_0$};
\node(ops_0_0_label) [anchor=east] at(ops_0_0_value.west) {\rmfamily $\omega_0.\textit{ops}_0$};

\node(dim2) [anchor=north west, xshift=-50mm] at(ops_2_0_value.north west) {\textbf{Dim 2:} $\omega_2.n = 384$};
\node(dim1) [anchor=north west, xshift=-50mm] at(ops_1_0_value.north west) {\textbf{Dim 1:} $\omega_1.n = 2$};
\node(dim0) [anchor=north west, xshift=-50mm] at(ops_0_0_value.north west) {\textbf{Dim 0:} $\omega_0.n = 4$};
\node(CollTiling) [draw=black, anchor=east, text width=20mm] at(dim1.west) {CollTiling\\$\omega: \Omega$ state};

\node(domain) [anchor=north east, yshift=-2mm, text width=50mm] at(dim2.south east) {The domain $\omega.D = (4, 2, 384)$ is given by ($\omega_0.n$, $\omega_1.n$, $\omega_2.n$) (def~\ref{sec:gDomain}).};

\end{tikzpicture}
}

\FloatBarrier
\newpage
\section{Distributed Memory}
\label{sec:DistributedMemory}

Both barrier and data allocations in CUDA scope are subject to distributed memory analysis, although the rules are somewhat different for the two.
For barrier allocations, all dimensions are distributed.
For data allocations, the memory type specifies a collective unit (the \myKeyA{native unit}) from which a collective type $\delta_0$ is unpacked with alignment and 1-padding (def~\ref{sec:gCollUnit}); this must not be agnostic (def~\ref{sec:gAgnostic}).
Exo-GPU then deduces a certain number of distributed dimensions (say, $R$) so that each slice $x[c_0,...,c_{R-1},:,...,:]$ is allocated on a different thread collective described by $\delta_0$ (Section~\ref{sec:CollTypeThreadCollective}).
The distributed dimensions are always to the left of non-distributed dimensions.
During codegen, Exo-GPU erases indicies and array extents corresponding to distributed dimensions.
The codegen functions for memory and instructions only see indices and extents corresponding to non-distributed dimensions.

\subsection{Collective Indexing Pairs}
\label{sec:CollIndexingPairs}

Distributed memory analysis for a variable is based on the collective tiling $\omega_0^\text{alloc}$ (Section~\ref{sec:CollTiling}) of the variable allocation statement, and the set of \myKeyA{collective indexing pairs} $\Omega \times \mathsf{Expr}^*$ collected from all statements and expressions that index the variable:
\begin{itemize}
  \item For read expressions ``$x[e^*]$'' not in the context of an instruction call, the collective indexing pair is $(\omega, e^*)$, where $\omega$ is the collective tiling of the statement containing the read expression.
  \item For write statements ``$x[e^*] \texttt{= \_}$'' and reduce statements ``$x[e^*] \texttt{+= \_}$'', the collective indexing pair is $(\omega, e^*)$, where $\omega$ is the collective tiling of the statement.
  \item For sync statements, the collective indexing pair is $(\omega, e^*)$, where $\omega$ is the collective tiling of the statement and $e^*$ is extracted from the home barrier expression $z[e^*]$ (def~\ref{sec:gHomeBarrier}).
  \item TODO describe instructions.
\end{itemize}

\subsection{Thread Pitch Requirement}
\label{sec:DistributedMemoryThreadPitch}

For each collective indexing pair, we deduce a thread pitch tuple.
The deduced number of distributed dimensions ($R$) is the length of this tuple.
All collective indexing pairs must lead to the same deduced tuple.

We will soon define what a \myKeyA{required iterator} is separately for data and barriers.
A \myKeyA{required index expression} is a plain read of a required iterator.
A \myKeyA{permitted index expression} is a plain read of a required iterator, or of a \lighttt{cuda\_threads} iterator with 0 thread pitch, as defined by $\omega$ (Section~\ref{sec:CollTilingState}).

\mainKey{Data:} A collective index pair $(\omega_0, e^*)$ is first updated with domain completion (def~\ref{sec:gDomainCompletion}) so that the memory type's collective type has the same domain as the collective tiling.
Let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.

Dimension $i$ is an \myKeyA{intact dimension} if $\delta.B_i = 1$.
An iterator is a \myKeyA{required iterator} if all these conditions apply:
\begin{itemize}
  \item appears in $\omega$
  \item does not appear in $\omega_0^\text{alloc}$
  \item its tiled dimension index is that of an intact dimension
  \item its thread pitch is not 0
\end{itemize}
with ``tiled dimension index'' and ``thread pitch'' as defined by $\omega$ (Section~\ref{sec:CollTilingState}).
The number of distributed dimensions $R$ is the lowest possible value $R$ such that $e_0, ..., e_{R-1}$ consists only of permitted index expressions and all required index expressions appear exactly once (fail if this is not possible).
The thread pitch tuple is $(g(e_0), ..., g(e_{R-1}))$ where $g(e)$ gives the thread pitch (as defined by $\omega$) of the iterator indexed by the permitted index expression $e$.

\mainKey{Barrier:} A collective index pair $(\omega, e^*)$ is processed without domain completion.
An iterator is a \myKeyA{required iterator} if all these conditions apply:
\begin{itemize}
  \item appears in $\omega$
  \item does not appear in $\omega_0^\text{alloc}$
  \item its thread pitch is not 0
\end{itemize}
with ``thread pitch'' as defined by $\omega$ (Section~\ref{sec:CollTilingState}).
The number of distributed dimensions $R$ is the number of index expressions $e^*$, which must consist only of permitted index expressions, and all required index expressions must appear exactly once (fail if this is not the case).
The thread pitch tuple is $(g(e_0), ..., g(e_{R-1}))$ where $g(e)$ gives the thread pitch (as defined by $\omega$) of the iterator indexed by the permitted index expression $e$.

\subsection{Range Requirement}
\label{sec:DistributedMemoryRange}

The iterator (control variable) used to index a distributed dimension must have range $[0, x_i - 1]_\mathbb{N}$ where $x_i$ is the extent of the array on that dimension.

\subsection{Base Threads Requirement}
\label{sec:DistributedMemoryBaseThreads}

The thread pitch tuple requirement will diagnose many cases of inconsistent sharding, but will not detect ``offset-only'' mismatches, such as $x[0], x[1], x[2]...$ being accessed by threads 0, 1, 2, ... in one usage and threads 64, 65, 66, ... in another usage (as could occur with warp specialization).
The base threads requirement addresses this issue.
We will define the \myKeyA{base offset} and \myKeyA{base box} of a collective tiling separately for data and barrier variables.

\mainKey{Barrier:} Given an $M$-dimensional collective tiling $\omega$, the \myKeyA{base offset} is defined by the tuple $(O_0, ..., O_{M-1})$ where $O_i = \sum_{j} \omega_i.\textit{ops}_j.\textit{offset}$ (Section~\ref{sec:CollTilingState}).
The \myKeyA{base box} is $\omega.B$.

\mainKey{Data:} Given a collective tiling $\omega_0$ and the collective type $\delta_0$ unpacked from the native unit, let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
The \myKeyA{base offset} $O$ and the \myKeyA{base box} $\textit{BB}$ are both $M$ tuples, $M$ being the dimensionality of $\omega$.
The $i^{th}$ dimension is a \myKeyA{intact dimension} if $\delta.B_i = \delta.D_i$.
For intact dimensions $i$, $O_i = 0$ and $\textit{BB}_i = \delta.D_i$.
For other dimensions, $O_i = \sum_{j} \omega_i.\textit{ops}_j.\textit{offset}$ (Section~\ref{sec:CollTilingState}) and $\textit{BB}_i = \omega.B_i$.

\mainKey{Base Offset Equivalence:} Compute the linear offset of $\omega$ as $\sum_{i} O_i (\omega.P_i)$, where $O$ denotes the base offset derived from $\omega$, and $\omega.P_i$ denotes the dimension thread pitch (def~\ref{sec:gThreadPitch}).
Base offset equivalence means that the linear offset computed for two collective tilings are the same.

\mainKey{Base Box Equivalence:} Given collective tilings $\omega_1$ and $\omega_2$ with base boxes $\textit{BB}_1$ and $\textit{BB}_2$ respectively, the two collective tilings have equivalent base boxes when $\delta_1 = \delta_2$ given $(\delta_1, \delta_2) = \mathsf{domainCompletionTypeOnly}((\omega_1.D, \textit{BB}_1), (\omega_2.D, \textit{BB}_2))$ (def~\ref{sec:gDomainCompletionTypeOnly}).

\mainKey{Barrier Requirements:} The base threads requirement is met for two collective indexing pairs when their two collective tilings satisfy base offset equivalence and base box equivalence.
The specific pairs that must satisfy this requirement varies depending on the barrier mechanism (def~\ref{sec:gBarrierMechanism}).

\mainKey{Data Requirements:} Any two collective indexing pairs for a given data variable must have their two collective tilings satisfy base offset equivalence and base box equivalence.


\subsection{Box Size Requirements}
\label{sec:DistributedMemoryBoxSize}

These additional requirements for data allocations only ensure the storage for each shard is truly allocated by a thread collective described by $\delta_0$.

\mainKey{Alloc Box Requirement:} Let $(\omega^\text{alloc}, \delta) = \mathsf{domainCompletion}(\omega_0^\text{alloc}, \delta_0)$.
The $i^{th}$ dimension is an \myKeyA{intact dimension} if $\delta.B_i = \delta.D_i$.
For each intact dimension $i$, we must have $\omega^\text{alloc}.B_i = \delta.B_i$.

\mainKey{Usage Box Requirement:} For each collective indexing pair $(\omega_0, e^*)$, let $(\omega, \delta) = \mathsf{domainCompletion}(\omega_0, \delta_0)$.
The $i^{th}$ dimension is a \myKeyA{subdivided dimension} if $\delta.B_i = 1$.
Given $\delta_0$ is in aligned form (def~\ref{sec:gAlignedForm}) and not agnostic (def~\ref{sec:gAgnostic}), this is mutually exclusive with being an intact dimension.
For each subdivided dimension $i$, we must have $\omega.B_i = 1$.

\FloatBarrier
\newpage
\section{Synchronization Usage}
\label{sec:SyncUsage}

We view each statement instance (def~\ref{sec:gStmtInstance}) as issuing reads or mutates to a set of array elements; the reads and mutates are executed by a thread collective (def~\ref{sec:gThreadCollective}) with a certain \myKeyA{initial qualitative timeline} (Section~\ref{sec:InstrTL}), which varies based on the issued instruction.
The purpose of the qualitative timeline is to model that the read or mutate is only synchronized by a subset of possible synchronization statements.
These synchronization statements (def~\ref{sec:gSyncStmt}) are parameterized with a \myKeyA{first synchronization timeline} $\tau_s^\mathrm{pre}$ and/or a \myKeyA{second synchronization timeline} $\tau_s^\mathrm{post}$, like so:

\hphantom{spacing}
\texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}
\hfill
\texttt{Arrive($\tau_s^\mathrm{pre}$) >}\texttt{> $e$}
\hfill
\texttt{Await($e, \tau_s^\mathrm{post}, n$)}
\hphantom{spacing}

where $e$ is a barrier expression (def~\ref{sec:gBarrierExpr}) and $n$ is an integer, which controls \lighttt{Arrive}/\lighttt{Await} pairing (Section~\ref{sec:ArriveAwaitPairing}).

The synchronization timeline (def~\ref{sec:gSyncTL}) contains a \myKeyA{full timeline set} $\tau_s.\mathrm{full}$ and \myKeyA{temporal timeline set} $\tau_s.\mathrm{temp}$ (both sets of \textsf{QualTL}), which filter (by qualitative timeline) the reads and mutates that the synchronization statement interacts with.

For the purposes of reasoning about correct synchronization, each executed read or mutate has additional attributes
\begin{itemize}
  \item extended qualitative timelines (set of \textsf{QualTL}), Section~\ref{sec:InstrTL}
  \item an atomic qualitative timeline, if the access is atomic, Section~\ref{sec:AtomicInstr}
  \item convergence flag, Section~\ref{sec:InstrConvergentAccess}.
  \item out-of-order flag, TODO
\end{itemize}
If the convergence flag is true, then the thread collective for each read and mutate executed by the statement instance is the thread collective assigned to the statement instance itself.
Otherwise (i.e. the access is non-convergent), we view each read and mutate as being done by one thread only, repeated for each thread in the thread collective (Section~\ref{sec:VisRecordCreation}).
This models the uncertainty in there being no way to ascribe responsibility for that access to a particular thread, rather than that the access is literally repeated in the underlying hardware.

In Exo-GPU, the user's job is to insert sufficient synchronization so that \myKeyA{sequential-parallel equivalence} is upheld, that is, for a given input, the value computed by interpreting the Exo-GPU program with sequential Exo value semantics will be the same as that computed by the parallelized CUDA program.
The sync-check step (whose ``abstract machine semantics'' are specified in Section~\ref{sec:SyncSemantics}) validates this.

Although this is not how the abstract machine formally works, it's useful to think of Exo-GPU synchronization in terms of dependency edges in a graph formed from a sequential trace of the Exo-GPU program.
The nodes are executed reads to array elements, executed mutates to array elements, and executed synchronization statement instances (with read/mutate nodes generated from statement instances as described above).
The following subsections will specify intended usage patterns of Exo-GPU.
These consist of two nodes appearing in the sequential trace, with dependency edges being formed between nodes involved in an intended usage pattern.

It must be possible to find a connection using dependency edges between one read/mutate to an array element and another read/mutate to that same array element, except when one of the following apply:
\begin{itemize}
    \item the accesses are both reads
    \item the array element is stored in sync-exempt memory (def~\ref{sec:gSyncExempt})
    \item the accesses are both from the same statement instance (due to non-convergent access)
\end{itemize}
While describing the intended usage patterns, we will also jump ahead and specify the rules in Section~\ref{sec:SyncSemantics} that are involved in detecting that pattern for sync-check.
This information is not required for understanding this section.

\subsection{Intended Usage Patterns -- Access Before Synchronization}
\label{sec:AccessBeforeSync}

\begin{equation*}
    \texttt{Read|Mutate} \to \texttt{Arrive(}\tau_s^\mathrm{pre}\texttt{)... | Fence(}\tau_s^\mathrm{pre},\texttt{...)}
\end{equation*}

A dependency edge forms between a read or mutate followed by an \lighttt{Arrive} or \lighttt{Fence} statement instance, where the initial qualitative timeline $q$ of the read or mutate is a member of $\tau_s^\mathrm{pre}.\mathrm{full}$, and there is a thread in common executing the two.
The synchronization timeline $\tau_s^\mathrm{pre}$ (def~\ref{sec:gSyncTL}) and barrier mechanism to use varies depending on the instruction issuing the read or mutate:
\begin{itemize}
  \item For non-async cuda instructions, use \lighttt{cuda\_in\_order} and a \lighttt{Fence} (Section~\ref{sec:FenceUsage}) or mbarrier (Section~\ref{sec:MbarrierUsage}).
  \item For sm\_80 non-bulk \lighttt{cp.async} instructions (Ampere), use \lighttt{Sm80\_cp\_async} and a commit group (Section~\ref{sec:CommitGroupUsage}) or mbarrier (Section~\ref{sec:MbarrierUsage}).
  \item For TMA instructions copying into SMEM, use mbarriers (Section~\ref{sec:MbarrierUsage}).
  \item For wgmma instructions and TMA instructions copying out of SMEM, use commit groups (Section~\ref{sec:CommitGroupUsage}).
\end{itemize}
This pattern is implemented by storing a new \textsf{VisRecord} (Section~\ref{sec:VisRecordCreation}) containing a timeline signature with one of the executing threads, the initial qualitative timeline $q$, and visibility flag $\mathsf{VF_{issue}}$, which the \lighttt{Arrive} or \lighttt{Fence} will witness (Section~\ref{sec:Witness}).

\subsection{Intended Usage Patterns -- Access After Synchronization}
\label{sec:AccessAfterSync}

\begin{equation*}
    \texttt{Await(..., $\tau_s^\mathrm{post}$, ...)|Fence(..., $\tau_s^\mathrm{post}$)} \to \texttt{Read|Mutate}
\end{equation*}

A dependency edge forms between an \lighttt{Await} or \lighttt{Fence} statement instance and a subsequent read or mutate when there is a thread in common executing the two, and either of
\begin{itemize}
  \item The access is a read, or a read-write mutate, and there is a qualitative timeline $q$ in common between $\tau_s^\mathrm{post}.\mathrm{full}$ and the extended qualitative timelines (Section~\ref{sec:InstrTL}) of the read or mutate.
  \item The access is a write-only mutate, and there is a qualitative timeline $q$ in common between $\tau_s^\mathrm{post}.\mathrm{temp}$ and the extended qualitative timelines (Section~\ref{sec:InstrTL}) of the mutate.
  This is ``temporal-only'' synchronization, which may elide proxy fences.
  \item The ``access'' is as a result of freeing shared memory (Section~\ref{sec:ChecksOnFree}).
  This is needed to safely alias future allocations onto the freed physical SMEM, whose effects we overapproximate as a non-convergent write-only mutate by each thread in the cluster.
\end{itemize}

The sychronization timeline $\tau_s^\mathrm{post}$ (def~\ref{sec:gSyncTL}) to use varies depending on the instruction issuing the read or mutate:
\begin{itemize}
  \item \lighttt{cuda\_in\_order} if the following reads and mutates are only issued by generic proxy instructions: non-async CUDA instructions and sm\_80 non-bulk \lighttt{cp.async}.
  These instructions include \lighttt{cuda\_in\_order\_ram\_qual} (def~\ref{sec:gQualTL}) in their extended timeline set for non-register parameters.
  \item \lighttt{cuda\_generic\_and\_async\_proxy} if any async proxy instructions are involved (TMA, wgmma, tcgen05); these instructions include \lighttt{cuda\_async\_proxy\_retired} (def~\ref{sec:gQualTL}) in their extended timeline set for non-register parameters.
  \item \lighttt{cuda\_temporal} may also be used if only temporal-only synchronization is required.
\end{itemize}
As a special case, \lighttt{Fence(wgmma\_fence\_1, wgmma\_fence\_2)} at \lighttt{cuda\_warpgroup}-scope (def~\ref{sec:gCollUnit}) should be used before any accesses to wgmma registers, even if no prior access occurs (this is \emph{not} enforced by sync-check, but will result in a ptxas warning and suboptimal CUDA code if violated).

With $(\iota, n)$ denoting a thread in common, and with
\begin{itemize}
    \item $\mathsf{vf} = \mathsf{VF_{temp}}$ (def~\ref{sec:gVisFlag}) and $q$ denoting a qualitative timeline in common to $\tau_s^\mathrm{post}.\mathrm{temp}$ and the extended timeline set, in the case of temporal-only synchronization.
    \item $\mathsf{vf} = \mathsf{VF_{full}}$ for non-atomic acceses or $\mathsf{vf} = \mathsf{VF_{atom}}$ for atomics, and $q$ denoting a qualitative timeline in common to $\tau_s^\mathrm{post}.\mathrm{full}$ and the extended timeline set, in the other case.
\end{itemize}
this pattern is implemented in sync-check by the augment step (Section~\ref{sec:Augment}) adding the timeline signature $((\iota, n), q, \mathsf{vf})$ to visibility records, which satisfy the checking requirements for the subsequent read (Section~\ref{sec:ChecksOnRead}) or mutate (Section~\ref{sec:ChecksOnMutate}).


\subsection{Intended Usage Patterns -- Arrive-Await Pairing, Trailing Barrier Expr}
\label{sec:ArriveAwaitPairing}

\begin{gather*}
    \texttt{Arrive(...) >> $e_a$} \to \texttt{Await(..., $e_w$, $n$)} \\
    \texttt{Read|Mutate >}\texttt{> } e_t \to \texttt{Await(..., $e_w$, $n$)}
\end{gather*}

In the Exo-GPU abstraction, each barrier array element contains its own arrive count and await count, both initially 0 (Section~\ref{sec:SyncEnv}).
The \lighttt{Arrive} statement may reference multiple barrier array elements (say, $B$-many), using multiple barrier expressions (def~\ref{sec:gBarrierExpr}) separated by \texttt{>}\texttt{>}.
Each ``batch'' of $B$-many instances of such an \lighttt{Arrive} statement (issued with $B$-many different thread collectives) results in incrementing the arrive count of each referenced barrier array element by 1.
This includes the common case where $B=1$.

The \lighttt{Await} statement must contain only a single barrier expression $e_w$, and it must reference only one barrier array element.
The behavior of the \lighttt{Await} changes based on $n$.

If $n \ge 0$, then this is the ``arrive-indexed'' case.
With $a$ being the arrive count of the referenced barrier array element, the eligible batches are the first $(a-n)$-many batches of \lighttt{Arrive} statement instances that reference the same barrier array element.
There are dependency edges from the eligible batches to the await.

If $n = \texttt{\textasciitilde lag}$ with \texttt{\textasciitilde} being the 2's complement and $\texttt{lag} \ge 0$, then this is the ``await-indexed'' case.
With $w$ being the await count of the referenced barrier, the eligible batches are the first $(w - \texttt{lag} + 1)$-many batches of \lighttt{Arrive} statement instances that reference the same barrier array element.
There are dependency edges from the eligible batches to the await.
Afterwards, the await count of the referenced barrier is incremented by 1.

In both cases, additional dependency edges are formed from reads and mutates executed by a statement instance with a trailing barrier expression $e_t$, when $e_t$ and $e_w$ reference the same barrier array element, and the read or mutate appears in the trace prior to at least one eligible batch of \lighttt{Arrive} statement instances.

This pattern is implemented in sync-check through the shared pending await state between \lighttt{Arrive} (Section~\ref{sec:ArriveSemantics}) and \lighttt{Await} (Section~\ref{sec:AwaitSemantics}), and the initial pending awaits added for reads/mutates that include a trailing barrier expression (Section~\ref{sec:VisRecordCreation}).
The to-be-described convergence requirements for multiple barriers (Section~\ref{sec:BarrierMulticast}) are required to ensure batches of arrives execute as intended.

Scheduling Functions:
\begin{itemize}
  \item \texttt{insert\_arrive}
  \item \texttt{insert\_await}
  \item \texttt{set\_trailing\_barrier\_expr}
\end{itemize}

\subsection{Intended Usage Patterns -- Multiple Memory Accesses}
\label{sec:MultipleMemoryAccesses}

\begin{equation*}
    \texttt{Read|Mutate} \to \texttt{Read|Mutate}
\end{equation*}

A thread may read/mutate an array element and then read/mutate that element again without intervening synchronization when the first access is not out-of-order and the initial timeline signature of the first access is included in the extended timeline signature set of the second access.
This pattern is implemented in sync-check through the timeline signature (def~\ref{sec:gTlSig}) with $\mathsf{VF_{full}}$ (def~\ref{sec:gVisFlag}) being added upon interpreting a non-out-of-order read or mutate (Section~\ref{sec:VisRecordCreation}).

Two atomic mutates may also access the same array element concurrently as long as their atomic qualitative timeline (Section~\ref{sec:InstrTL}) is the same.
Sequential-parallel equivalence is only possible in this case since Exo atomics don't return the updated value.
This pattern is implemented in sync-check through the timeline signature with $\mathsf{VF_{atom}}$ that is added upon the first atomic access (Section~\ref{sec:VisRecordCreation}) and used to pass the check in the second atomic access (Section~\ref{sec:ChecksOnMutate}).

% atomic, non-ooo

\subsection{Intended Usage Patterns -- Multiple Synchronization Statements (Transitivity)}
\label{sec:Transitivity}

\begin{equation*}
    \texttt{Await(..., $\tau_s^\mathrm{post}$, ...)|Fence(..., $\tau_s^\mathrm{post}$)} \to
    \texttt{Fence($\tau_s^\mathrm{pre}$, ...)|Arrive($\tau_s^\mathrm{pre}$)...}
\end{equation*}

A dependency edge forms between an \lighttt{Await} or \lighttt{Fence} statement instance and a subsequent \lighttt{Arrive} or \lighttt{Fence} statement instance when the thread collectives executing the two statement instances have a thread in common, and there exists a qualitative timeline $q$ in the intersection of the first statement instance's $\tau_s^\mathrm{post}$ and the second statement instance's $\tau_s^\mathrm{pre}$ (def~\ref{sec:gSyncTL}).

With $(\iota, n)$ denoting a thread in common, and with $q$ as described above, this pattern is implemented in sync-check through the timeline signature $((\iota, n), q, \mathsf{VF_{issue}})$ that is added by the augment (Section~\ref{sec:Augment}) for the first statement instance and detected by the witness (Section~\ref{sec:Witness}) of the second statement instance.

% reason for async proxy retired thing

\subsection{Fence Usage}
\label{sec:FenceUsage}

\texttt{Fence($\tau_s^\mathrm{pre}$, $\tau_s^\mathrm{post}$)}

\texttt{insert\_fence(..., $\tau_s^\mathrm{pre}$, $\tau_s^\mathrm{pre}$)}

Besides the special case \lighttt{Fence(wgmma\_fence\_1, wgmma\_fence\_2)}, which must be at \lighttt{cuda\_warpgroup}-scope (def~\ref{sec:gCollUnit}) and must appear before other usages of wgmma registers, all \lighttt{Fence} statements must be of the form \lighttt{Fence($\tau_s^\mathrm{pre}$, $\tau_s^\mathrm{post}$)} with
\begin{itemize}
    \item the statement appearing at \lighttt{cuda\_thread}, \lighttt{cuda\_warp}, \lighttt{cuda\_cta\_in\_cluster}, or \lighttt{cuda\_cluster} scope (def~\ref{sec:gCollUnit}).
    \item $\tau_s^\mathrm{pre}$ being \lighttt{Sm80\_generic}, or another synchronization timeline whose timeline sets are subsets of the respective timeline sets of \lighttt{Sm80\_generic} (def~\ref{sec:gSyncTL}).
    \item $\tau_s^\mathrm{post}$ being \lighttt{cuda\_generic\_and\_async\_proxy}, or another synchronization timeline whose timeline sets are subsets of the respective timeline sets of \lighttt{cuda\_generic\_and\_async\_proxy} (def~\ref{sec:gSyncTL}).
\end{itemize}
These non-wgmma \lighttt{Fence} configurations are referred to as \myKeyA{garden-variety fences}.
% Referenced in glossary entry gGardenVarietyFence.

If the \lighttt{Fence} statement appears at \lighttt{cuda\_cluster}-scope but not at \lighttt{cuda\_cta\_in\_cluster}-scope, then it is subject to the solitary barrier requirement (Section~\ref{sec:SolitaryBarrier}).

\subsection{Cluster Sync Usage}
\label{sec:ClusterSyncUsage}

A barrier variable $z$ with cluster sync barrier mechanism may be declared with

\texttt{$z$: barrier @ CudaClusterSync}

\texttt{insert\_barrier\_alloc(..., $z$, None, [], CudaClusterSync)}

The valid synchronization timelines (def~\ref{sec:gSyncTL}) for the \lighttt{Arrive} and \lighttt{Await} using the cluster sync barrier mechanism are the same as for garden-variety fences (Section~\ref{sec:FenceUsage}).

\mainKey{Statically-checked Requirements:}
\begin{itemize}
  \item Split barrier basic requirements (Section~\ref{sec:SplitBarrierBasic})
  \item Barrier guarding requirement (Section~\ref{sec:BarrierGuarding}) with only the arrive-first configuration allowed
  \item Solitary barrier requirement (Section~\ref{sec:SolitaryBarrier})
  \item Barrier expressions (def~\ref{sec:gBarrierExpr}) must use only point indices, not intervals.
  \item \lighttt{Await} must have $n=0$.
  \item \lighttt{Arrive} and \lighttt{Await} must be at \lighttt{cuda\_cluster}-scope (def~\ref{sec:gCollUnit}).
  \item The \lighttt{Arrive} and \lighttt{Await} statements for a given barrier array element must be executed by the same \myKeyA{thread collective} (def~\ref{sec:gThreadCollective}).
    This is enforced by requiring base thread equality (Section~\ref{sec:DistributedMemoryBaseThreads}) between collective indexing pairs collected both from \lighttt{Arrive} and \lighttt{Await} statements for $z$.
\end{itemize}

% solitary, sync-exempt, barrier variable vs CUDA implicit state

\subsection{Commit Group Usage}
\label{sec:CommitGroupUsage}

A barrier variable $z$ with commit group barrier mechanism may be declared with

\texttt{$z$: barrier[$e^*$] @ CudaCommitGroup}

\texttt{insert\_barrier\_alloc(..., $z$, None, [$e^*$], CudaCommitGroup)}

where the array size $e^*$ is subject to distributed memory analysis (Section~\ref{sec:DistributedMemory}).
In CUDA, the commit group is implicit state local to a thread, warp, or warpgroup; however in Exo-GPU we explicitly shard this barrier state onto the implicit thread state with distributed memory analysis.
This motivates the solitary barrier requirement (Section~\ref{sec:SolitaryBarrier}).

The synchronization timelines (def~\ref{sec:gSyncTL}) $\tau_s^\mathrm{pre}$ for the \lighttt{Arrive} and $\tau_s^\mathrm{post}$ for the \lighttt{Await} must match one of the rows in this table, which also defines the expected collective unit $\tau_u$ (def~\ref{sec:gCollUnit}).

\begin{tabular}{|r|r|r|}
$\tau_s^\mathrm{pre}$ & $\tau_s^\mathrm{post}$ & $\tau_u$ \\
\texttt{Sm80\_cp\_async} & \texttt{cuda\_in\_order} & \texttt{cuda\_thread} \\
\texttt{tma\_to\_gmem\_async} & \texttt{cuda\_generic\_or\_async\_proxy} & \texttt{cuda\_warp} \\
\texttt{wgmma\_async} & \texttt{cuda\_generic\_or\_async\_proxy} & \texttt{cuda\_warpgroup}
\end{tabular}

\mainKey{Statically-checked Requirements:}
\begin{itemize}
  \item Split barrier basic requirements (Section~\ref{sec:SplitBarrierBasic})
  \item Solitary barrier requirement (Section~\ref{sec:SolitaryBarrier})
  \item Barrier expressions (def~\ref{sec:gBarrierExpr}) must use only point indices, not intervals.
  \item \lighttt{Await} must have $n \ge 0$, i.e. this is an arrive-indexed barrier (Section~\ref{sec:ArriveAwaitPairing}, Section~\ref{sec:AwaitSemantics}).
  \item \lighttt{Arrive} and \lighttt{Await} must be at $\tau_u$-scope as defined in the above table (def~\ref{sec:gCollUnit}).
  \item The \lighttt{Arrive} and \lighttt{Await} statements for a given barrier array element must be executed by the same \myKeyA{thread collective} (def~\ref{sec:gThreadCollective}).
    This is enforced by requiring base thread equality (Section~\ref{sec:DistributedMemoryBaseThreads}) between collective indexing pairs collected both from \lighttt{Arrive} and \lighttt{Await} statements for $z$.
\end{itemize}

% solitary, sync-exempt, barrier variable vs CUDA implicit state

\subsection{Mbarrier Usage}
\label{sec:MbarrierUsage}

A barrier variable $z_a$ with mbarrier barrier mechanism may be declared with

\texttt{$z_a$: barrier[$e^*$] @ CudaMbarrier}

\texttt{insert\_barrier\_alloc(..., $z_a$, None, [$e^*$], CudaMbarrier)}

\texttt{$z_a$: barrier($z_g$) [$e^*$] @ CudaMbarrier}

\texttt{insert\_barrier\_alloc(..., $z_a$, $z_g$, [$e^*$], CudaMbarrier)}

where the array size $e^*$ is subject to distributed memory analysis (Section~\ref{sec:DistributedMemory}), and the $z_g$, variable if present, means that $z_a$ is explicitly guarded-by $z_g$ (def~\ref{sec:gGuardedBy}).
This is used in the barrier guarding requirement (Section~\ref{sec:BarrierGuarding}); in short, \lighttt{Arrive} statements using $z_a$ must be matched with \lighttt{Await} statements using $z_g$.

Each barrier array element is implemented as a \emph{ring buffer} of CUDA mbarrier objects (def~\ref{sec:gMbarrierRingBuffer}), resident in a single CTA.
You cannot implement ring buffering explicitly in Exo-GPU (the indexing pattern required will not pass distributed memory analysis), which is an intentional break from Exo's imperative programming style in order to simplify sync-check.

The first synchronization timeline (def~\ref{sec:gSyncTL}) $\tau_s^\mathrm{pre}$ for the \lighttt{Arrive} should be \lighttt{cuda\_in\_order}, \lighttt{Sm80\_cp\_async}, or \lighttt{cuda\_temporal}.
The second synchronization timeline $\tau_s^\mathrm{post}$ for the \lighttt{Await} should be \lighttt{cuda\_in\_order}, \lighttt{cuda\_generic\_or\_async\_proxy}, or \lighttt{cuda\_temporal}.
(Avoid using \lighttt{cuda\_generic\_or\_async\_proxy} unless required; in particular guarding against write-after-read (WAR) hazards for a producer warp using TMA requires only \lighttt{cuda\_temporal} for the producer warp's \lighttt{Await}).

\mainKey{Statically-checked Requirements:}
\begin{itemize}
  \item Split barrier basic requirements (Section~\ref{sec:SplitBarrierBasic}).
  \item Barrier guarding requirement, either arrive-first or await-first (Section~\ref{sec:BarrierGuarding}).
  \item Barrier expressions (def~\ref{sec:gBarrierExpr}) must meet the barrier multicast requirements (Section~\ref{sec:BarrierMulticast}).
  \item Each \lighttt{Await} for a given variable $z_a$ must have the same $n$ value, and $n < 0$, i.e. this is an await-indexed barrier (Section~\ref{sec:ArriveAwaitPairing}, Section~\ref{sec:AwaitSemantics}).
  \item Distributed memory analysis (Section~\ref{sec:DistributedMemory}) must be able to map each barrier array element into only a single CTA (TODO, explain how?).
  \item Each \lighttt{Arrive} for a given barrier array element must be executed by the same \myKeyA{thread collective} (def~\ref{sec:gThreadCollective}).
    This is enforced by requiring base thread equality (Section~\ref{sec:DistributedMemoryBaseThreads}) between collective indexing pairs collected both from all \lighttt{Arrive} statements for $z_a$.
  \item The above requirement applies separately for all \lighttt{Await} statements for $z_a$.
  \item The deduced ring buffer size must be positive (def~\ref{sec:gMbarrierRingBuffer}).
\end{itemize}

% multicast, guarding, hidden ring buffer & why ...

\subsection{Barrier Multicast}
\label{sec:BarrierMulticast}

% thread pitch multiple of blockDim, multicast convergence, home barrier expr

The \lighttt{Arrive} statement may contain multiple barrier expressions (Section~\ref{sec:gBarrierExpr}), some containing intervals instead of points.
For example,
\begin{equation*}
    \texttt{Arrive(...) >> $z$[m, :] >> $z$[:, n]}
\end{equation*}
There must be a unique \myKeyA{home barrier} (def~\ref{sec:gHomeBarrier}), which is the unique barrier in the intersection of all barrier expressions.
This is determined from a deduced home barrier expression (in this case, \texttt{z[m, n]}).

In the context of a given \lighttt{Arrive} statement, an iterator (control variable) $y$ is a \myKeyA{multicast iterator} if it appears at some index $j$ of a barrier expression, and another barrier expression has an interval at index $j$.
For example,
\begin{equation*}
    \texttt{Arrive(...) >> $z$[m, n] >> $z$[m, :]}
\end{equation*}
has only \texttt{n} being a multicast iterator.

Currently only \lighttt{CudaMbarrier}-mechanism split barriers (Section~\ref{sec:MbarrierUsage}) support multicasting.
Each multicast iterator must have a thread pitch (def~\ref{sec:gThreadPitch}) that is a multiple of \lighttt{blockDim} (Section~\ref{sec:CudaDeviceFunction}).
That is, multicast iterators must correspond to \lighttt{cuda\_threads} loops that iterate in the CTA-in-cluster dimension (Section~\ref{sec:CollTiling}).

We impose a conservative convergence requirement for multicast iterators.
This is the only case in Exo-GPU where convergence is enforced other than through scoping.
Each multicast iterator is defined by some \lighttt{cuda\_threads} loop (Section~\ref{sec:CollTilingCudaThreads}).
For a given \lighttt{Arrive} statement, it must not have any parent \lighttt{if} or \emph{sequential} \lighttt{for} loop statements that are not also parents of each multicast iterator's defining \lighttt{cuda\_threads} loop (Figure~\ref{fig:BadMulticastExample}).

\begin{figure}[t]
\codehrule
\input{b_samples/bad_multicast_example.0.tex}
\caption{Multicast Convergence Requirement Violation}
\label{fig:BadMulticastExample}
\codehrule
\end{figure}

\subsection{Split Barrier Basic Requirements}
\label{sec:SplitBarrierBasic}

For a given barrier variable $z$,
\begin{itemize}
  \item At least one \lighttt{Arrive} using $z$ must exist, each using the same $\tau_s^\mathrm{pre}$ (def~\ref{sec:gSyncTL}).
  \item At least one \lighttt{Await} using $z$ must exist, each using the same $\tau_s^\mathrm{post}$ (def~\ref{sec:gSyncTL}).
  \item \lighttt{Arrive} statements must have a valid home barrier expression (def~\ref{sec:gHomeBarrier}).
  \item \lighttt{Await} statements must have only a single barrier expression (def~\ref{sec:gBarrierExpr}) with all indices being points.
  \item Two different barrier variables must not be guarded-by the same barrier variable (def~\ref{sec:gGuardedBy}).
  \item The multicasts (def~\ref{sec:gMulticasts}) of all \lighttt{Arrive} statements using $z$ must be the same.
\end{itemize}

\subsection{Barrier Guarding Requirement}
\label{sec:BarrierGuarding}

This section applies only to barrier mechanisms that list it as a requirement.

Each barrier variable is guarded-by some other barrier variable:
\begin{itemize}
    \item Declare a barrier variable $z$ with no other barrier variable being explicitly-guarded-by $z$ (def~\ref{sec:gBarrierVariable}) to have $z$ be guarded by itself.
    \item Declare a barrier variable $z_0$ with no explictly-guarded-by variable, and declare $z_1$ to be explicitly guarded-by $z_0$, and no other barrier variable being explicitly guarded-by $z_1$ to have $z_0$ and $z_1$ guard each other.
    \item See the reference for ``guarded-by'' (def~\ref{sec:gGuardedBy}) for other use cases.
\end{itemize}
For each barrier variable $z_a$ subject to the barrier guarding requirement, the Exo-GPU program must pass some static analysis, specified in the reference (def~\ref{sec:gBarrierGuardingRequirement}),  that guarantees that each thread alternates between executing instances of
\begin{itemize}
    \item \lighttt{Arrive} statements involving $z_a$ (if this is the first stmt in the cycle, it's \myKeyA{arrive-first} usage)
    \item \lighttt{Await} statements involving $z_g$ (if this is the first stmt in the cycle, it's \myKeyA{await-first} usage)
\end{itemize}
as occurs in Figure~\ref{fig:mbarrier_2_cycle}.

\begin{figure}[t]
\codehrule
\input{b_samples/mbarrier_2_cycle.0.tex}
\caption{Correct barrier guarding for a barrier guard cycle of length 2 (def~\ref{sec:gBarrierGuardCycle}).}
\label{fig:mbarrier_2_cycle}
\codehrule
\end{figure}

If any instruction calls have trailing barrier expressions (def~\ref{sec:gTrailingBarrierExpr}) that use $z_a$, these calls must occur after the \lighttt{Await} and before the \lighttt{Arrive} in the above cycle.
Furthermore, the barrier array elements referenced by the trailing barrier expression must be a non-strict subset of the barrier array elements referenced in the \lighttt{Arrive}.

The await-first usage must be used for any barrier variable that is not guarded-by itself.

\subsection{Solitary Barrier Requirement}
\label{sec:SolitaryBarrier}

There must not be two barrier variables with the same lowered barrier type live in the same scope, if the lowered barrier type of the barrier appears in the following list:
\begin{itemize}
  \item \lighttt{cluster\_sync}, all cluster-sync mechanism split barriers (Section~\ref{sec:ClusterSyncUsage}).
  \item \lighttt{Sm80\_commit\_group}, commit-group mechanism split barriers for sm\_80 non-bulk \lighttt{cp.async} (Section~\ref{sec:CommitGroupUsage}).
  \item \lighttt{tma\_to\_gmem\_commit\_group}, commit-group mechanism split barriers for TMA copies out of SMEM (Section~\ref{sec:CommitGroupUsage}).
  \item \lighttt{wgmma\_commit\_group}, commit-group mechanism split barriers for wgmma (Section~\ref{sec:CommitGroupUsage}).
\end{itemize}
Additionally, a \lighttt{Fence} statement (Section~\ref{sec:FenceUsage}) at \lighttt{cuda\_cluster} scope, but not at \lighttt{cuda\_cta\_in\_cluster} scope (def~\ref{sec:gCollUnit}), must not appear in a scope with a live barrier variable with \lighttt{cluster\_sync} lowered barrier type.

\FloatBarrier
\newpage
\section{Synchronization Semantics}
\label{sec:SyncSemantics}

The synchronization semantics are defined by translating the Exo-GPU program into an abstract machine program, and interpreting the abstract machine program with respect to abstract machine semantics.
The program is synchronized correctly if no error condition is reached.
In the current implementation (with camspork), this can only be done for a given concrete problem size specified by \lighttt{exo.Procedure.sync\_check}.

Both existing Exo value semantics and new abstract machine semantics are defined sequentially.
The primary difference is that in the value semantics, a data array element holds a numerical value, while in abstract machine semantics, each data array element encodes the history of reads and mutates performed to it, with each access encoded as a \myKeyA{visibility record} (Section~\ref{sec:VisRecordState}).
Specifically, the state of an interpreted abstract machine program consists of
\begin{itemize}
  \item A synchronization environment $\rho$ (rho), Section~\ref{sec:SyncEnv}.
  \item A current task ID $\iota: \mathbb{I}$ (iota), Section~\ref{sec:SyncSemanticsThreadMapping}
  \item A control environment $\sigma: \Sigma$ (sigma), with $\Sigma \Coloneqq \mathbb{Y} \to \mathbb{Z}$ (def~\ref{sec:gControlEnv}).
\end{itemize}

When a \myKeyA{non-sync-exempt} (def~\ref{sec:gSyncExempt}) array element is read or mutated, a new visibility record (\textsf{VisRecord}) gets added to the synchronization environment for that element (Section~\ref{sec:VisRecordCreation}).
Each interpreted \lighttt{Fence} (Section~\ref{sec:FenceSemantics}), \lighttt{Arrive} (Section~\ref{sec:ArriveSemantics}), and \lighttt{Await} statement (Section~\ref{sec:AwaitSemantics}) conditionally modifies all visibility records in the \emph{entire environment}, via lifting (Section~\ref{sec:Lifting}) and augment (Section~\ref{sec:Augment}).
\lighttt{Arrive} and \lighttt{Await} statements may also modify the arrive and await count.
Finally, the abstract machine specified correctness checks for each read to (Section~\ref{sec:ChecksOnRead}), mutate of (Section~\ref{sec:ChecksOnMutate}), or free of (Section~\ref{sec:ChecksOnFree}) an array element; these checks are a function of the synchronization environment state for said array element.

For this informal documentation, we will skip the translation step and summarize the effects of relevant Exo-GPU statements upon the above state directly.

\subsection{Control Environment}
\label{sec:ControlEnv}

Function $\sigma: \mathbb{Y} \to \mathbb{Z}$, mapping control variable names (def~\ref{sec:gMathbbY}) to integer values.
This is common to both Exo value semantics, and Exo-GPU abstract machine semantics.

\subsection{Thread Mapping}
\label{sec:SyncSemanticsThreadMapping}

The thread mapping function for a given statement $s$ is denoted, in the PLDI submission, as $g_{s^\#}: \Sigma \to \mathbb{I} \to \mathcal{P}(\mathbb{G})$ where
\begin{itemize}
  \item $s^\#$ denotes $s$ translated to abstract machine IR, which this documentation skips over.
  \item $\Sigma$ is the set of control environments (Section~\ref{sec:ControlEnv}).
  \item $\mathbb{I}$ is the set of task IDs; each executed instance of a device task (def~\ref{sec:gDeviceTask}) defines a new current task ID.
  This abstracts over cluster IDs (def~\ref{sec:gCluster}), since the mapping between tasks and clusters is not statically encoded.
  \item $\mathbb{G} \Coloneqq \mathbb{I} \times \mathbb{N}$ is the set of global thread IDs (def~\ref{sec:gGlobalThreadID}); each is a pair of task ID and local thread index (def~\ref{sec:gLocalThreadIndex}).
  In the context of the abstract machine, a set of $\mathbb{G}$ is a \myKeyA{thread collective} (def~\ref{sec:gThreadCollective}).
\end{itemize}

For each \myKeyA{CPU scope} statement (def~\ref{sec:gCpuScope}), the thread mapping function always gives $\mathbb{G}$.

For each \myKeyA{CUDA scope} statement (def~\ref{sec:gCudaScope}), the thread mapping function is
\begin{equation*}
    g_{s^\#}(\sigma, \iota) = \{ (\iota, n) \mid n \in \textsf{collMap}(\omega_s, \sigma) \}
\end{equation*}
where $\omega_s$ is the collective tiling (Section~\ref{sec:CollTiling}) that collective analysis (def~\ref{sec:gCollAnalysis}) annotated the statement $s$ with, and $\mathsf{collMap}$ is as defined in Section~\ref{sec:CollTilingState}.

\mainKey{Implementation Note:} This is not at all how camspork works (i.e. we don't actually materialize the thread mapping function).
For the real implementation, we still encode the loop mode (def~\ref{sec:gLoopMode}) for each loop and store the current thread collective as a thread cuboid (def~\ref{sec:gThreadCuboid}), which is modified by parallel loops.


\subsection{Synchronization Environment (SyncEnv)}
\label{sec:SyncEnv}

A synchronization environment $\rho$ is a function of type
\begin{equation*}
    \rho: \mathbb{X} \cup \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathsf{VisRecord}) \times \mathcal{P}(\mathsf{VisRecord}) \times \mathbb{N}^2
\end{equation*}
which maps an index into a data variable $x: \mathbb{X}$ (def~\ref{sec:gMathbbX}) or a barrier variable $z: \mathbb{B}$ (def~\ref{sec:gMathbbB}) to a 3-tuple of
\begin{itemize}
  \item set of read visibility records, accessed with $.r$
  \item set of mutate visibility records, accessed with $.m$
  \item pair of arrive count and await count, accessed together as $.b$ and separately as $.b.a$ and $.b.w$ respectively.
\end{itemize}

A newly-allocated variable has all its array elements initialized to an empty read visibility record set, an empty mutate visibility record set, and 0 arrive and await count.

% Referenced in glossary for barrier env
\mainKey{Implementation Note:} the arrive and await count attributes are separated out into a separate barrier environment in camspork, i.e., the synchronization environment is implemented as $\mathbb{X} \cup \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathsf{VisRecord}) \times \mathcal{P}(\mathsf{VisRecord})$ only, and the barrier environment is $\mathbb{B} \to \mathbb{Z}^* \to \mathbb{N}^2$.


\subsection{VisRecord State}
\label{sec:VisRecordState}

Each visibility record $r$ is a pair of type $\mathcal{P}(\mathsf{TlSig}) \times \mathcal{P}(\mathbb{P})$, with the two elements being
\begin{itemize}
  \item The \myKeyA{visibility set} (def~\ref{sec:gVisSet}), denoted $r.s$, which is a set of timeline signatures $\mathsf{TlSig} \Coloneqq \mathbb{G} \times \mathsf{QualTL} \times \mathsf{VF}$; these being a global thread ID (def~\ref{sec:gGlobalThreadID}), a qualitative timeline (def~\ref{sec:gQualTL}), and a \myKeyA{visibility flag} (def~\ref{sec:gVisFlag}), which is one of $\mathsf{VF_{atom}}$ (atomic-only), $\mathsf{VF_{temp}}$ (temporal), $\mathsf{VF_{full}}$, or $\mathsf{VF_{issue}}$.
  The visibility set is conditionally augmented (Section~\ref{sec:Augment}) with more timeline signatures as a result of \lighttt{Fence} and \lighttt{Await} statements.
  \item The \myKeyA{pending await} set (def~\ref{sec:gPendingAwait}), denoted $r.p$, which are tuples of type $\mathbb{B} \times \mathbb{Z}^* \times \mathbb{Z}$, this being a combination of an indexed barrier variable (def~\ref{sec:gMathbbB}) and an arrive-count.
  These mediate the interaction between paired \lighttt{Arrive} statement instances (Section~\ref{sec:ArriveSemantics}) and \lighttt{Await} statement instances (Section~\ref{sec:AwaitSemantics}).
\end{itemize}

This is a different formulation than the PLDI paper; see the reference section (def~\ref{sec:gVisSet}, def~\ref{sec:gPendingAwait}) for more information.

The sync-check error messages format a visibility record's visibility set and pending await set separately.
An integer is used to represent the task ID, counting up starting from 0 for each device task instance (def~\ref{sec:gDeviceTask}) in each CUDA device function launch.

The visibility set is encoded as the union of zero or more ``timeline signature intervals'', each formatted as the lines \\
``\texttt{threads: [task\_index = $\iota_0$ [$c_0^*$],}\\
\texttt{task\_index = $\iota_1$ [$c_1^*$]], inclusive, formatted w/ domain [$D$]}''\\
followed by lines of the form\\
``\texttt{$q_0$ -> $\mathsf{vf}_0^*$}\\
\hphantom{``}\texttt{$q_1$ -> $\mathsf{vf}_1^*$}\\
...''\\
where each $q_i$ is a qualitative timeline (def~\ref{sec:gQualTL}) and each $\mathsf{vf}_i^*$ is a set of visibility flags (def~\ref{sec:gVisFlag}).
The timeline signature interval is the set of all $((\iota, n), q, \mathsf{vf})$ such that
\begin{itemize}
  \item $(\iota_0, \mathsf{toLocal}(D, c_0^*)) \le (\iota, n) \le (\iota_1, \mathsf{toLocal}(D, c_1^*))$, sorted lexicographically (def~\ref{sec:gToLocal}), and
  \item $0 \le n < \prod_m D_m$, and,
  \item $q = q_i$ for some $q_i$, with $\mathsf{vf} \in \mathsf{vf}_i^*$
\end{itemize}
Each pending await $(z, n^*, a)$ in the pending await set is formatted as \\
``\texttt{pending await: $z$[$n^*$] arrive\_count=$a$}''.

\subsection{VisRecord Creation}
\label{sec:VisRecordCreation}

Each read, mutate, \lighttt{Arrive}, or \lighttt{Await} statement that references an array element $x[n^*]$ or $z[n^*]$ in non-sync-exempt (def~\ref{sec:gSyncExempt}) memory causes a new set of visibility records $r^*$ (to be defined) to be associated with $\rho(x, n^*)$ or $\rho(z, n^*)$.
For brevity we will omit the $z$ case in the following description.

A read, \lighttt{Arrive}, or \lighttt{Await} access to an array element $x[n^*]$ causes $r^*$ to be added to the read visibility records set for that array element, i.e. $\rho$ is updated (def~\ref{sec:gUpdateNotation}) as
\begin{equation*}
    \rho[x \to n^* \to (\rho(x, n^*).r \cup r^*, \rho(x, n^*).m, \rho(x, n^*).b)]
\end{equation*}
This is done subsequent to the to-be-defined checks in Section~\ref{sec:ChecksOnRead}.

A mutate access to an array element $x[n^*]$ causes the read visibility records set to be cleared, the mutate visibility records set to be cleared if the access is not atomic (Section~\ref{sec:AtomicInstr}), then $r^*$ is added to the mutate visibility records set, i.e. $\rho$ is updated as
\begin{align*}
    & \rho[x \to n^* \to (\{\}, r^*, \rho(x, n^*).b)] & \text{if not atomic} \\
    & \rho[x \to n^* \to (\{\}, r^* \cup \rho(x, n^*).m, \rho(x, n^*).b)] & \text{if atomic}
\end{align*}
This is done subsequent to the to-be-defined checks in Section~\ref{sec:ChecksOnMutate}.

The new visibility records $r^*$ are initialized based on
\begin{itemize}
  \item the thread collective $g_{s^\#}(\iota, \sigma)$ (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item the convergence of the memory access (Section~\ref{sec:InstrConvergentAccess})
  \item the out-of-order flag (TODO)
  \item the initial qualitative timeline $q$ (Section~\ref{sec:InstrTL}) of the access.
  \item the set of barrier array elements referenced by the trailing barrier expression (Section~\ref{sec:InstrTrailingBarrierExpr}); this is empty for non-instr accesses or instrs without a trailing barrier expression.
\end{itemize}

Define $\mathsf{NewVisRecord}(g^*)$ as having a visibility set that is the union of
\begin{itemize}
    \item $g^* \times \{ q \} \times \{ \mathsf{VF_{issue}} \}$
    \item $g^* \times \{ q \} \times \{ \mathsf{VF_{atom}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}} \}$, only added if the access is \emph{not} out-of-order (TODO)
    \item $\mathbb{G} \times \{ q_a \} \times \{ \mathsf{VF_{atom}} \}$, only added if the access defines an atomic qualitative timeline $q_a$ (Section~\ref{sec:AtomicInstr}).
\end{itemize}
and a pending await set that is based off the arrive count (Section~\ref{sec:SyncEnv}) of each referenced barrier array element:
\begin{equation*}
    \{ (z, n^*, \rho(z, n^*).b.a) \mid \text{ for each $z[n^*]$ referenced (def~\ref{sec:gBarrierExpr}) in the trailing barrier expression} \}
\end{equation*}

If the access is convergent (Section~\ref{sec:InstrConvergentAccess}), $r^*$ contains a single visibility record:
\begin{equation*}
    r^* = \{ \mathsf{NewVisRecord}(g_{s^\#}(\iota, \sigma)) \}
\end{equation*}
Otherwise, $r^*$ contains one visibility record for each thread involved in the statement instance:
\begin{equation*}
    r^* = \{ \mathsf{NewVisRecord}(\{g\}) \mid g \in g_{s^\#}(\iota, \sigma) \}
\end{equation*}
However, for performance reasons, if the non-convergent access is out-of-order, the set of threads may be expanded during sync-check due to non-convergent out-of-order abstract machine optimization (def~\ref{sec:gOooOpt}).

Note that for single-threaded accesses, the convergence makes no difference to the result (setting aside said optimization).

\subsection{Lifting}
\label{sec:Lifting}

The semantics for synchronization statements entail mapping over each visibility record (Section~\ref{sec:VisRecordState}) in the entire synchronization environment (Section~\ref{sec:SyncEnv}).
This is done by applying the function $\mathsf{lift}$ to the synchronization environment, which takes $\lambda: \mathsf{VisRecord} \to \mathsf{VisRecord}$.
$\rho' = \mathsf{lift}(\rho, \lambda)$ is defined by
\begin{equation*}
    \rho'(x, n^*) = \{ \lambda(r) \mid r \in \rho(x, n^*).r \}, \{ \lambda(r) \mid r \in \rho(x, n^*).m \}, \rho(x, n^*).b
\end{equation*}

\subsection{VisRecord Synchronizes-with (Witness)}
\label{sec:Witness}

Given a first synchronization timeline $\tau_s^\mathrm{pre}$ (def~\ref{sec:gSyncTL}) and a thread collective $g^*$ mapped to execute (Section~\ref{sec:SyncSemanticsThreadMapping}) a certain \lighttt{Fence} or \lighttt{Arrive} statement instance, the statement instance ``witnesses'', or ``synchronizes-with'' a visibility record (Section~\ref{sec:VisRecordState}) when its visibility set contains a timeline signature (def~\ref{sec:gTlSig}) of the form $(g, q, \mathsf{VF_{issue}})$ with $g \in g^*$ and $q \in \tau_s^\mathrm{pre}.\mathrm{full}$.

The witness function $\mathcal{W}: \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisRecord} \to \mathsf{Bool}$ is defined as
\begin{equation*}
  \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) = \exists (g, q, v) \in r.s \text{ such that } g \in g^* \land q \in \tau_s^\mathrm{pre}.\mathrm{full} \land v \in \{ \mathsf{VF_{issue}}, \mathsf{VF_{full}} \}
\end{equation*}
The transitivity flag from the PLDI submission is removed. TODO is $\mathsf{VF_{full}}$ needed here?

\subsection{VisRecord Augment}
\label{sec:Augment}

Given a second synchronization timeline $\tau_s^\mathrm{post}$ (def~\ref{sec:gSyncTL}) and a thread collective $g^*$ mapped to execute (Section~\ref{sec:SyncSemanticsThreadMapping}) a certain \lighttt{Fence} or \lighttt{Await} statement instance, the statement instance ``augments'' a visibility record $r$ (Section~\ref{sec:VisRecordState}) by adding the following sets of timeline signatures (def~\ref{sec:gTlSig}) to the visibility set $r.s$:
\begin{align*}
    & \mathcal{A}_\mathrm{full} \Coloneqq g^* \times \tau_s^\mathrm{post}.\mathrm{full} \times \{ \mathsf{VF_{atom}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}}, \mathsf{VF_{issue}} \} \\
    & \mathcal{A}_\mathrm{temp} \Coloneqq g^* \times \tau_s^\mathrm{post}.\mathrm{temp} \times \{ \mathsf{VF_{temp}} \}
\end{align*}
The augment function $\mathcal{A}: \mathsf{VisRecord} \to \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisRecord}$ is defined based on the above as follows:
\begin{equation*}
    \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) = (r.s\cup \mathcal{A}_\mathrm{full} \cup \mathcal{A}_\mathrm{temp}, r.p)
\end{equation*}

\subsection{Fence Semantics}
\label{sec:FenceSemantics}

A \lighttt{Fence} statement augments (Section~\ref{sec:Augment}) all visibility records (Section~\ref{sec:VisRecordState}) that it witnesses (Section~\ref{sec:Witness}).
Specifically, for a given statement \texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}, with $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping}), define $\lambda_\mathrm{Fence}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Fence}(r) = \begin{cases}
        \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) & \text{ if } \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment is $\mathsf{lift}(\rho, \lambda_\mathrm{Fence})$ (Section~\ref{sec:Lifting}).

\subsection{Arrive Semantics}
\label{sec:ArriveSemantics}

A \lighttt{Arrive} statement adds pending await(s) (def~\ref{sec:gPendingAwait}) to all visibility records (Section~\ref{sec:VisRecordState}) that it witnesses (Section~\ref{sec:Witness}), and increments the arrive count of a barrier array element.

For a given statement \texttt{Arrive($\tau_s^\mathrm{post}$) >> $e^*$}, with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $z[n_h^*]$ being the home barrier of $e^*$ (def~\ref{sec:gHomeBarrier})
  \item $a$ being the arrive count of the home barrier (i.e. $\rho(z, n_h^*).b.a$)
\end{itemize}
define $\lambda_\mathrm{Arrive}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Arrive}(r) = \begin{cases}
        (r.s, r.p \cup \{ (z, n^*, a) \text{ for each $z[n^*]$ referenced by any barrier expr.} \})
        & \text{ if } \mathcal{W}(g^*, \tau_s^\mathrm{pre}.\mathrm{full}, r) \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment $\rho''$ is defined from $\rho$ (Section~\ref{sec:SyncEnv}) by
\begin{align*}
  & \rho' = \mathsf{lift}(\rho, \lambda_\mathrm{Arrive}) & \text{(Section~\ref{sec:Lifting})} \\
  & \rho'' = \rho'[z \to n_h^* \to (r.r, r.m, r.b.a + 1, r.b.w)] \text{ with } r = \rho'(z, n_h^*) & \text{(def~\ref{sec:gUpdateNotation})}
\end{align*}
The way we use and update only the arrive count of the home barrier (as a stand-in for the state of all barriers) is the reason for the barrier multicast convergence requirement (Section~\ref{sec:BarrierMulticast}).

\subsection{Await Semantics}
\label{sec:AwaitSemantics}

An \lighttt{Await} statement augments (Section~\ref{sec:Augment}) all visibility records (Section~\ref{sec:VisRecordState}) that contain the needed pending await (def~\ref{sec:gPendingAwait}), and updates the await count of the barrier array element referenced by the statement's barrier expression.

For a given statement \texttt{Await}($e, \tau_s^\mathrm{post}, n$) with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ assigned to execute it (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $z[n^*]$ being the barrier array element referenced by $e$ (def~\ref{sec:gBarrierExpr})
  \item $a$ being the arrive count of the referenced barrier array element (i.e. $\rho(z, n^*).b.a$)
  \item $w$ being the await count of the referenced barrier array element (i.e. $\rho(z, n^*).b.w$)
\end{itemize}
the behavior differs based on whether $n \ge 0$ (arrive-indexed case) or $n < 0$ (await-indexed case).

If $n \ge 0$ (arrive-indexed case), then define
\begin{align*}
    & a_\mathrm{max} = a - n - 1 \\
    & w' = \mathrm{max}(w, a_\mathrm{max} + 1)
\end{align*}
If $n < 0$ (await-indexed case), then define
\begin{align*}
    & \mathrm{lag} = -1 - n \\
    & a_\mathrm{max} = w - \mathrm{lag} \\
    & w' = w + 1
\end{align*}
Define $\lambda_\mathrm{Await}: \mathsf{VisRecord} \to \mathsf{VisRecord}$ as
\begin{equation*}
    \lambda_\mathrm{Await}(r) = \begin{cases}
        \mathcal{A}(r, g^*, \tau_s^\mathrm{post}.\mathrm{full}, \tau_s^\mathrm{post}.\mathrm{temp}) & \text{ if } \exists (z', n'^*, a') \in r.p \text{ with } z = z' \land n^* = n'^* \land a' \le a_\mathrm{max} \\
        r & \text{ otherwise }
    \end{cases}
\end{equation*}
The new synchronization environment $\rho''$ is defined from $\rho$ (Section~\ref{sec:SyncEnv}) by
\begin{align*}
  & \rho' = \mathsf{lift}(\rho, \lambda_\mathrm{Await}) & \text{(Section~\ref{sec:Lifting})} \\
  & \rho'' = \rho'[z \to n^* \to (r.r, r.m, r.b.a, w')] \text{ with } r = \rho'(z, n^*) & \text{(def~\ref{sec:gUpdateNotation})}
\end{align*}

\mainKey{Implementation Note:} camspork removes old pending awaits to avoid infinite state growth.

\subsection{Check VisRecord Helper}
\label{sec:CheckVisRecordHelper}

For the following checks, we define the helper \\
$\mathsf{CheckVisRecord}: \mathsf{Bool} \to \mathcal{P}(\mathbb{G}) \to \mathcal{P}(\mathsf{QualTL}) \to \mathsf{VisFlag} \to \mathsf{VisRecord} \to \mathsf{Bool}$ as
\begin{equation*}
    \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{vf}, r) = \begin{cases}
        \exists g \in g^*, \exists q \in q^*, (g, q, \mathsf{vf}) \in r.s & \text{ if $t$ is true} \\
        \forall g \in g^*, \exists q \in q^*, (g, q, \mathsf{vf}) \in r.s & \text{ if $t$ is false} \\
    \end{cases}
\end{equation*}
In this context $t$ is the \myKeyA{convergence flag}; note it only changes the qualifier $\forall$ or $\exists$ for $g \in g^*$, which has no effect when $g^*$ consists only of a single global thread ID (def~\ref{sec:gGlobalThreadID}).

\subsection{Checks on Read, Arrive, Await}
\label{sec:ChecksOnRead}

Read-like accesses only impose requirements on the mutate visibility record set.

For each array element $x[n^*]$ referenced by a read within a statement, an \lighttt{Arrive} statement, or an \lighttt{Await} statement,
\begin{itemize}
  \item let $g^* = g_{s^\#}(\iota, \sigma)$ be the thread collective assigned to execute the statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item let $q^*$ be the \myKeyA{extended timeline set} (Section~\ref{sec:InstrTL})
  \item let $t$ be the convergence flag (Section~\ref{sec:InstrConvergentAccess})
\end{itemize}
Require that
\begin{equation*}
    \forall r \in \rho(x, n^*).m, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{VF_{full}}, r)
\end{equation*}
If the above is not true, the abstract machine enters the error condition.
Otherwise, this step causes no state changes, but the abstract machine continues on to the changes specified in Section~\ref{sec:VisRecordCreation}.

\subsection{Checks on Mutate}
\label{sec:ChecksOnMutate}

Mutate accesses impose requirements on both the read and mutate visibility record sets.

For each array element $x[n^*]$ that is mutated as part of a write, reduce, or instr call statement,
\begin{itemize}
  \item let $g^* = g_{s^\#}(\iota, \sigma)$ be the thread collective assigned to execute the statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item let $q^*$ be the \myKeyA{extended timeline set} (Section~\ref{sec:InstrTL})
  \item let $t$ be the convergence flag (Section~\ref{sec:InstrConvergentAccess})
  \item let $\mathsf{vf}: \mathsf{VF}$ (def~\ref{sec:gVisFlag}) be $\mathsf{VF_{temp}}$ if the access is non-atomic write-only, $\mathsf{VF_{full}}$ if the access is non-atomic read-write, and $\mathsf{VF_{atom}}$ if the access is atomic (Section~\ref{sec:AtomicInstr}).
\end{itemize}
Require that
\begin{align*}
    & \forall r \in \rho(x, n^*).r, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{VF_{temp}}, r) \\
    & \forall r \in \rho(x, n^*).m, \mathsf{CheckVisRecord}(t, g^*, q^*, \mathsf{vf}, r)
\end{align*}
If the above is not true, the abstract machine enters the error condition.
Otherwise, this step causes no state changes, but the abstract machine continues on to the changes specified in Section~\ref{sec:VisRecordCreation}.

\subsection{Checks on Free}
\label{sec:ChecksOnFree}

When barrier variables are freed, the arrive and await count (Section~\ref{sec:SyncEnv}) must be equal for all elements in the freed barrier array.

Variables stored in shared memory must currently be allocated and freed at \lighttt{cuda\_cluster} scope (def~\ref{sec:gCollUnit}).
For such variables, upon free, we perform the same checks as for a non-atomic write-only access (Section~\ref{sec:ChecksOnMutate}) with
\begin{itemize}
  \item $g^* = g_{s^\#}(\iota, \sigma)$ being the thread collective assigned to execute the free statement (Section~\ref{sec:SyncSemanticsThreadMapping})
  \item $q^* = \{ \texttt{cuda\_in\_order\_qual} \}$ (def~\ref{sec:gQualTL})
  \item the convergence flag $t$ being false
\end{itemize}

\FloatBarrier
\newpage
\section{Instructions \& Timelines}
\label{sec:Instr}

\subsection{Instr Class}
\label{sec:InstrClass}

\subsection{Instruction Timeline (InstrTL)}
\label{sec:InstrTL}

\subsection{Instruction Convergent Access}
\label{sec:InstrConvergentAccess}

\subsection{Atomic Instructions}
\label{sec:AtomicInstr}

\subsection{Instruction Distributed Memory}
\label{sec:InstrDistributedMemory}

\subsection{Instruction Trailing Barrier Expression}
\label{sec:InstrTrailingBarrierExpr}

\subsection{Special Window}
\label{sec:SpecialWindow}

\FloatBarrier
\newpage
\section{Glossary \& Reference}
\label{sec:Glossary}

% >A

\subsection{$\mathcal{A}$ (Augment Function)}

Abstract machine helper function defined in Section~\ref{sec:Augment}.
Used to modify the set of visibility records (Section~\ref{sec:VisRecordState}) modified by an interpreted \lighttt{Fence} statement (Section~\ref{sec:FenceSemantics}) or \lighttt{Await} statement (Section~\ref{sec:AwaitSemantics}).

\subsection{Agnostic (Collective Type)}
\label{sec:gAgnostic}

A collective type $\delta$ is agnostic if any $\delta.B_i$ is $\top$.

\subsection{Aligned Form (Collective Type)}
\label{sec:gAlignedForm}

A collective type $\delta$ is in aligned form when all box coordinates $\delta.B_i$ satisfy $\delta.B_i \in \{\top, 1, \delta.D_i\}$.
If no box coordinate is $\top$, then if $\delta$ describes two thread collectives, those two thread collectives are either identical or disjoint.

% >B

\subsection{$\mathbb{B}$ (mathbb-B)}
\label{sec:gMathbbB}

$\mathbb{B}$ denotes the set of barrier variable names (def~\ref{sec:gBarrierVariable}).
Somewhat confusingly, we conventionally use $z$ to mean an element of $\mathbb{B}$, i.e. $z$ is the name of a barrier variable (def~\ref{sec:gBarrierVariable}).

\subsection{Barrier Environment}
\label{sec:gBarrierEnv}

This is an implementation detail of camspork, and ``officially'', this is part of the synchronization environment.
See Section~\ref{sec:SyncEnv}.

\subsection{Barrier Expression}
\label{sec:gBarrierExpr}

An expression ``$z[e^*]$'', where $z: \mathbb{B}$ is a barrier variable name and $e^*$ is a tuple of expressions (``$z$'' alone indicates $e^*$ is empty).
Each expression $e_i$ must either be a point expression consisting of a plain read of a control variable (def~\ref{sec:gMathbbY}) or an interval expression $0:x_i$, $x_i$ being the extent of the $i^{th}$ dimension of $z$'s array size.

A barrier expression $z[e_0, ..., e_{M-1}]$ references all barrier array elements $z[n_0, ..., n_{M-1}]$ where
\begin{itemize}
  \item $n_i = \sigma(e_i)$ (def~\ref{sec:gControlEnv}) for all $e_i$ that are point expressions.
  \item $n_i \in [0, x_i - 1]_\mathbb{N}$ for all $e_i$ that are intervals $0:x_i$.
\end{itemize}

\subsection{Barrier Guard Cycle}
\label{sec:gBarrierGuardCycle}

The barrier guard cycle for a given barrier name $z$ is the set of barrier names in $\{ \textsf{guardedBy}^n(z)\mid n = 1, 2, 3... \}$ (def~\ref{sec:gGuardedBy}).

\subsection{Barrier Guarding Requirement}
\label{sec:gBarrierGuardingRequirement}

Here we give a more complete description of the static analysis for the barrier guarding requirement (Section~\ref{sec:BarrierGuarding}).
Let $z_a$ be a barrier variable subject to the barrier guarding requirement, and let it be guarded-by $z_g$ (def~\ref{sec:gGuardedBy}).

\mainKey{Requirement 1/3:} The program must pass this pseudocode in Figure~\ref{fig:BarrierGuardingPseudocode} without reaching an assert.

\begin{figure}[t]
\codehrule
\blacktt{expect\_arrive = arrive\_first~}\codecomment{\# True in the arrive-first usage, false otherwise.}\\
\blacktt{stmt\_pair = []}\\
\blacktt{for stmt in dfs\_order:} \\
\blacktt{~~if stmt is an Arrive using $z_a$:}\\
\blacktt{~~~~\textbf{assert} expect\_arrive}\\
\blacktt{~~~~expect\_arrive = False}\\
\blacktt{~~~~stmt\_pair += [stmt]}\\
\blacktt{~~elif stmt has a trailing barrier expression using $z_a$:}\\
\blacktt{~~~~\textbf{assert} expect\_arrive}\\
\blacktt{~~elif stmt is an Await using $z_g$:}\\
\blacktt{~~~~\textbf{assert} not expect\_arrive}\\
\blacktt{~~~~expect\_arrive = True}\\
\blacktt{~~~~stmt\_pair += [stmt]}\\
\blacktt{~~if len(stmt\_pair) == 2:}\\
\blacktt{~~~~s1, s2 = stmt\_pair}\\
\blacktt{~~~~stmt\_pair = []}\\
\blacktt{~~~~}\codecomment{\# If one stmt is guarded by an if that the other isn't, we can't guarantee alternation.}\\
\blacktt{~~~~}\codecomment{\# Traverse from leaf-to-root of the AST to assert the following.}\\
\blacktt{~~~~\textbf{assert} s1 is in the body of an if statement iff s2 is}\\
\blacktt{~~~~\textbf{assert} s1 is in the orelse of an if statement iff s2 is}\\
\blacktt{~~~~\textbf{assert} s1 is in the body of a \emph{sequential} for loop iff s2 is}\\
\codecomment{\# End DFS}\\
\blacktt{\textbf{assert} len(stmt\_pair) == 0 }\codecomment{\# Unpaired Arrive/Await?}\\

\caption{Pseudocode for barrier guarding requirement}
\label{fig:BarrierGuardingPseudocode}
\codehrule
\end{figure}

\mainKey{Requirement 2/3:} Each collective indexing pair (Section~\ref{sec:CollIndexingPairs}) collected from \lighttt{Await} statements for $z_g$ must pass the base threads requirement (Section~\ref{sec:DistributedMemoryBaseThreads}) with each collective index expression collected from \lighttt{Arrive} statements for $z_a$.
This ensures the set of threads executing the matched \lighttt{Await} and \lighttt{Arrive} statements are the same.
TODO not really, we need to check the thread pitch.

\mainKey{Requirement 3/3:} This enforces the requirement that trailing barrier expressions don't reference any barrier array element not referenced in the \lighttt{Arrive} statement:

Let $m: (\mathsf{Bool}^*)^*$ be the multicasts (def~\ref{sec:gMulticasts}) of the \lighttt{Arrive} statement for $z_a$.

For each trailing barrier expression that uses $z_a$, let $c: \mathsf{Bool}^*$ be its multicast flags, and require there exists some $m_i: \mathsf{Bool}^*$ in $m$ such that
\begin{itemize}
  \item $c$ and $m_i$ have the same length (say, $M$).
  \item $m_{i,j}$ or not $c_j$ for all $j = 0, 1, ..., M-1$.
\end{itemize}

% TODO
% No non-shared sequential or if/else.
% trailing barrier expression
%

\subsection{Barrier Mechanism}
\label{sec:gBarrierMechanism}

Concept similar to \lighttt{Memory} for barrier variables (def~\ref{sec:gBarrierVariable}).
Currently, one of
\begin{itemize}
  \item \texttt{CudaClusterSync} (Section~\ref{sec:ClusterSyncUsage})
  \item \texttt{CudaCommitGroup} (Section~\ref{sec:CommitGroupUsage})
  \item \texttt{CudaMbarrier} (Section~\ref{sec:MbarrierUsage})
\end{itemize}

\subsection{Barrier Variable}
\label{sec:gBarrierVariable}

A barrier variable is declared with one of
\begin{itemize}
  \item Frontend syntax ``$z_a$: \texttt{barrier[$e^*$] @ }$\tau_b$'' where $z_a$ is the variable name, $e^*$ are the extents of the barrier array, and $\tau_b$ is a barrier mechanism (def~\ref{sec:gBarrierMechanism})
  \item Frontend syntax ``$z_a$: \texttt{barrier($z_g$)[$e^*$] @ }$\tau_b$'' where the declared barrier is \myKeyA{explicitly guarded by} the barrier named $z_g$, and all others are as above
  \item Scheduling operator \texttt{insert\_barrier\_alloc}
\end{itemize}
If $e^*$ is empty, the square brackets shall be omitted.

In semantics, we denote a barrier variable name with $z: \mathbb{B}$.

% >C

\subsection{Camspork}
\label{sec:gCamspork}

C Abstract Machine for Spork.
C++ library that implements the abstract machine interpreter used for sync-check.

\subsection{Cluster}
\label{sec:gCluster}
Group of \lighttt{clusterDim}-many CTAs (def~\ref{sec:gCta}) that execute concurrently on the same GPC, and can synchronize with cluster sync, or using mbarriers.
When \lighttt{clusterDim = 1} (which is the case by default for Exo-GPU), then CTA and cluster are synonymous.
The PTX variable \lighttt{cluster\_ctarank} is the 0-based index of the CTA in the cluster.

\subsection{Collective Analysis}
\label{sec:gCollAnalysis}

Compilation step that assigns collective tilings (Section~\ref{sec:CollTiling}) to each CUDA-scope statement (def~\ref{sec:gCudaScope}) and performs distributed memory analysis (Section~\ref{sec:DistributedMemory}).

\subsection{Collective Indexing Pair}
\label{sec:gCollIndexingPair}

Object of type $\Omega \times \mathsf{Expr}^*$, where $\Omega$ is a collective tiling (def~\ref{sec:gCollTiling}) and $\mathsf{Expr}^*$ is a tuple of array index expressions.
Used for distributed memory analysis (Section~\ref{sec:DistributedMemory}) of a variable.
Collective indexing pairs are collected from all statements and expressions that index said variable.

\subsection{Collective Tiling}
\label{sec:gCollTiling}

A per-statement attribute of statements at CUDA scope (def~\ref{sec:gCudaScope}).
We denote a collective tiling with $\omega \in \Omega$.
This describes via the function $\textsf{collMap}: \Omega \to \Sigma_c \to \mathcal{P}(\mathbb{N})$ (Section~\ref{sec:CollTilingState}) a mapping between control variable values and the local thread indices (def~\ref{sec:gLocalThreadIndex}) of the thread collective (def~\ref{sec:gThreadCollective}) assigned to execute an instance of the statement (def~\ref{sec:gStmtInstance}).

\subsection{Collective Type (Scope)}
\label{sec:gCollType}

Description of a certain number and arrangement of threads in a cluster, e.g. thread, warp, CTA (Section~\ref{sec:CollType}).
A collective type $\delta$ has a certain dimensionality $M$, and consists of
\begin{itemize}
  \item domain, $\delta.D: \mathbb{N}_{\ge2}^M$ (coordinates are natural numbers at least 2).
  \item box, $\delta.B: \mathbb{N}_\top^M$ (coordinates are natural numbers or $\top$).
\end{itemize}
As well as implied dimension thread pitch values $\delta.P$ (def~\ref{sec:gThreadPitch}).
A statement is at $\delta$-scope when it is in CUDA scope (def~\ref{sec:gCudaScope}) and the thread collectives assigned to execute instances of that statement are always described by $\delta$ (Section~\ref{sec:CollTypeThreadCollective}).


\subsection{Collective Unit (Scope)}
\label{sec:gCollUnit}

Syntactic construct that wraps a collective type.
This may be parameterized based on \lighttt{blockDim} and \lighttt{clusterDim}.
A collective unit instance is $\tau_u$ in the grammar, and is an instance of \lighttt{CollUnit} in Python, with $\top$ represented by \lighttt{None} and other coordinate values represented by an instance of \lighttt{CollSizeExpr}.

A statement is at $\tau_u$-scope when it is in $\delta$-scope (def~\ref{sec:gCollType}), where $\delta$ is unpacked from $\tau_u$ with alignment and 1-padding; we define this after the table.

{
\footnotesize
\centering
\arraycolsep=1.8pt\def\arraystretch{1.0}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{rrlll}
\toprule
& & & \emph{domain} & \emph{box} \\
$\tau_u : \mathrm{CollUnit} $ & $\Coloneqq$ &
  \texttt{standalone\_thread} & \texttt{(1,)} & \texttt{(1,)} \\
  &|& \texttt{$n_1$ * cuda\_thread} & \texttt{(blockDim,)} & \texttt{($n_1$,)} \\
  &|& \texttt{cuda\_quadpair} & \texttt{(blockDim/16, 16)} & \texttt{(2, 4)} \\
  &|& \texttt{$n_1$ * cuda\_warp} & \texttt{(blockDim,)} & \texttt{($n_1$ * 32,)} \\
  &|& \texttt{$n_1$ * cuda\_warpgroup} & \texttt{(blockDim,)} & \texttt{($n_1$ * 128,)} \\
  &|& \texttt{$n_1$ * cuda\_threads\_strided($n_2$, $n_3$)} & \texttt{(blockDim/$n_3$, $n_3$)} & \texttt{($n_1$, $n_2$)} \\
  &|& \texttt{$n_1$ * cuda\_warp\_in\_cluster} & \texttt{(clusterDim, blockDim)} & \texttt{($n_1$, 32)} \\
  &|& \texttt{$n_1$ * cuda\_cta\_in\_cluster} & \texttt{(clusterDim * blockDim,)} & \texttt{($n_1$ * blockDim,)} \\
  &|& \texttt{cuda\_cluster} & \texttt{(clusterDim * blockDim,)} & \texttt{(clusterDim * blockDim,)} \\
  &|& \texttt{$n_1$ * cuda\_cta\_in\_cluster\_strided($n_3$)} & \texttt{(ClusterDim/$n_3$, $n_3$, blockDim)} & \texttt{($n_1$, 1, blockDim)} \\
  &|& \texttt{$n_1$ * cuda\_warp\_in\_cluster\_strided($n_3$)} & \texttt{(clusterDim/$n_3$, $n_3$, blockDim)} & \texttt{($n_1$, 1, 32)} \\
  &|& \texttt{cuda\_agnostic\_sub\_cta} & \texttt{(clusterDim, blockDim)} & \texttt{(1, $\top$)} \\
  &|& \texttt{cuda\_agnostic\_intact\_cta} & \texttt{(clusterDim, blockDim)} & \texttt{($\top$, blockDim)} \\
\bottomrule
\end{tabular}
}

The conversion to a collective type may or may not be \textit{aligned} and may or may not be \textit{1-padded}.
The steps to unpack a collective type from a collective unit are

\begin{itemize}
  \item Substitute concrete values for \lighttt{clusterDim} and \lighttt{blockDim}.
    This converts the domain and box into tuples of rational numbers or $\top$.
  \item Fail if any non-$\top$ coordinate is not a natural number, or if $B_i \ne \top \land B_i \notin [1, D_i]$ for any box coordinate $B_i$ and corresponding domain coordinate $D_i$.
  \item Remove any domain coordinates $D_i$ with $D_i = 1$ and remove corresponding box coordinates $B_i$ (it must be the case that $B_i = 1$ or $B_i = \top$ by the above check).
  \item Initialize the collective type $\delta$ with the box and domain.
  \item Let $f = \frac{\texttt{clusterDim * blockDim}}{\delta.D_0 \times \delta.D_1 \times ...}$; fail if $f \notin \mathbb{N}$.
  \item If $f > 1$, prepend $f$ to $\delta.D$, and prepend $1$ or $\top$ to $\delta.B$, for the 1-padded and non-1-padded cases, respectively.
  \item If the conversion is \textit{aligned}, reshape (Section~\ref{sec:CollTypeReshape}) until the collective type is in aligned form (def~\ref{sec:gAlignedForm}); fail if this cannot be done.
  \item \textbf{NOTE:} not all of these failures seem to be checked by Exo-GPU today.
\end{itemize}

\mainKey{Example 1:} Suppose \lighttt{clusterDim = 2}, \lighttt{blockDim = 256}, and the collective unit has domain (\lighttt{blockDim},), box (128,).

If we have alignment and 1-padding, then
\begin{itemize}
  \item Substitution gives $\delta.D = (256,)$, $\delta.B = (128,)$.
  \item No domain coordinates are 1.
  \item $f = 2$, so we prepend $2$ to $\delta.D$ and $1$ to $\delta.B$ (since 1-padding is on).
    Now $\delta.D = (2, 256)$ and $\delta.B = (1, 128)$.
  \item Since we have alignment, split dimension 1 by 128 to get the final collective type $\delta$, with $\delta.D = (2, 2, 128)$ and $\delta.B = (1, 1, 128)$.
\end{itemize}
If we have neither alignment nor 1-padding, then the result would instead be $\delta.D = (2, 256)$ and $\delta.B = (\top, 128)$.

\mainKey{Example 2:} Suppose \lighttt{clusterDim = 8}, \lighttt{blockDim = 384}, and the collective unit has domain (\lighttt{clusterDim}, \lighttt{blockDim}) and box (2, 128).

If we have alignment, then
\begin{itemize}
  \item Substitution gives $\delta.D = (8, 384), \delta.B = (2, 128)$.
  \item No domain coordinates are 1.
  \item $f = 1$, so no change needed.
  \item Since we have alignment, we do two splits to get $\delta.D = (4, 2, 3, 128)$ and $\delta.B = (1, 2, 1, 128)$.
\end{itemize}
If we don't have alignment, then the result would instead be $\delta.D = (8, 384)$ and $\delta.B = (2, 128)$.
1-padding does not impact this result.

\mainKey{Example 3:} Suppose \lighttt{clusterDim=1}, \lighttt{blockDim = 128}, and the collective unit has domain $(\lighttt{clusterDim}, \lighttt{blockDim})$ and box $(1, \top)$.
\begin{itemize}
  \item Substitution gives $\delta.D = (1, 128), \delta.B = (1, \top)$.
  \item Remove the $0^{th}$ dimension as $\delta.D_0 = 1$. So $\delta.D = (128,), \delta.B = (\top,)$.
  \item $f = 1$, so no change needed.
\end{itemize}
Alignment and 1-padding don't affect this example.
The unpacked collective type is always $\delta.D = (128,), \delta.B = (\top,)$.

\subsection{Control Environment}
\label{sec:gControlEnv}

Function $\sigma: \mathbb{Y} \to \mathbb{Z}$, mapping control variable names (def~\ref{sec:gMathbbY}) to integer values.
This is common to both Exo value semantics, and Exo-GPU abstract machine semantics.

\subsection{CPU Scope}
\label{sec:gCpuScope}

Statements outside of a \lighttt{CudaDeviceFunction} block (def~\ref{sec:gCudaDeviceFunction}), including said statement itself, are at CPU scope.

\subsection{CTA}
\label{sec:gCta}
Cooperative thread array, also known as a ``thread block''.
Group of \lighttt{blockDim}-many CUDA threads that execute concurrently on the same SM, and can be synchronized with \lighttt{\_\_syncthreads()} in CUDA C++.

Exo-GPU only parallelizes on the x-dimension, so \lighttt{threadIdx.x} identifies a thread within a CTA; it ranges from 0 to \lighttt{blockDim - 1}.

\subsection{CUDA Device Function Block}
\label{sec:gCudaDeviceFunction}

Statement that launches its body as a CUDA device function (``kernel''/``grid'') (Section~\ref{sec:CudaDeviceFunction}).

\input{b_samples/CudaDeviceFunction.0.tex}

The above example may be scheduled using

\input{b_samples/CudaDeviceFunction_scheduling.0.tex}

\subsection{CUDA Scope}
\label{sec:gCudaScope}

Statements within a \lighttt{CudaDeviceFunction} block (def~\ref{sec:gCudaDeviceFunction}) are at CUDA scope.

% >D

\subsection{Delta ($\delta: \Delta$)}
\label{sec:gDelta}

$\Delta$ denotes the set of all collective types (def~\ref{sec:gCollType}, Section~\ref{sec:CollType}) and $\delta$ is an element of that set.

\subsection{deriveCollTiling}
\label{sec:gDeriveCollTiling}

The function $\mathsf{deriveCollTiling}: \Omega \to \mathbb{Y} \to \Delta \to \mathbb{N} \to \mathbb{N} \to \mathbb{N} \to \Omega$ is used for computing the collective tilings for \lighttt{cuda\_threads} loops (Section~\ref{sec:CollTilingCudaThreads}), \lighttt{with CudaWarps} (Section~\ref{sec:CollTilingCudaWarps}), and instructions with distributed memory (Section~\ref{sec:InstrDistributedMemory}).

Recall, $\Omega$ denotes collective tilings (Section~\ref{sec:CollTiling}), $\mathbb{Y}$ denotes names of control variables (def~\ref{sec:gMathbbY}), $\Delta$ denotes collective types (Section~\ref{sec:CollType}), and $\delta.B$ and $\omega.B$ denote the box.

$\mathsf{deriveCollTiling}(\omega_\text{raw}, y, \delta_\text{raw}, \texttt{lo}, \texttt{hi}, \texttt{tileCount})$ is defined by
\begin{itemize}
  \item Let $\omega, \delta = \mathsf{domainCompletion}(\omega_\text{raw}, \delta_\text{raw})$ (def~\ref{sec:gDomainCompletion}).
  \item The $k^{th}$ dimension is a \myKeyA{tiled dimension} if $\delta.B_k \notin \{ \top, \omega.B_k, \omega.D_k \}$.
  \item If no tiled dimension exists,
  \begin{itemize}
    \item We must have $\texttt{lo} = 0$ and $\texttt{hi} = \texttt{tileCount} = 1$ or the program is ill-formed.
    \item Return $\omega$.
  \end{itemize}
  \item If multiple tiled dimensions exist, the program is ill-formed (``ambiguous tiling'').
  \item Let $k$ be the index of the unique tiled dimension.
  \item If $(\texttt{hi})(\delta.B_k) > \omega.B_k$, the program is ill-formed (``asked for too many threads'').
  \item Append to $\omega_k.\textit{ops}$ (Section~\ref{sec:CollTilingState}) a collective dimension operator with
  \begin{itemize}
    \item $\textit{iter} = y$
    \item $\textit{offset} = (\texttt{lo})(\delta.B_k)$
    \item $\textit{box} = \delta.B_k$
    \item $\textit{tileCount} = \texttt{tileCount}$
  \end{itemize}
  \item Return $\omega$.
\end{itemize}

\subsection{Device Task}
\label{sec:gDeviceTask}

The body of the inner-most \lighttt{cuda\_tasks} loop is a device task (see~\ref{sec:gCudaDeviceFunction}).

\subsection{Domain}
\label{sec:gDomain}

Attribute $\delta.D$ of a collective type $\delta$ (Section~\ref{sec:CollType}) and attribute $\omega.D$ (Section~\ref{sec:CollTiling}) of a collective tiling $\omega$.
Both are of type $\mathbb{N}_{\ge2}^M$ for some dimension $M: \mathbb{N}$.
This describes the arrangement of threads within a cluster into an $M$-dimensional grid via the \textsf{toLocal} function (def~\ref{sec:gToLocal}).

\subsection{Domain Completion}
\label{sec:gDomainCompletion}

$\textsf{domainCompletion}: \Omega \times \Delta \to \Omega \times \Delta$ takes a collective tiling $\omega$ (def~\ref{sec:gCollTiling}) and a collective type $\delta$ (def~\ref{sec:gCollType}) and returns $(\omega', \delta')$ such that $\omega'.D = \delta'.D$ (def~\ref{sec:gDomain}), with $\omega$ converted to $\omega'$ and $\delta$ converted to $\delta'$ using only reshape operations.
This ensures the ``meaning'' of $\omega'$ and $\delta'$ is the same as $\omega$ and $\delta$, except that $\delta'$ may imply stricter alignment requirements.

An example (inefficient) implementation sketch of domain completion is given:

\begin{itemize}
  \item If $\omega.D = \delta.D$, return $(\omega, \delta)$.
  \item If $\prod \omega.D_i \ne \prod \delta.D_i$, fail (the implied cluster thread count isn't equal).
  \item Calculate the thread pitch sets (def~\ref{sec:gThreadPitch}) $\omega.P$ and $\delta.P$.
  \item Pick $p \in \omega.P \setminus \delta.P$, if it exists, then,
  \begin{itemize}
    \item Let $\delta.P_i = \max \{ a \in \delta.P \mid a < p \}$.
    \item Let $f = p / \delta.P_i$; fail if this is not an integer.
    \item Split the $i^{th}$ dimension of $\delta$ by $f$ (Section~\ref{sec:CollTypeReshape}); this has the effect of adding $p$ to the thread pitch set of $\delta$.
  \end{itemize}
  \item Otherwise, pick $p \in \delta.P \setminus \omega.P$, which must exist, then,
  \begin{itemize}
    \item Let $\omega.P_i = \max \{ a \in \omega.P \mid a < p \}$.
    \item Let $f = p / \omega.P_i$; fail if this is not an integer.
    \item Split the $i^{th}$ dimension of $\omega$ by $f$ (Section~\ref{sec:CollTilingReshape}); this has the effect of adding $p$ to the thread pitch set of $\omega$.
  \end{itemize}
  \item Return $\mathsf{domainCompletion}(\omega, \delta)$.
\end{itemize}

\subsection{Domain Completion Type Only}
\label{sec:gDomainCompletionTypeOnly}

$\textsf{domainCompletionTypeOnly}: \Delta \times \Delta \to \Delta \times \Delta$ is defined as $\textsf{domainCompletion}$ (def~\ref{sec:gDomainCompletion}), but with the role of $\omega: \Omega$ replaced with $\delta_2: \Delta$.

% >E
% >F
% >G

\subsection{Garden-variety Fence}
\label{sec:gGardenVarietyFence}

See ``Fence Usage'' (Section~\ref{sec:FenceUsage}).

\subsection{Global Thread ID ($\mathbb{G}$, mathbb-G)}
\label{sec:gGlobalThreadID}

Pair of task ID $\iota: \mathbb{I}$ (Section~\ref{sec:SyncSemanticsThreadMapping}) and local thread index $n \in \mathbb{N}$ (def~\ref{sec:gLocalThreadIndex}).

\subsection{Guarded-by}
\label{sec:gGuardedBy}

A barrier variable declaration for $z_a$ may or may not include an explicit-guarded-by barrier name $z_g$ (def~\ref{sec:gBarrierVariable}).
Two barrier variables must not share the same explicit-guarded-by barrier name.
If $z_a$ was declared with an explicit-guarded-by barrier name $z_g$, then $z_a$ is guarded-by $z_g$.
Otherwise, $z_a$ is guarded-by the barrier found by the following lookup process:

Let $\textsf{explicitGuards}: \mathbb{B} \to \mathbb{B}$ be the function
\begin{equation*}
  \textsf{explicitGuards}(z_g') = \begin{cases}
    z_a' & \text{ if $z_a'$ has explicit-guarded-by barrier name $z_g'$ (def~\ref{sec:gBarrierVariable})} \\
    z_g' & \text{ otherwise }
  \end{cases}
\end{equation*}
$z_a$ is guarded-by the fixed point of $\textsf{explicitGuards}^n(z_a), n = 1, 2, 3...$.

We use $\textsf{guardedBy}: \mathbb{B} \to \mathbb{B}$, which maps a barrier name $b$ to the name of the barrier that $b$ is guarded by.

See also ``Barrier Guard Cycle'' (def~\ref{sec:gBarrierGuardCycle}).

% >H

\subsection{Home Barrier (Expression)}
\label{sec:gHomeBarrier}

Each barrier expression (def~\ref{sec:gBarrierExpr}), when interpreted, references a certain set of barrier array elements.
The intersection of these sets for all barrier expressions in a synchronization statement (def~\ref{sec:gSyncStmt}) must be a single barrier array element; this is the home barrier.
The \myKeyA{home barrier expression} is an expression constructed to reference this home barrier; it is constructed from the barrier expressions $z_0[e_0^*] ... z_{E-1}[e_{E-1}^*]$ attached to an \lighttt{Arrive} statement as follows:
\begin{itemize}
  \item All expressions must use the same barrier variable name $z$.
  \item All tuples $e^*$ must have the same length $M$.
  \item For each $j \in [0, M-1]_\mathbb{N}$, at least one $e^*$ must have a non-interval (point) as its $j^{th}$ element, and all such tuples must match in their $j^{th}$ element. Call this common expression $e'_j$.
\end{itemize}
The home barrier expression is $z[e'_0, ..., e'_{M-1}]$.

Examples

\begin{tabular}{l l}
\textbf{Arrive statement} & \textbf{Home barrier expression} \\
\texttt{Arrive(...) >> $z$[$y_1, y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z$[$y_1$, :]} & Missing point expression for rightmost dimension \\
\texttt{Arrive(...) >> $z$[$y_1$, :] >> $z$[$y_1$, $y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z$[$y_1$, :] >> $z$[:, $y_2$]} & $z$[$y_1, y_2$] \\
\texttt{Arrive(...) >> $z_1$[$y_1$, :] >> $z_2$[:, $y_2$]} & Invalid, mismatched barrier variable names $z$
\end{tabular}

% >I

\subsection{\textrm{I}ota, mathbb-\textrm{I} ($\iota: \mathbb{I}$)}
\label{sec:gIota}

$\mathbb{I}$ denotes the set of all task IDs (Section~\ref{sec:SyncSemanticsThreadMapping}) and $\iota$ is an element of that set.

% >J
% >K
% >L

\subsection{Local Thread Index}
\label{sec:gLocalThreadIndex}

0-based integer index uniquely identifying a thread within a cluster.
The threads are indexed lexicographically by (CTA index, thread index in CTA).
The closed-form equation is \lighttt{cluster\_ctarank * blockDim.x + threadIdx.x}

Note, not to be confused with linear thread IDs (tid) in camspork, which also encodes the task index (Section~\ref{sec:SyncSemanticsThreadMapping}).

\subsection{Loop Mode}
\label{sec:gLoopMode}

Each Exo loop has an included loop mode object, which is one of these Python objects:
\begin{itemize}
  \item \lighttt{Seq(pragma\_unroll: Optional[int])}: sequential loop.
  \item \lighttt{Par()}: OpenMP parallel-for.
  \item \lighttt{CudaTasks()}: distribute iterations (device tasks,~\ref{sec:gDeviceTask}) to CUDA clusters; defines new task IDs in abstract machine semantics (Section~\ref{sec:SyncSemanticsThreadMapping}).
  \item \lighttt{CudaThreads(unit: CollUnit)}: distribute iterations to thread collectives within a cluster.
\end{itemize}
This does not affect the sequential semantics of the loop.

The respective frontend syntax is

\input{b_samples/loop_modes.0.tex}

Scheduling functions:
\begin{itemize}
  \item \texttt{set\_loop\_mode}: use new loop mode object.
  \item \texttt{update\_loop\_mode}: modify attribute of loop mode.
  \item \texttt{parallelize\_loop}: use \lighttt{Par()} as the loop mode (legacy).
\end{itemize}

\subsection{Lowered Barrier Type}
\label{sec:gLoweredBarrierType}

See Section~\ref{sec:SolitaryBarrier}.

% >M

\subsection{Mbarrier Ring Buffer}
\label{sec:gMbarrierRingBuffer}

Each barrier array element with \lighttt{CudaMbarrier} barrier mechanism (Section~\ref{sec:MbarrierUsage}) is lowered as a ring buffer of CUDA mbarrier objects.
The depth of this ring buffer depends on the number of barriers on the barrier guard cycle (def~\ref{sec:gBarrierGuardCycle}) for the barrier variable being lowered.

Each barrier variable $z$ has a unique value $n(z)$ used as its $n$ parameter for \lighttt{Await}.
Let $\texttt{lag}(z) = -1 - n(z)$.
Let $C$ be the barrier guard cycle for $z$.
The ring buffer depth is
\begin{equation*}
  \begin{cases}
    1 + \texttt{lag}(z) & \text{ if $C$ is of size 1 } \\
    \sum_{z' \in C} \texttt{lag}(z') & \text{ otherwise }
  \end{cases}
\end{equation*}
If the calculated depth is non-positive, the compilation fails.

\subsection{Multicast Flags}
\label{sec:gMulticastFlags}

See ``Multicasts'' (def~\ref{sec:gMulticasts}).

\subsection{Multicasts}
\label{sec:gMulticasts}

Each barrier expression (def~\ref{sec:gBarrierExpr}) has its own \myKeyA{multicast flags}.
This is a tuple of boolean values $t_i$ where $t_i$ is true if the $i^{th}$ index is an interval, and false if the $i^{th}$ index is a point.

The multicasts of an \lighttt{Arrive} statement (Section~\ref{sec:BarrierMulticast}) is a tuple-of-tuple-of-booleans, consisting of each barrier expression's multicast flags.
For example,

\input{b_samples/multicast_flags_example.0.tex}


% >N

\subsection{$\mathbb{N}$ (mathbb-N)}

Set of natural numbers (non-negative integers).
We use $\mathbb{N}_{\ge x}$ to mean the set of natural numbers greater-than-or-equal-to $x$, and $\mathbb{N}_\top$ to mean the set of natural numbers and the symbol $\top$ (top).

$[a, b]_\mathbb{N}$ denotes the set of natural numbers $n$ where $a \le n \le b$.

% >O
\subsection{Omega ($\omega: \Omega$)}
\label{sec:gOmega}

$\Omega$ denotes the set of all collective tilings (def~\ref{sec:gCollTiling}, Section~\ref{sec:CollTiling}) and $\omega$ is an element of that set.

\subsection{Out-of-order Non-convergent Abstract Machine Optimization}
\label{sec:gOooOpt}

TODO

% >P

\subsection{Pending Await ($\mathbb{P}$ / mathbb-P)}
\label{sec:gPendingAwait}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
Tuple of type $\mathbb{B} \times \mathbb{Z}^* \times \mathbb{Z}$ where the respective tuple elements are
\begin{itemize}
  \item Barrier variable name
  \item Array indices
  \item Arrive count
\end{itemize}
Note the PLDI submission formulates this differently, as a function $p : \mathbb{B} \to \mathbb{Z}^* \to \mathcal{P}(\mathbb{N})$.
Given $z: \mathbb{B}$ (def~\ref{sec:gMathbbB}), $i^* : \mathbb{Z}^*$, and $p'$ is a set of pending awaits as currently formulated, the correspondence between $p'$ and equivalent $p$ is $(z, i^*, n) \in p' \iff n \in p(z, i^*)$.

\subsection{Power Set ($\mathcal{P}$ / mathcal-P)}
\label{sec:gPowerSet}

$a^* \in \mathcal{P}(A)$ or $a^*: \mathcal{P}(A)$ means that $a^*$ is of type ``set of $A$''.

% >Q

\subsection{Qualitative Timelines (QualTL)}
\label{sec:gQualTL}

Additional description on each memory access, beyond the IDs of the thread(s) performing the access.
A synchronization timeline (SyncTL, def~\ref{sec:gSyncTL}) groups together qualitative timelines.

\input{spork_b/QualTL.tex}

\subsection{Queue Barrier}
\label{sec:gQueueBarrier}

Old name for a barrier array element.

% >R
\subsection{$\mathbb{R}$ (mathbb-R)}
\label{sec:gMathbbR}

Used in the PLDI submission to mean $\mathsf{VisRecord}$ (def~\ref{sec:gVisRecord}).

% >S

\subsection{$\mathbb{S}$ (mathbb-S)}
\label{sec:gMathbbS}

Used in the PLDI submission to mean the set of all visibility functions, which corresponds to visibility sets (def~\ref{sec:gVisSet}) in Exo-GPU as it is currently formulated.
Note ``visibility function'' is not a term used in the PLDI submission; this is retroactive terminology for this deprecated concept.

\subsection{Sigma ($\sigma: \Sigma$)}
\label{sec:gSigma}

$\Sigma$ denotes the set of all control environments (def~\ref{sec:gControlEnv}) and $\sigma$ is an element of that set.
Note, historically sometimes $\Sigma_c$ or $\sigma_c$ are used.

\subsection{Square Brackets}
\label{sec:gSquareBrackets}

May denote indexing an array, or function update notation (def~\ref{sec:gUpdateNotation}).

\subsection{Statement Instance}
\label{sec:gStmtInstance}

A \emph{statement} is a syntactic construct, while a \emph{statement instance} is a single interpretation/``execution'' of a statement. For example, ``\lighttt{for i in seq(0, 10): $s_1$; $s_2$}'' is a loop containing two statements: $s_1$ and $s_2$, and when the loop is executed, 20 statement instances are created (10 instances of $s_1$ and 10 instances of $s_2$).

\subsection{Sync-Exempt}
\label{sec:gSyncExempt}

Certain memory types are sync-exempt, as defined by their Python\\
\lighttt{AllocableMemWin.sync\_exempt()} function.
All reads and writes to array elements stored in sync-exempt memory type are no-ops in the abstract machine (Section~\ref{sec:SyncSemantics}), i.e., no hazards will ever be diagnosed for variables stored in sync-exempt memory.

\subsection{Synchronization Environment (SyncEnv)}
\label{sec:gSyncEnv}

See Section~\ref{sec:SyncEnv}.

\subsection{Synchronization Statement}
\label{sec:gSyncStmt}

One of

\hphantom{spacing}
\texttt{Fence($\tau_s^\mathrm{pre}, \tau_s^\mathrm{post}$)}
\hfill
\texttt{Arrive($\tau_s^\mathrm{pre}$) >}\texttt{> $e$}
\hfill
\texttt{Await($e, \tau_s^\mathrm{post}, n$)}
\hphantom{spacing}

where $\tau_s^\mathrm{pre}$ and $\tau_s^\mathrm{post}$ are \myKeyA{sync timelines} (\textsf{SyncTL}, def~\ref{sec:gSyncTL}), which filter the set of qualitative timelines (def~\ref{sec:gQualTL}) of memory accesses that are synchronized, and $e$ and $n$ are a barrier expression (def~\ref{sec:gBarrierExpr}) and an integer, which together control pairing of executed \lighttt{Arrive} and \lighttt{Await} instances (Section~\ref{sec:ArriveAwaitPairing}).
The \lighttt{Arrive} statement may take additional barrier expressions, separated by \texttt{>}\texttt{>} (def~\ref{sec:gHomeBarrier}).
Currently, an \lighttt{Await} statement may take only one barrier expression, and it must reference only a single barrier array element, i.e. it must be its own home barrier expression (def~\ref{sec:gHomeBarrier}).

Scheduling functions:
\begin{itemize}
  \item \texttt{insert\_barrier\_alloc}: insert allocation of a barrier variable (def~\ref{sec:gBarrierVariable})
  \item \texttt{insert\_fence}
  \item \texttt{insert\_arrive}
  \item \texttt{insert\_await}
\end{itemize}

\subsection{Synchronization Timeline (SyncTL)}
\label{sec:gSyncTL}

Parameter for synchronization statements (\lighttt{Fence}, \lighttt{Arrive}, \lighttt{Await}), which are defined as a composition of a \myKeyA{full timeline set} (set of \textsf{QualTL}, def~\ref{sec:gQualTL}) and a \myKeyA{temporal timeline set} (set of \textsf{QualTL}).
For a given $\tau_s$, these are denoted as $\tau_s.\mathrm{full}$ and $\tau_s.\mathrm{temp}$ respectively.

We list the sync timelines $\tau_s$ in the following table, where ``temp.'' in a qualitative timeline's column indicates membership of the qualitative timeline in $\tau_s.\mathrm{temp}$, and ``full'' indicates membership in \emph{both} $\tau_s.\mathrm{full}$ and $\tau_s.\mathrm{temp}$. The PLDI submission also had ``transitive'', which is gone now.

{
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{|r|l l|l l|l l l| l l l l|l|}
\hline
$\tau_s$ & cpu & strm & cuda1 & cuda2 & Sm80 & tmaS & tmaG & wgA & wgD & wgS & wg0 & async \\
\hline
\texttt{empty\_sync\_tl} &  &  &  &  &  &  &  &  &  &  &  & \\
\texttt{cpu\_in\_order} & full &  &  &  &  &  &  &  &  &  &  & \\
\texttt{cuda\_stream\_sync} &  & full & full & full & full & full & full & full & full & full &  & full\\
\texttt{cuda\_in\_order} &  &  & full & full &  &  &  &  &  &  & temp. & temp.\\
\texttt{cuda\_temporal} &  &  & temp. & temp. &  &  &  &  &  &  & temp. & temp.\\
\texttt{Sm80\_cp\_async} &  &  &  &  & full &  &  &  &  &  &  & \\
\texttt{Sm80\_generic} &  &  & full & full & full &  &  &  &  &  & temp. & temp.\\
\texttt{tma\_to\_smem\_async} &  &  &  &  &  & full &  &  &  &  &  & \\
\texttt{tma\_to\_gmem\_async} &  &  &  &  &  &  & full &  &  &  &  & \\
\texttt{wgmma\_async\_smem} &  &  &  &  &  &  &  &  &  & full &  & \\
\texttt{wgmma\_fence\_1} &  &  & full &  &  &  &  & full & full &  &  & \\
\texttt{wgmma\_fence\_2} &  &  &  &  &  &  &  & full & full &  &  & \\
\texttt{wgmma\_async} &  &  &  &  &  &  &  & full & full & full &  & \\
\texttt{cuda\_generic\_and\_async\_proxy} &  &  & full & full &  &  &  &  &  &  & temp. & full\\
\hline
\end{tabular}
}

\textsf{QualTL} key:

\input{spork_b/QualTL.tex}

% NB note alphabetical sorting
%           |
% Synchronization
% Synchronizes
\subsection{Synchronizes-with}
\label{sec:gSynchronizesWith}

camspork's name for the witness function (Section~\ref{sec:Witness}).

% >T
\subsection{Thread Collective}
\label{sec:gThreadCollective}

Set of threads assigned to execute one statement instance (def~\ref{sec:gStmtInstance}).
On the CUDA device, the threads in the thread collective are uniquely identified by its cluster index (which is not statically analyzed) and its local thread index (def~\ref{sec:gLocalThreadIndex}), which is analyzed by collective analysis (Section~\ref{sec:CollTiling}).
In the context of the abstract machine, the cluster index is replaced with a task index, since the mapping between tasks and clusters is a hardware-dependent detail we abstract over.
Specifically, a thread collective in this context is a set of global thread IDs (def~\ref{sec:gGlobalThreadID}).

\subsection{Thread Cuboid}
\label{sec:gThreadCuboid}

camspork implementation concept, which takes the place of the thread mapping (Section~\ref{sec:SyncSemanticsThreadMapping}).
The thread cuboid has a task ID $\iota: \mathbb{I}$, and equal-dimensional integer tuples domain ($D$), box ($B$), and offset ($\textit{Off}$).
This encodes via the \textsf{toLocal} function (def~\ref{sec:gToLocal}) the thread collective (def~\ref{sec:gThreadCollective}) of global thread IDs
\begin{equation*}
  \{ (\iota, \mathsf{toLocal}(D, c) \mid c \in
      [\textit{Off}_0, \textit{Off}_0 + B_0 - 1]_\mathbb{N} \times ...
      [\textit{Off}_1, \textit{Off}_1 + B_1 - 1]_\mathbb{N} \times ...  \}
\end{equation*}

\subsection{Thread Pitch (Set)}
\label{sec:gThreadPitch}

The thread pitch is used in multiple contexts.
In all cases, it describes the ``distance'', in local thread indices (def~\ref{sec:gLocalThreadIndex}), between adjacent items of some sort.

\mainKey{\texttt{cuda\_threads} Loop Iterator:} Let $\mu: \mathcal{P}(\mathbb{N})$ be the local thread indices of the thread collective executing the 0th iteration of the loop.
The local thread indices of the thread collective execucting the $j^{th}$ iteration of the loop are $\{t + jp \mid t \in \mu\}$, $p$ being the thread pitch of the loop iterator.
If the loop has no more than 1 iteration, then the thread pitch of the loop iterator is 0.

\mainKey{Distributed Memory:} Let $\mu: \mathcal{P}(\mathbb{N})$ be the local thread indices of the thread collective allocating the physical memory holding $x[0, ..., 0]$, and let the deduced thread pitch tuple for the variable $x$ be $(p_0, ..., p_{M-1})$.
Then the local thread indices of the thread collective for $x[i_0, i_1, ...]$ are
\begin{equation*}
  \left \{ t + \sum_{k=0}^{M-1} p_k i_k \mid t \in \mu \right \}
\end{equation*}

\mainKey{Domain:} For a domain $(D_0, ..., D_{M-1})$, we define respective dimension thread pitch values as
\begin{equation*}
    P_m = \prod_{k=m+1}^{M-1} D_k
\end{equation*}
As a shorthand, we say $\delta.P_k$ or $\omega.P_k$ to mean the $k^{th}$ dimension thread pitch defined above, with respect to $\delta.D$ or $\omega.D$.
The thread pitch set of $D$ is $\{P_0, ..., P_{M-1}\}$; note $P_{M-1} = 1$ always.

\mainKey{Collective Tiling/Type:} The thread pitch set of a collective tiling/type is that of its domain.

\subsection{Tiled Dimension}
\label{sec:gDeriveCollTiling}

See \textsf{deriveCollTiling} (def~\ref{sec:gDeriveCollTiling}).

\subsection{Timeline Signature (TlSig)}
\label{sec:gTlSig}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
Triple $(g, q, v)$ of type $\mathbb{G} \times \mathsf{QualTL} \times \mathsf{VF}$, i.e. a global thread id $g$ (def~\ref{sec:gGlobalThreadID}), qualitative timeline $q$ (def~\ref{sec:gQualTL}), and visibility flag $v$ (def~\ref{sec:gVisFlag}).
These are composed by visibility records (def~\ref{sec:gVisRecord}).

\subsection{toLocal}
\label{sec:gToLocal}

We define the mapping $\mathsf{toLocal}: \mathbb{N}^M \to \mathbb{N}^M \to \mathbb{N}$, which converts a domain (def~\ref{sec:gDomain}) and coordinates to a local thread index, as
\begin{align*}
    \mathsf{toLocal}((D_0,...,D_{M-1}), (c_0,...,c_{M-1})) \mapsto \sum_{k=0}^{M-1} c_k \times D_{k+1} \times ... \times D_{M-1}
\end{align*}
i.e. the coordinates $[0, D_0-1]_\mathbb{N} \times ... \times [0, D_{M-1}-1]_\mathbb{N}$ get mapped to local thread indices (def~\ref{sec:gLocalThreadIndex}) in lexicographical order.
For this definition to work as expected, the product of the domain coordinates $D_0 \times ... \times D_{M-1}$ must be equal to the number of threads in the cluster (\lighttt{clusterDim.x * blockDim.x}).

\subsection{Trailing Barrier Expression}
\label{sec:gTrailingBarrierExpr}

Syntax ``\texttt{>}\texttt{> }$e$'' added after the closing \texttt{)} of an instruction call, with $e$ being a single barrier expression (def~\ref{sec:gBarrierExpr}).
This allows direct usage of \lighttt{Await} (Section~\ref{sec:ArriveAwaitPairing}) to wait for the instruction to retire, which is treated as a special case in sync-check (Section~\ref{sec:VisRecordCreation}, Section~\ref{sec:AwaitSemantics}).

The trailing barrier expression for an instruction is separate from its formal parameters.
The \lighttt{replace} function does \textbf{not} require a correct trailing barrier expression.
Use the scheduling function \lighttt{set\_trailing\_barrier\_expr} after the \lighttt{replace}.

Note, the home barrier expression (def~\ref{sec:gHomeBarrier}) concept does \textbf{not} apply to the trailing barrier expression.

% >U

\subsection{Update Notation}
\label{sec:gUpdateNotation}

$\rho[a_0 \to b_0, a_1 \to b_1, ...]$ means the function $\rho$ with outputs for $a_0, a_1,...$ modified:
\begin{equation*}
    \rho[...](x) = \begin{cases}
        b_0 & \text{if } x = a_0 \\
        b_1 & \text{if } x = a_1 \\
        ... & ... \\
        \rho(x) & \text{if no other case}
    \end{cases}
\end{equation*}

% >V

\subsection{Visibility Flag ($\mathsf{VF}$)}
\label{sec:gVisFlag}

Abstract machine concept (Section~\ref{sec:SyncSemantics}).
One of
\begin{itemize}
  \item $\mathsf{VF_{atom}}$, \texttt{vis\_flag\_atomic\_only}
  \item $\mathsf{VF_{temp}}$, \texttt{vis\_flag\_temporal}
  \item $\mathsf{VF_{full}}$, \texttt{vis\_flag\_full}
  \item $\mathsf{VF_{issue}}$, \texttt{vis\_flag\_issue}
\end{itemize}
These are part of a timeline signature (def~\ref{sec:gTlSig}).

\subsection{Visibility Record (VisRecord)}
\label{sec:gVisRecord}

Defined in abstract machine semantics (Section~\ref{sec:VisRecordState}).
Pair of type $\mathcal{P}(\mathsf{TlSig}) \times \mathcal{P}(\mathbb{P})$, i.e.
\begin{itemize}
  \item visibility set, which is a set of timeline signatures (def~\ref{sec:gTlSig})
  \item set of pending awaits (def~\ref{sec:gPendingAwait}).
\end{itemize}

\subsection{Visibility Set}
\label{sec:gVisSet}

Note, the PLDI submission incorrectly uses ``visibility set'' to mean ``visibility record'' sometimes.

Set of timeline signatures (def~\ref{sec:gTlSig}).
This differs from the PLDI submission nomenclature.
The PLDI submission used a visibility function $\mathbb{S} \Coloneqq \mathbb{G} \to \mathsf{QualTL} \to \mathsf{VL}$, with \textsf{VL} being the deprecated ``visibility level'' value.

Approximately, the correspondence between a visibility function $f$ and a visibility set $s$ is
\begin{itemize}
  \item Each visibility level corresponds to some set of visibility flags (def~\ref{sec:gVisFlag}), specifically, none corresponds to $\{\}$, atomic-only corresponds to $\{\mathsf{VF_{atom}}\}$, unordered corresponds to either $\{\mathsf{VF_{issue}}\}$ or $\{\mathsf{VF_{issue}}, \mathsf{VF_{atom}}\}$ (this ambiguity being a reason for this change, and abandoning the strict ordering of visiblity levels), temporally-ordered corresponds to $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}\}$ or $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}, \mathsf{VF_{atom}}\}$, and fully-ordered corresponds to $\{\mathsf{VF_{issue}}, \mathsf{VF_{temp}}, \mathsf{VF_{full}}, \mathsf{VF_{atom}}\}$.
  \item Given visibility function $f: \mathbb{S}$, $g: \mathbb{G}$ (def~\ref{sec:gGlobalThreadID}), $q: \mathsf{QualTL}$ (def~\ref{sec:gQualTL}), and the visibility level $v$ is $v = f(g, q)$, then the visibility set $s$ contains $(g, q, v')$ if and only if $v'$ is a visibility flag in the set of visibility flags corresponding to the visibility level $v$.
\end{itemize}

% >W

\subsection{Warp}
\label{sec:gWarp}

32 CUDA threads with consecutive \lighttt{threadIdx.x} values, aligned so that the lowest index is a multiple of 32.
(Note, this is simplified from the real CUDA definition, which takes into account the y and z dimensions that Exo-GPU does not parallelize on).

\subsection{Warp Variable}
\label{sec:gWarpVariable}

The warp variables for a specific CUDA device function are specified by the \lighttt{warp\_config: List[CudaWarpConfig]} parameter of \lighttt{CudaDeviceFunction}.
This specifies a name and register count for a certain number of warps in the CTA.
See example~\ref{sec:gCudaDeviceFunction}, Section~\ref{sec:CudaDeviceFunction}.

If \lighttt{blockDim} is given instead of \lighttt{warp\_config}, then there's implicitly a single warp variable with name \lighttt{""}, warp count $\lighttt{blockDim}/32$, and no register count modification.

The ``prefix'' of a warp variable is the sum of the warp counts of the warp variables specified before itself.

The CUDA device function is compiled separately for each warp variable, with code paths for other warp variables statically elimintated.

\subsection{Warpgroup}
\label{sec:gWarpgroup}

128 CUDA threads with consecutive \lighttt{threadIdx.x} values, aligned so that the lowest index is a multiple of 128.
(Note, this is simplified from the real CUDA definition, which takes into account the y and z dimensions that Exo-GPU does not parallelize on).

\subsection{$\mathcal{W}$ (Witness Function)}

Abstract machine helper function defined in Section~\ref{sec:Witness}.
Used to filter the set of visibility records (Section~\ref{sec:VisRecordState}) modified by an interpreted \lighttt{Fence} statement (Section~\ref{sec:FenceSemantics}) or \lighttt{Arrive} statement (Section~\ref{sec:ArriveSemantics}).

% >X

\subsection{$\mathbb{X}$ (mathbb-x)}
\label{sec:gMathbbX}

Set of data variable names.
These are variables that are not of control ($\mathbb{Y}$, def~\ref{sec:gMathbbY}) or barrier ($\mathbb{B}$, def~\ref{sec:gMathbbB}) type.

% >Y

\subsection{$\mathbb{Y}$ (mathbb-y)}
\label{sec:gMathbbY}

Set of control variable names: names of index and size variables (either function parameters of index/size type, or loop iterators).

% >Z

\subsection{$z: \mathbb{B}$}
\label{sec:gZ}

Somewhat confusingly, we conventionally use $z$ to mean an element of $\mathbb{B}$, i.e. $z$ is the name of a barrier variable (def~\ref{sec:gBarrierVariable}).

\subsection{$\mathbb{Z}$ (mathbb-z)}
\label{sec:gMathbbZ}

Set of all integers.

\end{document}
